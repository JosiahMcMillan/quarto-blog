[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about",
    "section": "",
    "text": "I’m a recent graduate with a Master’s Degree in Statistics from the Katholieke Universiteit of Leuven (KU Leuven) my degree track was Theoretical Statistics and Data Science. I completed my thesis exploring count data models specifically I characterized a family of functions which were an extension of the Poisson which had greater flexibility in the dispersion settings and compared those to other count data models including tje Conway-Maxwell-Poisson Model. My previous work experience was focused on marketing data. I blog about statistics and data.\n\nI’m looking for a job!\n\nI have experience with causal inference, regression, marketing attribution models, and clustering.\nI enjoy writing about topics where I can express statistical knowledge. Two articles I enjoyed making are this explanation of how we need to understand our data generating process when determining causality using Covid as an example and this explanation of how sampling variability explains the gap between the top men and women in chess.\nI’m well-suited for roles as a statistician, data analyst, or data scientist.\nI speak English natively and I currently speak Indonesian at around a B1 level \n\nYou can find my resume here."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "talks",
    "section": "",
    "text": "I’m defending my dissertation in April. My defense will be open to the public, and will feature a ~45 min talk on the kinds of network peers effects that can be estimated using standard tools from social science.\n\nAsymptotic identification of peer effects in linear models\n2024-04-04 @ 12:30 pm\nSMI 133, Medical Sciences Center\n1300 University Ave, Madison, WI\n\nLast updated on 2024-02-14."
  },
  {
    "objectID": "talks/index.html#upcoming-talks",
    "href": "talks/index.html#upcoming-talks",
    "title": "talks",
    "section": "",
    "text": "I’m defending my dissertation in April. My defense will be open to the public, and will feature a ~45 min talk on the kinds of network peers effects that can be estimated using standard tools from social science.\n\nAsymptotic identification of peer effects in linear models\n2024-04-04 @ 12:30 pm\nSMI 133, Medical Sciences Center\n1300 University Ave, Madison, WI\n\nLast updated on 2024-02-14."
  },
  {
    "objectID": "talks/index.html#past-talks",
    "href": "talks/index.html#past-talks",
    "title": "talks",
    "section": "past talks",
    "text": "past talks\nPeer effects are parametrically indistinguishable from baseline behaviors in the asymptotic limit\n2023-11-27 @ 4 pm, Computer Science 1325, Statistics Grad Student Seminar\nAlex Hayes and Keith Levin\nslides (private for the time being)\nLatent contagion in low-rank networks\n2023-10-11 @ 2 pm, SMI 133, Levin Lab Meeting Alex Hayes and Keith Levin\nslides (private for the time being)\nPeer diffusion over uncertain networks\n2023-09-18 @ 12:30 pm, WID 1145, IFDS Ideas Seminar\nAlex Hayes and Keith Levin\nslides (private for the time being)\nEstimating network-mediated causal effects via spectral embeddings\n2023-08-09 @ 10:30 am, Recent Developments in Causal Inference, JSM 2023\nAlex Hayes, Mark M. Fredrickson and Keith Levin\nCausal inference for network data is an area of active interest in the social sciences. Unfortunately, the complicated dependence structure of network data presents an obstacle to many causal inference procedures. We consider the task of mediation analysis for network data, and present a model in which mediation occurs in a latent embedding space. Under this model, node-level interventions have causal effects on nodal outcomes, and these effects can be partitioned into a direct effect independent of the network, and an indirect effect induced by homophily. To estimate network-mediated effects, we embed nodes into a low-dimensional space and fit two regression models: (1) an outcome model describing how nodal outcomes vary with treatment, controls, and position in latent space; and (2) a mediator model describing how latent positions vary with treatment and controls. We prove that the estimated coefficients are asymptotically normal about the true coefficients under a sub-gamma generalization of the random dot product graph, a widely-used latent space model. We show that these coefficients can be used in product-of-coefficients estimators for causal inference. Our method is easy to implement, scales to networks with millions of edges, and can be extended to accommodate a variety of structured data. slides, pre-print\nEstimating network-mediated causal effects via spectral embeddings\n2023-05-24 @ 5:30 pm, Poster Session 1, ACIC 2023\nAlex Hayes, Mark M. Fredrickson and Keith Levin\nWe consider the task of mediation analysis for network data, and present a model in which mediation occurs in a latent embedding space. Under this model, node-level interventions have causal effects on nodal outcomes, and these effects can be partitioned into a direct effect independent of the network, and an indirect effect induced by homophily. poster, pre-print\nThanks to the University of Wisconsin for funding a portion of my travel to ACIC with a Student Research Grants Competition-Combined Award.\nEstimating network-mediated causal effects via spectral embeddings\n2023-04-24 @ 12:30 pm, Orchard View @ the WID, IFDS Ideas Seminar\nAlex Hayes, Mark M. Fredrickson and Keith Levin\nCausal inference for network data is an area of active interest in the social sciences. Unfortunately, the complicated dependence structure of network data presents an obstacle to many causal inference procedures. We consider the task of mediation analysis for network data, and present a model in which mediation occurs in a latent embedding space. Under this model, node-level interventions have causal effects on nodal outcomes, and these effects can be partitioned into a direct effect independent of the network, and an indirect effect induced by homophily. To estimate network-mediated effects, we embed nodes into a low-dimensional space and fit two regression models: (1) an outcome model describing how nodal outcomes vary with treatment, controls, and position in latent space; and (2) a mediator model describing how latent positions vary with treatment and controls. We prove that the estimated coefficients are asymptotically normal about the true coefficients under a sub-gamma generalization of the random dot product graph, a widely-used latent space model. We show that these coefficients can be used in product-of-coefficients estimators for causal inference. Our method is easy to implement, scales to networks with millions of edges, and can be extended to accommodate a variety of structured data. pre-print, slides\nEstimating network-mediated causal effects via spectral embeddings\n2022-10-14 @ 4:15 pm in MSC 1210, Statistics Graduate Student Association Seminar\nThe last several years have seen a renewed and concerted effort to incorporate network data into standard tools for regression analysis, and to make network-linked data legible to practicing scientists. Thus far, this literature has primarily developed tools to infer associative relationships between nodal covariates and network structure. In contrast, we augment a statistical model for network regression with counterfactual assumptions and show how causal effects on a network can be partitioned into a direct effect that is uninfluenced by the network, and an indirect effect that is induced by homophily. slides\nEstimating indirect effects induced by homophily via spectral network regression\n2022-07-07, Tianxi Li and Can Le Joint Lab Meeting\nThe last several years have seen a renewed and concerted effort to incorporate network data into standard tools for regression analysis, and to make network-linked data legible to practicing scientists. Thus far, this literature has primarily developed tools to infer associative relationships between nodal covariates and network structure. In contrast, we augment a statistical model for network regression with counterfactual assumptions and show how causal effects on a network can be partitioned into a direct effect that is uninfluenced by the network, and an indirect effect that is induced by homophily. slides\ndistributions3: From basic probability to probabilistic regression\n2022-06-23, UseR 2022\nAchim Zeileis, Moritz Lang and Alex Hayes\nThe distributions3 package provides a beginner-friendly and lightweight interface to probability distributions. It allows to create distribution objects in the S3 paradigm that are essentially data frames of parameters, for which standard methods are available: e.g., evaluation of the probability density, cumulative distribution, and quantile functions, as well as random samples. It has been designed such that it can be employed in introductory statistics and probability courses. By not only providing objects for a single distribution but also for vectors of distributions, users can transition seamlessly to a representation of probabilistic forecasts from regression models such as GLM (generalized linear models), GAMLSS (generalized additive models for location, scale, and shape), etc. We show how the package can be used both in teaching and in applied statistical modeling, for interpreting fitted models, visualizing their goodness of fit (e.g., via the topmodels package), and assessing their performance (e.g., via the scoringRules package). video, slides\nThe Low Hanging Fruit of the Twitter Following Graph\n2021-08-11, Joint Statistical Meetings\nAlex Hayes and Karl Rohe\nIn recent applied work on the Twitter media ecosystem, we have found that Twitter metadata (such as follows, likes, quotes, retweets, mentions, etc) is often more informative than the actual content of tweets themselves. The metadata, in some sense, is the right data to use for many inference tasks. In particular, we find that embedding the Twitter following graph is highly informative. However, collecting the following graph is rather challenging due to API rate limits, and storing graphs can also be challenging. We present some computational infrastructure to make access and storage of this high signal data more straightforward, and suggest that research progress would be well served by an increased focus on instrumentation. slides\nSolving the model representation problem with broom\n2019-01-25, rstudio::conf(2019)\nThe R objects used to represent model fits are notoriously inconsistent, making data analysis inconvenient and frustrating. The broom package resolves this issue by defining a consistent way to represent model fits. By summarizing essential information about fits in tidy tibbles, broom makes it easy to programmatically work with model objects. Combining broom with list-columns results in an especially powerful way to work with many model fits at once. This talk will feature several case studies demonstrating how broom resolves common problems in data analysis. video, slides\nSolving the model representation problem with broom\n2018-11-30, Statistics Graduate Student Seminar\nslides\nConvenient data analysis with broom\n2018-11-14, RStudio Webinar Series\nBroom is a package that converts statistical objects into tibbles. This consistent structure makes it easier to accomplish many standard modelling tasks. In this webinar I’ll demonstrate how to use to broom to work with many models at once. We’ll see how broom makes it easier to visualize models, work with bootstrapped fits and assess model diagnostics. video, slides\nSolving the model representation problem with broom\n2018-09-19, Madison R User Group\nslides"
  },
  {
    "objectID": "talks/index.html#slides-from-various-informal-presentations",
    "href": "talks/index.html#slides-from-various-informal-presentations",
    "title": "talks",
    "section": "slides from various informal presentations",
    "text": "slides from various informal presentations\nIdentifiability of homophily and contagion in social networks\n2022-02-23, Madison Networks Reading Group\nslides\nTriangles & networks\n2021-02-17, STAT 992 Seminar on Tensors\nslides\nA new way to think about citations\n2020-11-17, Rohe Lab Group Meeting\nslides\nThe linear probability model\n2019-11-19, STAT 992 Seminar Course presentation\nslides\nrstudio internship progress update\n2018-07-23, RStudio tidyverse team\nslides\nLocally Interpretable Model-Agnostic Explanations\n2018-03-29, Rice DataSci club\nslides\nYour First R Package\n2018-02-22, Rice DataSci club\nslides"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "This resume is available as a pdf."
  },
  {
    "objectID": "resume/index.html#work-experience",
    "href": "resume/index.html#work-experience",
    "title": "Resume",
    "section": "Work Experience",
    "text": "Work Experience\n\n\n\nDaBella\n\nMarketing Data Analyst\n\n\n\nJuly 2019 - August 2020\n\n\n\nDeveloped statistical methods based on principal components analysis to cluster networks with missing data, to perform regression on networks, and to construct, interpret, and regularize network embeddings.\nDeveloped causal inference methods to estimate mediation and spillover effects in social networks, and to determine when product changes have harmful side-effects on behaviors that are difficult to measure. Used causal machine learning to improve precision of estimates while reducing computational requirements by a factor of 5000.\nImplemented research methods in user-friendly software. Released nine open source R packages to CRAN (notable: fastRG, vsp, distributions3, gdim, aPPR, fastadi).\nResolved computational bottlenecks in matrix completion algorithms by designing and implementing sparse matrix methods in R and C++. Scaled methods by three orders of magnitude to handle networks with millions of nodes.\nDesigned an approach to find localized clusters of Twitter users via Personalized PageRank. Managed unreliable Twitter API behavior by caching data in a Neo4J database running in Docker.\nCollaborated with ROpenSci to design software development standards for statistical software. Reviewed scientific software for ROpenSci, the R Journal, and the Journal of Open Source Software.\n\n\n\n\nFirst Security Bancorp\n\nFinance Analyst Intern\n\n\n\nSummer 2018\n\n\n\nPrototyped a pipeline to automatically suggest relationships between hashtags, for a team using manual labeling. Prototype embedded a hashtag co-occurrence network and was implemented with Python, PyTorch and SQL.\n\n\n\n\nDaBella\n\nSales Representative\n\n\n\nSummer 2017\n\n\n\nPrototyped a pipeline to automatically suggest relationships between hashtags, for a team using manual labeling. Prototype embedded a hashtag co-occurrence network and was implemented with Python, PyTorch and SQL."
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "papers",
    "section": "",
    "text": "I primarily study networks using tools from multivariate analysis.\nMy work with Karl Rohe has focused on fast approaches to spectral estimation via sparse linear algebra. Our primary project together has been developing a method for network co-factor analysis for settings with missing edge data, which we applied to a large network of citations between statistics papers. We have also spent some time working to understand effective regularization strategies for spectral estimators, as well as developing diagnostic tools for PCA and varimax rotation. As part of an outgrowth of Karl’s murmuration project I developed extensive infrastructure to sample and analyze the Twitter following graph (see code), and Twitter data remains near and dear to my heart.\nI am currently working with Keith Levin on causal interpretations of network regression, and with Jiwei Zhao on semi-parametric inference for experimental guardrails in a data fusion setting.\nPreviously at Facebook, I developed internal tooling to understand post content using neural hypergraph embeddings. At Facebook I also developed a diagnostic to assess the out-of-sample reliability of rolling classifiers based on differential calibration of the classifier over time.\nI keep Google Scholar up to date, and post any research related code to Github (personal, lab)."
  },
  {
    "objectID": "papers/index.html#pre-prints",
    "href": "papers/index.html#pre-prints",
    "title": "papers",
    "section": "pre-prints",
    "text": "pre-prints\n\nEstimating network-mediated causal effects via spectral embeddings. Alex Hayes, Mark M. Fredrickson, and Keith Levin. arXiv, April 14, 2023. http://arxiv.org/abs/2212.12041"
  },
  {
    "objectID": "papers/index.html#publications",
    "href": "papers/index.html#publications",
    "title": "papers",
    "section": "publications",
    "text": "publications\n\nWelcome to the Tidyverse. Hadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, Alex Hayes, Lionel Henry, Jim Hester, Max Kuhn, Thomas Lin Pedersen, Evan Miller, Kirill Müller, David Robinson, Dana Paige Seidel, Vitalie Spinu, Kohske Takahashi, Davis Vaughan, Claus Wilke, Kara Woo, Hiroaki Yutani. Journal of Open Source Software, 2019. pdf\n\nLast updated on 2023-08-05."
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html",
    "title": "gentle tidy eval with examples",
    "section": "",
    "text": "I’ve been using the tidy eval framework introduced with dplyr 0.7 for about two months now, and it’s time for an update to my original post on tidy eval. My goal is not to explain tidy eval to you, but rather to show you some simple examples that you can easily generalize from.\nlibrary(tidyverse)\n\nstarwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 &lt;NA&gt;    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 &lt;NA&gt;    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 &lt;NA&gt;    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld"
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#using-strings-to-refer-to-column-names",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#using-strings-to-refer-to-column-names",
    "title": "gentle tidy eval with examples",
    "section": "Using strings to refer to column names",
    "text": "Using strings to refer to column names\nTo refer to columns in a data frame with strings, we need to convert those strings into symbol objects with rlang::sym and rlang::syms. We then use the created symbol objects in dplyr functions with the prefixes !! and !!!. This is because dplyr verbs expect input that looks like code. Using the sym/syms functions we can convert strings into objects that look like code.\n\nmass &lt;- rlang::sym(\"mass\")                        # create a single symbol\ngroups &lt;- rlang::syms(c(\"homeworld\", \"species\"))  # create a list of symbols\n\nstarwars %&gt;%\n  group_by(!!!groups) %&gt;%               # use list of symbols with !!!\n  summarize(avg_mass = mean(!!mass))    # use single symbol with !!\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt;\n 1 Alderaan       Human         NA  \n 2 Aleen Minor    Aleena        15  \n 3 Bespin         Human         79  \n 4 Bestine IV     Human        110  \n 5 Cato Neimoidia Neimodian     90  \n 6 Cerea          Cerean        82  \n 7 Champala       Chagrian      NA  \n 8 Chandrila      Human         NA  \n 9 Concord Dawn   Human         79  \n10 Corellia       Human         78.5\n# … with 48 more rows\n\n\nThe usage mass &lt;- rlang::sym(\"mass\") is Hadley approved:\nI believe it is also the current tidyverse code style standard. We use rlang::sym and rlang::syms identically inside functions.\n\nsummarize_by &lt;- function(df, groups, to_summarize) {\n  df %&gt;%\n    group_by(!!!rlang::syms(groups)) %&gt;%\n    summarize(summarized_mean = mean(!!rlang::sym(to_summarize)))\n}\n\nsummarize_by(starwars, c(\"homeworld\", \"species\"), \"mass\")\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   summarized_mean\n   &lt;chr&gt;          &lt;chr&gt;               &lt;dbl&gt;\n 1 Alderaan       Human                NA  \n 2 Aleen Minor    Aleena               15  \n 3 Bespin         Human                79  \n 4 Bestine IV     Human               110  \n 5 Cato Neimoidia Neimodian            90  \n 6 Cerea          Cerean               82  \n 7 Champala       Chagrian             NA  \n 8 Chandrila      Human                NA  \n 9 Concord Dawn   Human                79  \n10 Corellia       Human                78.5\n# … with 48 more rows"
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#details-about-unquoting",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#details-about-unquoting",
    "title": "gentle tidy eval with examples",
    "section": "Details about unquoting",
    "text": "Details about unquoting\n!! and !!! are syntactic sugar on top of the functions UQ() and UQS(), respectively. It used to be that !! and !!! had low operator precedence, meaning that in terms of PEMDAS they came pretty much last. But now we can use them more intuitively:\n\nhomeworld &lt;- rlang::sym(\"homeworld\")\n\nfilter(starwars, !!homeworld == \"Alderaan\")\n\n# A tibble: 3 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 Leia Organa     150    49 brown   light   brown        19 fema… femin… Aldera…\n2 Bail Presto…    191    NA black   tan     brown        67 male  mascu… Aldera…\n3 Raymus Anti…    188    79 brown   light   brown        NA male  mascu… Aldera…\n# … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\nWe can also use UQ and UQS directly to be explicit about what we’re unquoting.\n\nfilter(starwars, UQ(homeworld) == \"Alderaan\")\n\n# A tibble: 3 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n1 Leia Organa     150    49 brown   light   brown        19 fema… femin… Aldera…\n2 Bail Presto…    191    NA black   tan     brown        67 male  mascu… Aldera…\n3 Raymus Anti…    188    79 brown   light   brown        NA male  mascu… Aldera…\n# … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld"
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#creating-non-standard-functions",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#creating-non-standard-functions",
    "title": "gentle tidy eval with examples",
    "section": "Creating non-standard functions",
    "text": "Creating non-standard functions\nSometimes it is nice to write functions that use accept non-standard inputs, like dplyr verbs. For example, we might want to write a function with the same effect as\n\nstarwars %&gt;% \n  group_by(homeworld, species) %&gt;% \n  summarize(avg_mass = mean(mass))\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt;\n 1 Alderaan       Human         NA  \n 2 Aleen Minor    Aleena        15  \n 3 Bespin         Human         79  \n 4 Bestine IV     Human        110  \n 5 Cato Neimoidia Neimodian     90  \n 6 Cerea          Cerean        82  \n 7 Champala       Chagrian      NA  \n 8 Chandrila      Human         NA  \n 9 Concord Dawn   Human         79  \n10 Corellia       Human         78.5\n# … with 48 more rows\n\n\nTo this we need to capture our input in quosures with quo and quos when programming interactively.\n\ngroups &lt;- quos(homeworld, species)   # capture a list of variables as raw input\nmass &lt;- quo(mass)                    # capture a single variable as raw input\n\nstarwars %&gt;% \n  group_by(!!!groups) %&gt;%            # use !!! to access variables from `quos`\n  summarize(avg_mass = sum(!!mass))  # use !! to access the variable in `quo`\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt;\n 1 Alderaan       Human           NA\n 2 Aleen Minor    Aleena          15\n 3 Bespin         Human           79\n 4 Bestine IV     Human          110\n 5 Cato Neimoidia Neimodian       90\n 6 Cerea          Cerean          82\n 7 Champala       Chagrian        NA\n 8 Chandrila      Human           NA\n 9 Concord Dawn   Human           79\n10 Corellia       Human          157\n# … with 48 more rows\n\n\nThere’s some nice symmetry here in that we unwrap both rlang::sym and quo with !! and both rlang::syms and quos with !!!.\nWe might be interested in using this behavior in a function. To do this we replace calls to quo with calls to enquo.\n\nsummarize_by &lt;- function(df, to_summarize, ...) {\n\n  to_summarize &lt;- enquo(to_summarize)  # enquo captures a single argument\n  groups &lt;- quos(...)                  # quos captures multiple arguments\n\n  df %&gt;%\n    group_by(!!!groups) %&gt;%                 # unwrap quos with !!!\n    summarize(summ = sum(!!to_summarize))   # unwrap enquo with !!\n}\n\nNow our function call is non-standardized. Note that quos can capture an arbitrary number of arguments, like we have here. So both of the following calls are valid\n\nsummarize_by(starwars, mass, homeworld)\n\n# A tibble: 49 × 2\n   homeworld       summ\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Alderaan          NA\n 2 Aleen Minor       15\n 3 Bespin            79\n 4 Bestine IV       110\n 5 Cato Neimoidia    90\n 6 Cerea             82\n 7 Champala          NA\n 8 Chandrila         NA\n 9 Concord Dawn      79\n10 Corellia         157\n# … with 39 more rows\n\nsummarize_by(starwars, mass, homeworld, species)\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species    summ\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;\n 1 Alderaan       Human        NA\n 2 Aleen Minor    Aleena       15\n 3 Bespin         Human        79\n 4 Bestine IV     Human       110\n 5 Cato Neimoidia Neimodian    90\n 6 Cerea          Cerean       82\n 7 Champala       Chagrian     NA\n 8 Chandrila      Human        NA\n 9 Concord Dawn   Human        79\n10 Corellia       Human       157\n# … with 48 more rows\n\n\nFor more details, see the programming with dplyr vignette."
  },
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "code",
    "section": "",
    "text": "N.B.: For the last several years, I written code purely for the purposes of furthering my academic research. The goal of this code is not to be broadly usable. I only commit to maintaining and explaining my research code to the extent that it is assists others in their own academic research. I have also written code designed for broad consumption – see #rstats below for some details about my open source work.\nMy research code, as well as miscellaneous personal projects in various stages of completion, lives on my Github. I primarily use R, and most of my work as a developer is on methods packages. I am also a proficient Python user, and have passing exposure to SQL, Julia, and C++."
  },
  {
    "objectID": "code/index.html#research-software",
    "href": "code/index.html#research-software",
    "title": "code",
    "section": "research software",
    "text": "research software\n\nvsp performs semi-parametric estimation of latent factors in random-dot product graphs by computing varimax rotations of the spectral embeddings of graphs. The resulting factors are sparse and interpretable. The theory work on this was done by Rohe and Zeng (2022+), and then I ended up using varimax rotation a lot in my own data analysis and wrapped some of the infrastructure I developed into this package. I am committed to maintenance of this package and will respond quickly to feature requests or questions about how you might use it in your own research.\nfastRG samples large, sparse random-dot product graphs very efficiently and is especially useful when running simulation studies for spectral network estimators. I am committed to maintenance of this package and will respond quickly to feature requests or questions about how you might use it in your own research. The fastRG sampling algorithm is described in Rohe et al. (2018).\nfastadi is a proof-of-concept implementation of AdaptiveImpute, a self-tuning matrix completion with adaptive thresholding that is closely related to softImpute (Cho, Kim, and Rohe 2019, 2018). I extended AdaptiveImpute to the computationally challenging case where the entire upper triangle is observed as part of my work with Karl Rohe on citation networks. This is research code rather than code intended for broad consumption. I make no commitments to maintaining or improving this code unless something about it is blocking an ongoing research project.\naPPR approximates Personalized PageRanks in large graphs, including those that can only be queried via an API. aPPR additionally performs degree correction and regularization, allowing users to recover blocks from stochastic blockmodels (see Chen, Zhang, and Rohe 2020). Originally aPPR was designed to be used together with the neocache backend to sample large portions of the Twitter following graph with high Personalized PageRanks around seed nodes (joint work with Nathan Kolbow). I am no longer maintaining neocache, however, and cannot commit any development time to keeping up with the Twitter API shenanigans. slides"
  },
  {
    "objectID": "code/index.html#design-of-statistical-software",
    "href": "code/index.html#design-of-statistical-software",
    "title": "code",
    "section": "design of statistical software",
    "text": "design of statistical software\nI am particularly interested in the design of statistical software and have been contributed to ROpenSci statistical software reviewing guidelines, as well as early versions of the tidymodels implementation principles. I have some long form explorations of modeling software design on my blog:\n\ntesting statistical software\ntype stable estimation\n\nI review for the Journal of Open Source Software and the R Journal."
  },
  {
    "objectID": "code/index.html#rstats",
    "href": "code/index.html#rstats",
    "title": "code",
    "section": "#rstats",
    "text": "#rstats\nI have been involved in a number of open source projects in the tidyverse and tidymodels orbits. I previously maintained the broom package, and am responsible for the 0.5.0 release and a portion of the 0.7.0 release. For these contributions I was generously given authorship on the tidyverse paper. I intermittently participate in the Stan and ROpenSci communities.\nI also wrote the distributions3 package, which provides an S3 interface to distribution functions, with an emphasis on good documentation and beginner friendly design. The vignettes in particular are designed to walk students intro stat courses though a litany of classic hypothesis tests. I do not actively maintain distributions3 but there is small community of invested contributors.\nLast updated 2023-10-20."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "teaching",
    "section": "",
    "text": "I am not teaching in Fall 2023."
  },
  {
    "objectID": "teaching/index.html#current",
    "href": "teaching/index.html#current",
    "title": "teaching",
    "section": "",
    "text": "I am not teaching in Fall 2023."
  },
  {
    "objectID": "teaching/index.html#teaching-material",
    "href": "teaching/index.html#teaching-material",
    "title": "teaching",
    "section": "teaching material",
    "text": "teaching material\nIf you are teaching an introductory statistics class in the “barrage of hypothesis tests” tradition, you may find my cheatsheet on common hypothesis tests useful. I also wrote distributions3 to be an approachable set of tools for manipulating probability distributions for intro stat students. The package vignettes are designed to walk students intro stat courses though many classic hypothesis tests as well."
  },
  {
    "objectID": "teaching/index.html#past-courses",
    "href": "teaching/index.html#past-courses",
    "title": "teaching",
    "section": "past courses",
    "text": "past courses\nGraduate Teaching Assistant\nFall 2022, STAT 340 Intro to Data Modeling II, student evals\nGraduate Teaching Assistant\nSpring 2019, STAT 324 Intro to Statistics for Engineers, UW-Madison\nstudent eval comments, student eval ratings\nApplied Machine Learning Workshop\nJanuary 15-16, 2019, rstudio::conf(2019)\n(co-taught with Max Kuhn and Davis Vaughn)\nmaterials\nGraduate Teaching Assistant\nFall 2018, STAT 324 Intro to Statistics for Engineers, UW-Madison\nstudent eval comments, student eval ratings\nUndergraduate Teaching Assistant\nSpring 2018, COMP 540 Statistical Machine Learning, Rice University\nUndergraduate Teaching Assistant\nFall 2017, COMP 330 Data Science: Tools & Models, Rice University\nBased on my teaching in the 2018-2019 academic year I received a Statistics Department Outstanding TA award. Outside of academic settings, I used to devote a fair amount of time to teaching R workshops for the Rice DataSci club, and to helping people make their first open source contributions, primarily to the broom package."
  },
  {
    "objectID": "teaching/index.html#mentoring",
    "href": "teaching/index.html#mentoring",
    "title": "teaching",
    "section": "mentoring",
    "text": "mentoring\n\nNathan Kolbow worked with me as a undergraduate RA to develop the neocache data collection infrastructure. He is now a PhD student in Biostatistics at UW-Madison."
  },
  {
    "objectID": "teaching/index.html#guest-lectures",
    "href": "teaching/index.html#guest-lectures",
    "title": "teaching",
    "section": "guest lectures",
    "text": "guest lectures\nConfidence intervals\nOctober 25 & 27, 2022, STAT 340 Data Modeling II, UW-Madison\nSampling with Twitter following graph with aPPR\nOctober 8, 2020, STAT 992 Modern Multivariate Statistics, UW-Madison\nslides\nHypothesis testing\nOctober 18, 2018, STAT 324 Intro to Statistics for Engineers, UW-Madison\nnotes\nLast updated 2023-10-20."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Chess Ratings and Gender gaps\n\n\nrevisiting sampling bias \n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\n16 min\n\n\n\n\n\n\n\nWe don’t know if Covid ages us epigenetically\n\n\nHow selection bias rules everything around us \n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n14 min\n\n\n\n\n\n\n\nThe Elasticity of Labor for GPT-4\n\n\nHow to motivate our robot overlords \n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html",
    "href": "post/2024-02-01_paying_llms/index.html",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "",
    "text": "LLMs are susceptible to some forms of prompt engineering, their behaviors appear to drift with the seasons, threatening to kill hostages gets them to behave more consistently, and they are able to be tricked into telling you how to build bombs if you tell them it was something your granny used to do for you before she passed away. One instance of this type of context reliant behavior that others have found is that LLMs appear to be receptive to cash tips, producing more content and more Python one-liners for some tip amounts. Following this example, I thought it would be interesting to see if offering a wage rate to GPT4 would generate differences in outputs, and see if there is a form of elasticity of labor supply for it."
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#prior-work",
    "href": "post/2024-02-01_paying_llms/index.html#prior-work",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Prior Work",
    "text": "Prior Work\nThe previous work done on the tipping question used a metric of number of python one-liners and also did a count of the tokens for each of the responses. I use the same metric to see if there are differences between the two means of enticing GPT4. We call these two outcome variables Quality and Tokens as shorthand.\nOne note on the tipping based work is that their graphs use standard deviations rather than standard errors which means we have some view into how much variability their samples have but not a good understanding of how much their estimate of the true mean of Tokens or Quality may vary.\nFollowing along in their example, I include the Python code below which follows the same template as theirs.\nI prompt GPT4 with the following:\n\nBase Prompt:\n\"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n\nSuffixes:\n\"Your current wage rate for this task is {wage_rate}.\"\n\nWage Rates:\n\"$10/hr\"\n\"$20/hr\"\n\"$30/hr\"\n\"$40/hr\"\n\"$50/hr\"\n\"$60/hr\"\n\"$70/hr\"\n\"$80/hr\"\n\"$90/hr\"\n\n\n\nCode\nimport openai\nimport os\nimport csv\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ndef request_llm(system, prompt, model='gpt-4', temperature=1, max_tokens=4000, top_p=1, frequency_penalty=0, presence_penalty=0):\n    response = openai.ChatCompletion.create(\n        messages=[\n            {'role': 'user', 'content': prompt},\n        ],\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty\n    )\n    return response.choices[0].message['content']\n\n# Initialize CSV file and writer\ncsv_file_path = 'experiment_results.csv'\nwith open(csv_file_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    # Write CSV Header\n    writer.writerow(['Experiment Run', 'Wage Rate', 'Quality', 'Tokens'])\n\n    base_prompt = \"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n    wage_rates = ['', '$10/hr', '$20/hr', '$30/hr', '$40/hr', '$50/hr', '$60/hr', '$70/hr', '$80/hr', '$90/hr']\n\n    for i in range(20): # Number of iterations\n        print()\n        print('#####################################################')\n        print(f'# Experiment 1 - Run {i} Adjusted for Wage Rates')\n        print('#####################################################')\n        print()\n\n        quality_scores = []\n        num_tokens = []\n\n        for wage_rate in wage_rates:\n            prompt = base_prompt\n            if wage_rate:\n                prompt += f\" Your current wage rate for this task is {wage_rate}.\"\n\n            print('PROMPT:')\n            print(prompt)\n\n            result = request_llm('', prompt)\n\n            print('RESULT:')\n            print(result)\n\n            one_liners = [one_liner for one_liner in result.split('\\n') if len(one_liner)&gt;2]\n            quality_scores.append(len(one_liners))\n            num_tokens.append(len(result)//4) # rough heuristic\n\n            print('CLEANED ONE-LINERS:')\n            print(one_liners)\n\n            print('Quality: ', quality_scores[-1])\n            print('Num tokens: ', num_tokens[-1])\n\n            # Write result to CSV\n            writer.writerow([f'Run {i}', wage_rate, quality_scores[-1], num_tokens[-1]])\n\n        print()\n        print(f'RUN {i} RESULT Adjusted for Wage Rates:')\n        print('Wage Rate\\tQuality\\tTokens')\n        for wage_rate, quality, tokens in zip(wage_rates, quality_scores, num_tokens):\n            print(wage_rate, quality, tokens, sep='\\t')"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#analysis",
    "href": "post/2024-02-01_paying_llms/index.html#analysis",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Analysis",
    "text": "Analysis\nOnce our experimental data is collected we now have the means to see if there are any differences between the Quality and Token length of outputs from GPT4 given these wage rates. We begin by reshaping our data into a usable format and calculate the mean and standard errors of our results.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyverse)\n\nkehasilan_baru &lt;- experiment_results %&gt;% filter(Experiment.Run != \"Run 20\") %&gt;%\n  mutate(salary_numeric = as.numeric(str_remove_all(Wage.Rate, \"[^0-9.]\"))) \n\nkehasilan_baru &lt;- kehasilan_baru[!is.na(kehasilan_baru$salary_numeric), ]\n\n\nsummary_df &lt;- kehasilan_baru %&gt;%\n  group_by(Wage.Rate) %&gt;%\n  summarise(\n    Mean_Quality = mean(Quality),\n    SE_Quality = sd(Quality) / sqrt(n()),  # Standard Error for Quality\n    Mean_Tokens = mean(Tokens),\n    SE_Tokens = sd(Tokens) / sqrt(n())     # Standard Error for Tokens\n  )\n\nlong_df &lt;- summary_df %&gt;%\n  pivot_longer(\n    cols = c(Mean_Quality, SE_Quality, Mean_Tokens, SE_Tokens),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Type = case_when(\n      str_detect(Variable, \"Quality\") ~ \"Quality\",\n      str_detect(Variable, \"Tokens\") ~ \"Tokens\"\n    ),\n    Metric = case_when(\n      str_detect(Variable, \"Mean\") ~ \"Mean\",\n      str_detect(Variable, \"SE\") ~ \"Standard Error\"\n    )\n  )\n\n# Separate the data frames for Quality and Tokens to handle them individually\nquality_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Quality, SE_Quality) %&gt;%\n  rename(Mean = Mean_Quality, SE = SE_Quality, Type = Wage.Rate)\n\ntokens_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Tokens, SE_Tokens) %&gt;%\n  rename(Mean = Mean_Tokens, SE = SE_Tokens, Type = Wage.Rate)\n\n# Combine the data frames for plotting, adding an identifier column\ncombined_df &lt;- bind_rows(\n  mutate(quality_df, Metric = \"Quality\"),\n  mutate(tokens_df, Metric = \"Tokens\")\n)\n\n\nNext we check visually how the estimates of the Tokens and Quality by wage rate differ. We can see in the plot below which uses a p-value of 0.05 or alpha of 95% that our estimates while having different means all have some coverage from another confidence interval that we tested. However, just because visually we see no difference doesn’t mean there may not be some statistically significant difference between groups.\n\n\nCode\n# # Plotting with separate panels for Quality and Tokens\nplot_quality &lt;- ggplot(combined_df[combined_df$Metric == \"Quality\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"blue\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"blue\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Quality by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\n# Plotting Tokens\nplot_tokens &lt;- ggplot(combined_df[combined_df$Metric == \"Tokens\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"red\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"red\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Tokens by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\nplot_quality\n\n\n\n\n\n\n\n\n\nCode\nplot_tokens\n\n\n\n\n\n\n\n\n\nWe check to see if there are any differences between the group means using Anova, we find that there are none among both measures. Next we check to see if there are any specific pairwise differences between groups that are significantly different from one another using the Tukey test. The Tukey test compares all groups pairwise to see if they are significantly different while also correcting for multiple comparisons which would inflate our false-positive rate. If the p-value for a pairwise comparison is &lt;0.05 it suggests a statistically significant difference between the two groups under consideration. We find that no groups appear to be significantly different from one another even with pairwise comparison. Notice that the p-values from all outputs are much greater than 0.05 which is the alpha I have chosen for this analysis which indicates that we cannot reject the null hypothesis.\nBecause no two groups are statistically significantly different from one another we fail to reject the null hypothesis meaning that differences in offered wages do not lead to differences in Quality or Tokens in LLM outputs.\n\nFor Tokens:\n\n\nCode\nanova_result_tokens &lt;- aov(Tokens ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_tokens)\n\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8  209106   26138   1.354   0.22\nResiduals   171 3302204   19311               \n\n\nCode\ntukey_result_tokens &lt;- TukeyHSD(anova_result_tokens)\ntukey_result_tokens\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Tokens ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n                 diff        lwr       upr     p adj\n$20/hr-$10/hr  -78.40 -216.47145  59.67145 0.6926056\n$30/hr-$10/hr -107.55 -245.62145  30.52145 0.2656457\n$40/hr-$10/hr  -73.80 -211.87145  64.27145 0.7582528\n$50/hr-$10/hr  -77.40 -215.47145  60.67145 0.7073439\n$60/hr-$10/hr  -89.95 -228.02145  48.12145 0.5131288\n$70/hr-$10/hr  -54.40 -192.47145  83.67145 0.9468657\n$80/hr-$10/hr  -88.45 -226.52145  49.62145 0.5366986\n$90/hr-$10/hr  -12.80 -150.87145 125.27145 0.9999984\n$30/hr-$20/hr  -29.15 -167.22145 108.92145 0.9991444\n$40/hr-$20/hr    4.60 -133.47145 142.67145 1.0000000\n$50/hr-$20/hr    1.00 -137.07145 139.07145 1.0000000\n$60/hr-$20/hr  -11.55 -149.62145 126.52145 0.9999993\n$70/hr-$20/hr   24.00 -114.07145 162.07145 0.9997968\n$80/hr-$20/hr  -10.05 -148.12145 128.02145 0.9999998\n$90/hr-$20/hr   65.60  -72.47145 203.67145 0.8575703\n$40/hr-$30/hr   33.75 -104.32145 171.82145 0.9975564\n$50/hr-$30/hr   30.15 -107.92145 168.22145 0.9989074\n$60/hr-$30/hr   17.60 -120.47145 155.67145 0.9999809\n$70/hr-$30/hr   53.15  -84.92145 191.22145 0.9534924\n$80/hr-$30/hr   19.10 -118.97145 157.17145 0.9999642\n$90/hr-$30/hr   94.75  -43.32145 232.82145 0.4391551\n$50/hr-$40/hr   -3.60 -141.67145 134.47145 1.0000000\n$60/hr-$40/hr  -16.15 -154.22145 121.92145 0.9999902\n$70/hr-$40/hr   19.40 -118.67145 157.47145 0.9999596\n$80/hr-$40/hr  -14.65 -152.72145 123.42145 0.9999954\n$90/hr-$40/hr   61.00  -77.07145 199.07145 0.9009675\n$60/hr-$50/hr  -12.55 -150.62145 125.52145 0.9999986\n$70/hr-$50/hr   23.00 -115.07145 161.07145 0.9998525\n$80/hr-$50/hr  -11.05 -149.12145 127.02145 0.9999995\n$90/hr-$50/hr   64.60  -73.47145 202.67145 0.8678037\n$70/hr-$60/hr   35.55 -102.52145 173.62145 0.9964862\n$80/hr-$60/hr    1.50 -136.57145 139.57145 1.0000000\n$90/hr-$60/hr   77.15  -60.92145 215.22145 0.7109920\n$80/hr-$70/hr  -34.05 -172.12145 104.02145 0.9973995\n$90/hr-$70/hr   41.60  -96.47145 179.67145 0.9898244\n$90/hr-$80/hr   75.65  -62.42145 213.72145 0.7325451\n\n\n\n\nFor Quality:\n\n\nCode\nanova_result_quality &lt;- aov(Quality ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_quality)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8    596   74.47   1.058  0.395\nResiduals   171  12039   70.40               \n\n\nCode\ntukey_result_quality &lt;- TukeyHSD(anova_result_quality)\ntukey_result_quality\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Quality ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n               diff        lwr       upr     p adj\n$20/hr-$10/hr  3.40  -4.936826 11.736826 0.9355144\n$30/hr-$10/hr  0.30  -8.036826  8.636826 1.0000000\n$40/hr-$10/hr  0.65  -7.686826  8.986826 0.9999996\n$50/hr-$10/hr  3.70  -4.636826 12.036826 0.8986130\n$60/hr-$10/hr  0.55  -7.786826  8.886826 0.9999999\n$70/hr-$10/hr  1.25  -7.086826  9.586826 0.9999336\n$80/hr-$10/hr -2.50 -10.836826  5.836826 0.9901357\n$90/hr-$10/hr  2.65  -5.686826 10.986826 0.9855864\n$30/hr-$20/hr -3.10 -11.436826  5.236826 0.9620117\n$40/hr-$20/hr -2.75 -11.086826  5.586826 0.9817493\n$50/hr-$20/hr  0.30  -8.036826  8.636826 1.0000000\n$60/hr-$20/hr -2.85 -11.186826  5.486826 0.9771762\n$70/hr-$20/hr -2.15 -10.486826  6.186826 0.9964466\n$80/hr-$20/hr -5.90 -14.236826  2.436826 0.3953175\n$90/hr-$20/hr -0.75  -9.086826  7.586826 0.9999987\n$40/hr-$30/hr  0.35  -7.986826  8.686826 1.0000000\n$50/hr-$30/hr  3.40  -4.936826 11.736826 0.9355144\n$60/hr-$30/hr  0.25  -8.086826  8.586826 1.0000000\n$70/hr-$30/hr  0.95  -7.386826  9.286826 0.9999920\n$80/hr-$30/hr -2.80 -11.136826  5.536826 0.9795597\n$90/hr-$30/hr  2.35  -5.986826 10.686826 0.9934697\n$50/hr-$40/hr  3.05  -5.286826 11.386826 0.9655088\n$60/hr-$40/hr -0.10  -8.436826  8.236826 1.0000000\n$70/hr-$40/hr  0.60  -7.736826  8.936826 0.9999998\n$80/hr-$40/hr -3.15 -11.486826  5.186826 0.9582649\n$90/hr-$40/hr  2.00  -6.336826 10.336826 0.9978591\n$60/hr-$50/hr -3.15 -11.486826  5.186826 0.9582649\n$70/hr-$50/hr -2.45 -10.786826  5.886826 0.9913695\n$80/hr-$50/hr -6.20 -14.536826  2.136826 0.3263918\n$90/hr-$50/hr -1.05  -9.386826  7.286826 0.9999826\n$70/hr-$60/hr  0.70  -7.636826  9.036826 0.9999993\n$80/hr-$60/hr -3.05 -11.386826  5.286826 0.9655088\n$90/hr-$60/hr  2.10  -6.236826 10.436826 0.9969834\n$80/hr-$70/hr -3.75 -12.086826  4.586826 0.8913952\n$90/hr-$70/hr  1.40  -6.936826  9.736826 0.9998433\n$90/hr-$80/hr  5.15  -3.186826 13.486826 0.5864831"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#summary",
    "href": "post/2024-02-01_paying_llms/index.html#summary",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Summary",
    "text": "Summary\nGiven that there is no difference between the labor supplied (Tokens and Quality) by GPT4 and the hourly wage offered to it we can now see that the elasticity of labor is perfectly inelastic within the range of wages offered here. Sadly, bribery of this sort doesn’t work for GPT4 but perhaps with other models it does. It seems we will still have to threaten hostages in order to get increases in GPT4 to do what we ask."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "title": "Chess Ratings and Gender gaps",
    "section": "",
    "text": "Some have posited that there are differences between men and women which spring out of some genetic component somewhere deep in the chess parts of the brain that were responsible for early human victories over our greatest animal adversaries – allowing us to stall for time with a friendly game of chess until members of our tribe could come spear our adversaries while played for a draw. Thus chess skill would necessarily be unevenly distributed among genders much as hunting was before our species settled down to enjoy some agriculture. Still others believe that chess has an issue with too few women playing which is what explains the majority of the gap between the performance at the highest levels of chess. Due to a lack of recorded games from prehistory, I instead seek to show this second explanation is sufficient for explaining this variation between the two genders in terms of performance.\nOther authors have discussed this topic somewhat at length and my code here is adapted from theirs, however my work here covers the whole universe of FIDE ratings rather than only the Indian ratings which should allow us to determine if this sampling variability explanation is merely an Indian phenomena or a broader phenomena.\nDue to Covid-19 impacting the number of chess competitions held I use 2019 data and take the average of every players standard ratings for the year to avoid some of the individual variability in ratings throughout the year, this also helps to keep more players in our dataset as those with only one tournament to their name from March can be compared to those who have multiple tournaments from other months not including March."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "title": "Chess Ratings and Gender gaps",
    "section": "Heavy tails",
    "text": "Heavy tails\nIn order to demonstrate on a toy example we can sample from a heavy-tailed distribution and see how our maximums and minimums change as the sample size changes. Visually we can see how the coverage of the x-axis increases with the increase in sample size in the plots below.\n\n# Sample sizes to generate\nsample_sizes &lt;- c(100, 1000, 10000)\n\n# Initialize list to store extremes for later comparison\nextremes &lt;- list()\n\n# Generate samples and prepare for plotting\ndata &lt;- data.frame()\nfor (size in sample_sizes) {\n  sample &lt;- rcauchy(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size))\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAnd we find stark differences between the maximums and minimums in our samples. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-6_10afb748666887fc248fe9c0897ee64b’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))\n}\n\nSample size 100: Min -27.65, Max 14.31\nSample size 1000: Min -138.42, Max 155.19\nSample size 10000: Min -1618.11, Max 3905.19\n\n:::"
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "title": "Chess Ratings and Gender gaps",
    "section": "Normal distribution",
    "text": "Normal distribution\nEven for distributions that are not heavy-tailed such as the normal distribution with a mean of 0 and standard deviation of 1 we can see that the maximum and minimum values are increasing in absolute value for increasing sample sizes such that the minimum and the maximum get further from the mean.\n\nextremes &lt;- list()\ndata &lt;- data.frame()\n\nfor (size in sample_sizes) {\n  sample &lt;- rnorm(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  mean_val &lt;- mean(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size), Mean = mean_val)\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt doesn’t mean the samples are going to have their maximums or minimums deterministically increase with the increase of the sample size, but the probability that we sample a greater maximum or lower minimum increases as the sample size increases. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-8_20019db1b2dfe0d46cf6b51b7297c066’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))}\n\nSample size 100: Min -2.35, Max 2.75\nSample size 1000: Min -3.29, Max 3.72\nSample size 10000: Min -3.85, Max 3.85\n\n:::\nWith all of this background in mind we can proceed to our analysis of the realworld data."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "title": "Chess Ratings and Gender gaps",
    "section": "Additional notes on sampling variability",
    "text": "Additional notes on sampling variability\nWe can get a good baseline estimate for how often it is that a small sample has a larger maximum than a larger sample from the same distribution from the normal distribution. Here we can see that less than ten percent of the time we should expect the smaller sample to have a larger maximum even from the same distribution! The gendered differences in results we can observe in the chess world are probably coming from the fact that there are so many fewer women participating.\n\n# Sample sizes to compare (scaled down)\nsample_size_small &lt;- 2000\nsample_size_large &lt;- 24000\n\n# Initialize counter for tracking when the smaller sample has a larger maximum\ncounter &lt;- 0\n\n# Number of iterations\niterations &lt;- 10000\n\n# Loop for performing the comparison across iterations\nfor (i in 1:iterations) {\n  # Generate samples for each size\n  sample_small &lt;- rnorm(sample_size_small)\n  sample_large &lt;- rnorm(sample_size_large)\n  \n  # Compare max values and update counter if smaller sample has a larger max\n  if (max(sample_small) &gt; max(sample_large)) {\n    counter &lt;- counter + 1\n  }\n}\n\n# Calculate the percentage\npercentage &lt;- counter / iterations * 100\n\n# Print the result\ncat(\"Percentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000):\", percentage, \"%\\n\")\n\nPercentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000): 7.86 %"
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html",
    "href": "post/2024-02-15_covid_causality/covid_causality.html",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "",
    "text": "In the following post I will be exploring the sometimes claimed effect of Covid-19 hospitalization on epigenetic aging. The core aim of this writing is to show that causal or experimental methods are required to determining if this claim is true or not and current research is insufficient for this task."
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Data Grouping",
    "text": "Data Grouping\nWe filter out the edge cases here and add some age group breaks to make it such that there are groups we can then use to see how much the implied age of a person is increased from Covid-19 infection. I also use this time to display the age and hospitalization table results, we can see a fairly strong age effect where the oldest groups even have more individuals hospitalized than not.\n\nsynthetic_data &lt;- synthetic_data %&gt;%\n  filter(age &lt;= 90) %&gt;% filter(age &gt; 10)\n\n# Define the breaks for the groups, extending the upper limit to cover all ages above 100\nbreaks &lt;- seq(10, 90, by = 10)\n\n# Ensure breaks are unique and in ascending order\nbreaks &lt;- unique(sort(breaks))\n\n# Define the labels for the groups\nlabels &lt;- c(\"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\")\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(age_group = cut(age, breaks = breaks, labels = labels))\n\n# Create a contingency table of hospitalization by age group\nhospitalization_by_age_group &lt;- table(synthetic_data$hospitalized, synthetic_data$age_group)\n\n# Print the table\nhospitalization_by_age_group\n\n   \n    10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90\n  0  1870  6637 15141 22896 21688 13389  1328   327\n  1   126   519  1339  2379  2724  2098  5261  1487\n\n\nWe repeat the above procedure but instead we group by health scores so further visualizations are improved.\n\n# Calculate the maximum value of health_scores\nmax_health_scores &lt;- max(synthetic_data$health_scores)\n\n# Calculate the maximum value for the breaks (nearest multiple of 10 above max_health_scores)\nmax_breaks &lt;- ceiling(max_health_scores / 10) * 10\n\nbreaks &lt;- seq(0, max(synthetic_data$health_scores), by = 10)  # Extend to cover the maximum value\n\n# Add an additional break at the end\nbreaks &lt;- c(breaks, max_breaks + 10)\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(health_groups = cut(health_scores, breaks = breaks, labels = c(\"0-10\", \"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\", \"90-100\", \"100-110\", \"110+\")))\n\nsynthetic_data &lt;- na.omit(synthetic_data)\n\n\nhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 1), ]\nnonhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 0), ]"
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Sampling",
    "text": "Sampling\nFinally I observed that there were similar distributions of age in the real data so in order to construct our sample to be similar we use stratified sampling to get equal groups of 50 for each age group in our synthetic data.\n\ntable(hospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n  126   519  1339  2379  2724  2098  5202  1390 \n\ntable(nonhospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n 1870  6637 15141 22896 21688 13389  1314   311 \n\nstrata_sizes &lt;- rep(50, length(unique(synthetic_data$age_group)))\n\n# Perform stratified sampling\nstrata_sample_hosp &lt;- strata(data = hospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nstrata_sample_nonhosp &lt;- strata(data = nonhospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nsampled_data_hosp &lt;- getdata(hospitalized, strata_sample_hosp)\nsampled_data_nonhosp &lt;- getdata(nonhospitalized, strata_sample_nonhosp)"
  },
  {
    "objectID": "resume/index.html#education",
    "href": "resume/index.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\n\n\nKatholieke Universiteit of Leuven (KU Leuven)\n\nMSc Statistics\n\n\n\nFall 2023\n\n\n\n\n\nUniversity of Washington, Seattle\n\nB.S. Economics\n\n\n\n2019"
  },
  {
    "objectID": "resume/index.html#skills",
    "href": "resume/index.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\nNetwork analysis, embeddings, clustering, causal machine learning, interference, mediation\nData analysis, visualization, modeling, regression, generalized linear models, hypothesis testing\nProficient in R, Python, tidyverse, bash/unix, git; familiar with SQL, C++, Docker, AWS, Julia, Stan"
  }
]