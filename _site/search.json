[
  {
    "objectID": "code/index.html",
    "href": "code/index.html",
    "title": "code",
    "section": "",
    "text": "My research code, as well as miscellaneous personal projects in various stages of completion, lives on my Github. I primarily use R, and most of my work as a developer is on methods packages. I am also a proficient Python user, and have passing exposure to SQL, Julia, and C++."
  },
  {
    "objectID": "code/index.html#research-software",
    "href": "code/index.html#research-software",
    "title": "code",
    "section": "research software",
    "text": "research software\n\naPPR approximates Personalized PageRanks in large graphs, including those that can only be queried via an API, such as the Twitter following graph. aPPR additionally performs degree correction and regularization, allowing users to recover blocks from stochastic blockmodels (see Chen, Zhang, and Rohe 2020). Additionally you can combine aPPR with the neocache backend to sample large portions of the Twitter following graph with high Personalized PageRanks around seed nodes (joint work with Nathan Kolbow). I strongly believe that researchers are not analyzing the Twitter following graph enough and I am happy to help you use these packages to collect and analyze that data. Sometimes the code does go stale due to changes in the Twitter API – let me know when this happens and I’ll push a bugfix as fast as I can. slides\nvsp performs semi-parametric estimation of latent factors in random-dot product graphs by computing varimax rotations of the spectral embeddings of graphs. The resulting factors are sparse and interpretable. The theory work on this was done by Rohe and Zeng (2022+), and then I ended up using varimax rotation a lot in my own data analysis and wrapped some of the infrastructure I developed in this package. I am committed to maintenance of this package and will respond quickly to feature requests or questions about how you might use it in your own research.\nfastRG samples large, sparse random-dot product graphs very efficiently and is especially useful when running simulation studies for spectral network estimators. I am committed to maintenance of this package and will respond quickly to feature requests or questions about how you might use it in your own research. The fastRG sampling algorithm is described in Rohe et al. (2018).\nfastadi is a proof-of-concept implementation of AdaptiveImpute, a self-tuning matrix completion with adaptive thresholding that is closely related to softImpute (Cho, Kim, and Rohe 2019, 2018). I extended AdaptiveImpute to the computationally challenging case where the entire upper triangle is observed as part of my work with Karl Rohe on citation networks. This is research code rather than code intended for broad consumption. I make no commitments to maintaining or improving this code unless something about it is blocking on an ongoing research project."
  },
  {
    "objectID": "code/index.html#design-of-statistical-software",
    "href": "code/index.html#design-of-statistical-software",
    "title": "code",
    "section": "design of statistical software",
    "text": "design of statistical software\nI am particularly interested in the design of statistical software and have been contributed to ROpenSci statistical software reviewing guidelines, as well as early versions of the tidymodels implementation principles. I have some long form explorations of modeling software design on my blog:\n\ntesting statistical software\ntype stable estimation\n\nI review for the Journal of Open Source Software and the R Journal."
  },
  {
    "objectID": "code/index.html#rstats",
    "href": "code/index.html#rstats",
    "title": "code",
    "section": "#rstats",
    "text": "#rstats\nI have been involved in a number of open source projects in the tidyverse and tidymodels orbits. I previously maintained the broom package, and am responsible for the 0.5.0 release and a portion of the 0.7.0 release. For these contributions I was generously given authorship on the tidyverse paper. I intermittently participate in the Stan and ROpenSci communities.\nI also wrote the distributions3 package, which provides an S3 interface to distribution functions, with an emphasis on good documentation and beginner friendly design. The vignettes in particular are designed to walk students intro stat courses though a litany of classic hypothesis tests. I do not actively maintain distributions3 but there is small community of invested contributors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about",
    "section": "",
    "text": "I primarily use this website to write blog posts on topics of passing interest. These days I’m particularly interested in the foundations of statistics and statistical software design.\nI am responsive on Twitter at @alexpghayes and via email.\nLast updated on 2022-04-28."
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "papers",
    "section": "",
    "text": "I primarily study networks using tools from multivariate analysis.\nMy work with Karl Rohe has focused on fast approaches to spectral estimation via sparse linear algebra. Our primary project together has been developing a method for network co-factor analysis for settings with missing edge data, which we applied to a large network of citations between statistics papers. We have spent some time working to understand effective regularization strategies for spectral estimators, as well as developing diagnostic tools for PCA and varimax rotation. As part of an outgrowth of Karl’s murmuration project I developed extensive infrastructure to sample and analyze the Twitter following graph (see code), and Twitter data remains near and dear to my heart.\nI am currently working with Keith Levin on causal interpretations of network regression, and with Jiwei Zhao on semi-parametric inference for experimental guardrails in a data fusion setting.\nPreviously at Facebook, I developed internal tooling to understand post content using neural hypergraph embeddings. At Facebook I also developed a diagnostic to assess the out-of-sample reliability of rolling classifiers based on differential calibration of the classifier over time.\nI keep Google Scholar up to date, and post any research related code to Github (personal, lab)."
  },
  {
    "objectID": "papers/index.html#publications",
    "href": "papers/index.html#publications",
    "title": "papers",
    "section": "publications",
    "text": "publications\n\nWelcome to the Tidyverse. Hadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, Alex Hayes, Lionel Henry, Jim Hester, Max Kuhn, Thomas Lin Pedersen, Evan Miller, Kirill Müller, David Robinson, Dana Paige Seidel, Vitalie Spinu, Kohske Takahashi, Davis Vaughan, Claus Wilke, Kara Woo, Hiroaki Yutani. Journal of Open Source Software, 2019. pdf\n\nLast updated on 2022-04-27."
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html",
    "title": "gentle tidy eval with examples",
    "section": "",
    "text": "I’ve been using the tidy eval framework introduced with dplyr 0.7 for about two months now, and it’s time for an update to my original post on tidy eval. My goal is not to explain tidy eval to you, but rather to show you some simple examples that you can easily generalize from."
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#using-strings-to-refer-to-column-names",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#using-strings-to-refer-to-column-names",
    "title": "gentle tidy eval with examples",
    "section": "Using strings to refer to column names",
    "text": "Using strings to refer to column names\nTo refer to columns in a data frame with strings, we need to convert those strings into symbol objects with rlang::sym and rlang::syms. We then use the created symbol objects in dplyr functions with the prefixes !! and !!!. This is because dplyr verbs expect input that looks like code. Using the sym/syms functions we can convert strings into objects that look like code.\n\nmass <- rlang::sym(\"mass\")                        # create a single symbol\ngroups <- rlang::syms(c(\"homeworld\", \"species\"))  # create a list of symbols\n\nstarwars %>%\n  group_by(!!!groups) %>%               # use list of symbols with !!!\n  summarize(avg_mass = mean(!!mass))    # use single symbol with !!\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   <chr>          <chr>        <dbl>\n 1 Alderaan       Human         NA  \n 2 Aleen Minor    Aleena        15  \n 3 Bespin         Human         79  \n 4 Bestine IV     Human        110  \n 5 Cato Neimoidia Neimodian     90  \n 6 Cerea          Cerean        82  \n 7 Champala       Chagrian      NA  \n 8 Chandrila      Human         NA  \n 9 Concord Dawn   Human         79  \n10 Corellia       Human         78.5\n# … with 48 more rows\n\n\nThe usage mass <- rlang::sym(\"mass\") is Hadley approved:\n\n\n{{% tweet \"885993307968593920\" %}}\n\n\nI believe it is also the current tidyverse code style standard. We use rlang::sym and rlang::syms identically inside functions.\n\nsummarize_by <- function(df, groups, to_summarize) {\n  df %>%\n    group_by(!!!rlang::syms(groups)) %>%\n    summarize(summarized_mean = mean(!!rlang::sym(to_summarize)))\n}\n\nsummarize_by(starwars, c(\"homeworld\", \"species\"), \"mass\")\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   summarized_mean\n   <chr>          <chr>               <dbl>\n 1 Alderaan       Human                NA  \n 2 Aleen Minor    Aleena               15  \n 3 Bespin         Human                79  \n 4 Bestine IV     Human               110  \n 5 Cato Neimoidia Neimodian            90  \n 6 Cerea          Cerean               82  \n 7 Champala       Chagrian             NA  \n 8 Chandrila      Human                NA  \n 9 Concord Dawn   Human                79  \n10 Corellia       Human                78.5\n# … with 48 more rows"
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#details-about-unquoting",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#details-about-unquoting",
    "title": "gentle tidy eval with examples",
    "section": "Details about unquoting",
    "text": "Details about unquoting\n!! and !!! are syntactic sugar on top of the functions UQ() and UQS(), respectively. It used to be that !! and !!! had low operator precedence, meaning that in terms of PEMDAS they came pretty much last. But now we can use them more intuitively:\n\nhomeworld <- rlang::sym(\"homeworld\")\n\nfilter(starwars, !!homeworld == \"Alderaan\")\n\n# A tibble: 3 × 14\n  name   hei…¹  mass hai…² ski…³ eye…⁴ bir…⁵ sex   gen…⁶ hom…⁷ spe…⁸ films veh…⁹\n  <chr>  <int> <dbl> <chr> <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <lis> <lis>\n1 Leia …   150    49 brown light brown    19 fema… femi… Alde… Human <chr> <chr>\n2 Bail …   191    NA black tan   brown    67 male  masc… Alde… Human <chr> <chr>\n3 Raymu…   188    79 brown light brown    NA male  masc… Alde… Human <chr> <chr>\n# … with abbreviated variable names ¹​height, ²​hair_color, ³​skin_color,\n#   ⁴​eye_color, ⁵​birth_year, ⁶​gender, ⁷​homeworld, ⁸​species, ⁹​vehicles, and 1\n#   more variable: starships <list>\n\n\nWe can also use UQ and UQS directly to be explicit about what we’re unquoting.\n\nfilter(starwars, UQ(homeworld) == \"Alderaan\")\n\n# A tibble: 3 × 14\n  name   hei…¹  mass hai…² ski…³ eye…⁴ bir…⁵ sex   gen…⁶ hom…⁷ spe…⁸ films veh…⁹\n  <chr>  <int> <dbl> <chr> <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <lis> <lis>\n1 Leia …   150    49 brown light brown    19 fema… femi… Alde… Human <chr> <chr>\n2 Bail …   191    NA black tan   brown    67 male  masc… Alde… Human <chr> <chr>\n3 Raymu…   188    79 brown light brown    NA male  masc… Alde… Human <chr> <chr>\n# … with abbreviated variable names ¹​height, ²​hair_color, ³​skin_color,\n#   ⁴​eye_color, ⁵​birth_year, ⁶​gender, ⁷​homeworld, ⁸​species, ⁹​vehicles, and 1\n#   more variable: starships <list>"
  },
  {
    "objectID": "post/2017-08-07_gentle-tidy-eval/index.html#creating-non-standard-functions",
    "href": "post/2017-08-07_gentle-tidy-eval/index.html#creating-non-standard-functions",
    "title": "gentle tidy eval with examples",
    "section": "Creating non-standard functions",
    "text": "Creating non-standard functions\nSometimes it is nice to write functions that use accept non-standard inputs, like dplyr verbs. For example, we might want to write a function with the same effect as\n\nstarwars %>% \n  group_by(homeworld, species) %>% \n  summarize(avg_mass = mean(mass))\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   <chr>          <chr>        <dbl>\n 1 Alderaan       Human         NA  \n 2 Aleen Minor    Aleena        15  \n 3 Bespin         Human         79  \n 4 Bestine IV     Human        110  \n 5 Cato Neimoidia Neimodian     90  \n 6 Cerea          Cerean        82  \n 7 Champala       Chagrian      NA  \n 8 Chandrila      Human         NA  \n 9 Concord Dawn   Human         79  \n10 Corellia       Human         78.5\n# … with 48 more rows\n\n\nTo this we need to capture our input in quosures with quo and quos when programming interactively.\n\ngroups <- quos(homeworld, species)   # capture a list of variables as raw input\nmass <- quo(mass)                    # capture a single variable as raw input\n\nstarwars %>% \n  group_by(!!!groups) %>%            # use !!! to access variables from `quos`\n  summarize(avg_mass = sum(!!mass))  # use !! to access the variable in `quo`\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species   avg_mass\n   <chr>          <chr>        <dbl>\n 1 Alderaan       Human           NA\n 2 Aleen Minor    Aleena          15\n 3 Bespin         Human           79\n 4 Bestine IV     Human          110\n 5 Cato Neimoidia Neimodian       90\n 6 Cerea          Cerean          82\n 7 Champala       Chagrian        NA\n 8 Chandrila      Human           NA\n 9 Concord Dawn   Human           79\n10 Corellia       Human          157\n# … with 48 more rows\n\n\nThere’s some nice symmetry here in that we unwrap both rlang::sym and quo with !! and both rlang::syms and quos with !!!.\nWe might be interested in using this behavior in a function. To do this we replace calls to quo with calls to enquo.\n\nsummarize_by <- function(df, to_summarize, ...) {\n\n  to_summarize <- enquo(to_summarize)  # enquo captures a single argument\n  groups <- quos(...)                  # quos captures multiple arguments\n\n  df %>%\n    group_by(!!!groups) %>%                 # unwrap quos with !!!\n    summarize(summ = sum(!!to_summarize))   # unwrap enquo with !!\n}\n\nNow our function call is non-standardized. Note that quos can capture an arbitrary number of arguments, like we have here. So both of the following calls are valid\n\nsummarize_by(starwars, mass, homeworld)\n\n# A tibble: 49 × 2\n   homeworld       summ\n   <chr>          <dbl>\n 1 Alderaan          NA\n 2 Aleen Minor       15\n 3 Bespin            79\n 4 Bestine IV       110\n 5 Cato Neimoidia    90\n 6 Cerea             82\n 7 Champala          NA\n 8 Chandrila         NA\n 9 Concord Dawn      79\n10 Corellia         157\n# … with 39 more rows\n\nsummarize_by(starwars, mass, homeworld, species)\n\n# A tibble: 58 × 3\n# Groups:   homeworld [49]\n   homeworld      species    summ\n   <chr>          <chr>     <dbl>\n 1 Alderaan       Human        NA\n 2 Aleen Minor    Aleena       15\n 3 Bespin         Human        79\n 4 Bestine IV     Human       110\n 5 Cato Neimoidia Neimodian    90\n 6 Cerea          Cerean       82\n 7 Champala       Chagrian     NA\n 8 Chandrila      Human        NA\n 9 Concord Dawn   Human        79\n10 Corellia       Human       157\n# … with 48 more rows\n\n\nFor more details, see the programming with dplyr vignette."
  },
  {
    "objectID": "post/2017-10-18_gradient-checks/index.html",
    "href": "post/2017-10-18_gradient-checks/index.html",
    "title": "numerical gradient checks",
    "section": "",
    "text": "Suppose you have some loss function \\(\\mathcal{L}(\\beta) : \\mathbb{R}^n \\to \\mathbb{R}\\) you want to minimize with respect to some model parameters \\(\\beta\\). You understand how gradient descent works and you have a correct implementation of \\(\\mathcal{L}\\) but aren’t sure if you took the gradient correctly or implemented it correctly in code."
  },
  {
    "objectID": "post/2017-10-18_gradient-checks/index.html#solution",
    "href": "post/2017-10-18_gradient-checks/index.html#solution",
    "title": "numerical gradient checks",
    "section": "Solution",
    "text": "Solution\nWe can compare our implemention of the gradient of \\(\\mathcal{L}\\) to a finite difference approximation of the gradient. Recall that the gradient of \\(\\mathcal{L}\\), \\(\\nabla_\\mathcal{L}\\), in a direction \\(d \\in \\mathbb{R}^n\\) at a point \\(x \\in \\mathbb{R}^n\\) is defined as\n\\[d^T \\nabla_\\mathcal{L}(x) = \\lim_{\\epsilon \\to 0} \\frac{\\mathcal{L}(x + \\epsilon \\cdot d) - \\mathcal{L}(x - \\epsilon \\cdot d)}{2 \\epsilon}\\]\nIf we take \\(\\epsilon\\) to be fixed and small, we can use this formula to approximate the gradient in any direction. By approximating the gradient in each unit direction, we construct an approximation of the gradient of \\(\\mathcal{L}\\) at a particular point \\(x\\)."
  },
  {
    "objectID": "post/2017-10-18_gradient-checks/index.html#example-checking-the-gradient-of-linear-regression",
    "href": "post/2017-10-18_gradient-checks/index.html#example-checking-the-gradient-of-linear-regression",
    "title": "numerical gradient checks",
    "section": "Example: Checking the gradient of linear regression",
    "text": "Example: Checking the gradient of linear regression\nSuppose that we have \\(n = 20\\) data points in \\(\\mathbb{R}^2\\) with responses \\(y \\in \\mathbb{R}\\). Linear regression assumes the responses \\(y\\) are related linearly to the data matrix \\(X\\) via the equation\n\\[y = X \\beta + \\epsilon\\]\nWe want to find an estimate \\(\\hat \\beta\\) that minimizes the sum of squared error of the predicted values \\(\\hat y = X \\hat \\beta\\)\n\\[\\mathcal{L}(\\beta) = \\frac{1}{2n} \\sum_i (y_i - \\hat y_i)^2 = \\frac{1}{2n} \\sum_i (y_i - x_i \\beta)^2 = \\frac{1}{2n} (y - X \\beta)^T (y - X \\beta)\\]\nIn the final step above we recognize that the sum of squared residuals can be written as a dot product. Next we’d like to the gradient of this dot product. There’s a beautiful explanation of how to take the gradient of a quadratic form here. The gradient (in matrix notation) is\n\\[\\nabla_\\mathcal{L}(\\beta) = -\\frac{1}{n} (y - X \\beta)^T X\\]\nWe can now implement an analytical version of \\(\\nabla_\\mathcal{L}(\\beta)\\) and compare it to a finite difference approximation. First we simulate and visualize some data:\n\nlibrary(tidyverse)\n\nn <- 20\nX <- matrix(rnorm(n * 2), ncol = 2)\ny <- rnorm(n)\n\nggplot(NULL, aes(x = X[, 1], y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"One dimension of the simulated data\", x = expression(X[1])) +\n  theme_classic()\n\n\n\n\nNext we implement our loss and gradient functions. We assume the loss function is implemented correctly but want to check the analytical_grad implementation.\n\nloss <- function(beta) {\n  resid <- y - X %*% beta\n  sum(resid^2) / (2 * n)\n}\n\nanalytical_grad <- function(beta) {\n  grad <- -t(y - X %*% beta) %*% X / n\n  as.vector(grad)\n}\n\nTo perform this check, we need get approximate the gradient in a direction \\(d\\):\n\n#' @param f function that takes a single vector argument x\n#' @param x point at which to evaluate derivative of f (vector)\n#' @param d direction in which to take derivative of f (vector)\n#' @param eps epsilon to use in the gradient approximation\nnumerical_directional_grad <- function(f, x, d, eps = 1e-8) {\n  (f(x + eps * d) - f(x - eps * d)) / (2 * eps)\n}\n\nAnd then to approximate the entire gradient, we need to combine directional derivatives in each of the unit directions:\n\nzeros_like <- function(x) {\n  rep(0, length(x))\n}\n\nnumerical_grad <- function(f, x, eps = 1e-8) {\n  grad <- zeros_like(x)\n  for (dim in seq_along(x)) {\n    unit <- zeros_like(x)\n    unit[dim] <- 1\n    grad[dim] <- numerical_directional_grad(f, x, unit, eps)\n  }\n  grad\n}\n    \nrelative_error <- function(want, got) {\n  (want - got) / want  # assumes want is not zero\n}\n\nNow we can check the relative error between our analytical implementation of the gradient and the numerical approximation.\n\nb <- c(2, 3)  # point in parameter space to check gradient at\n\nnum_grad <- numerical_grad(loss, b)\nana_grad <- analytical_grad(b)\n\nnum_grad\n\n[1] 2.112107 3.286946\n\nana_grad\n\n[1] 2.112107 3.286946\n\nrelative_error(num_grad, ana_grad)\n\n[1] -2.810374e-08  1.553777e-08\n\n\nThe relative error is small, and we can feel confident that our implementation of the gradient is correct.\nThis post is based off of Tim Vieira’s fantastic post on how to use numerical gradient checks in practice, but with R code. See also the numDeriv package."
  },
  {
    "objectID": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html",
    "href": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html",
    "title": "predictive performance via bootstrap variants",
    "section": "",
    "text": "When we build a predictive model, we are interested in how the model will perform on data it hasn’t seen before. If we have lots of data, we can split it into training and test sets to assess model performance. If we don’t have lots of data, it’s better to fit a model using all of the available data and to assess its predictive performance using resampling techniques. The bootstrap is one such resampling technique. This post discusses several variants of the bootstrap that are appropriate for estimating predictive performance."
  },
  {
    "objectID": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#a-brief-introduction-to-the-bootstrap",
    "href": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#a-brief-introduction-to-the-bootstrap",
    "title": "predictive performance via bootstrap variants",
    "section": "A brief introduction to the bootstrap",
    "text": "A brief introduction to the bootstrap\nThe bootstrap isn’t a particular procedure so much as a general strategy. We assume that our data comes from some underlying population \\(F\\) that we care about. We’re interested in the sampling distribution of some statistic \\(T\\) that we calculate from the data. In our case \\(T\\) is predictive performance.\nWe don’t know \\(F\\), but we can treat the data as an approximation \\(\\hat F\\) of \\(F\\). Here \\(\\hat F\\) is the empirical distribution of the data, which gives equal probability to each observed data point. So we know how to sample from \\(\\hat F\\). The bootstrap says to sample from \\(\\hat F\\) many times and calculate \\(T\\) using these samples. The sampling distribution of \\(T\\) under \\(\\hat F\\) then approximates the sampling distribution of \\(T\\) under \\(F\\).\nThe canonical diagram comes from Efron and Tibshirani (1994). In the diagram the bootstrap world approximates the real world:\n\nIn terms of assessing model performance, this means that we sample our original data \\(X \\in \\mathbb{R}^{n, p}\\) with replacement to generate new datasets \\(X^*_1, ..., X^*_B \\in \\mathbb{R}^{n, p}\\), and then we estimate model performance on each of the bootstrap samples \\(X^*_b\\)."
  },
  {
    "objectID": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#data-model",
    "href": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#data-model",
    "title": "predictive performance via bootstrap variants",
    "section": "Data & Model",
    "text": "Data & Model\nIn this post, we’ll fit a linear regression1 and compare:\n\nbootstrap in-sample error\nbootstrap out-of-sample error\nthe optimism bootstrap\nthe 0.632 bootstrap\nthe 0.632+ bootstrap\n\nFor data, we’ll use average college tuition costs in each state. The original data source is here, but I downloaded it from the Tidy Tuesday repository.\nIn particular, we’ll see how predictive tuition costs in 2014-15 are for 2015-16 using a simple linear regression of the form2:\n\\[y_i = f(x_i) = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\sim \\mathrm{N}(0, \\sigma^2)\\]\nHere \\(y_i\\) is the average tuition in state \\(i\\) for the 2015-16 academic year3, and \\(x_i\\) is the average tuition in state \\(i\\) for the 2014-15 academic year. We have \\(n = 50\\) data points. Once we fit a model, we’ll refer to the predicted tuition for state \\(i\\) as \\(f(x_i)\\). The data looks like:\n\nlibrary(readxl)\nlibrary(here)\n\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(Metrics)\n\nset.seed(27)\n\ndata <- read_xlsx(here(\"post\", \"2018-05-03_performance-assessments-via-bootstrap-variants\", \"us_avg_tuition.xlsx\")) %>% \n  transmute(state = State,\n            avg_tuition_15_16 = `2015-16`,\n            avg_tuition_14_15 = `2014-15`) %>% \n  mutate_if(is.numeric, round)\n\ndata\n\n# A tibble: 50 × 3\n   state       avg_tuition_15_16 avg_tuition_14_15\n   <chr>                   <dbl>             <dbl>\n 1 Alabama                  9751              9496\n 2 Alaska                   6571              6149\n 3 Arizona                 10646             10414\n 4 Arkansas                 7867              7606\n 5 California               9270              9187\n 6 Colorado                 9748              9299\n 7 Connecticut             11397             10664\n 8 Delaware                11676             11515\n 9 Florida                  6360              6345\n10 Georgia                  8447              8063\n# … with 40 more rows\n\n\nWe quickly plot the data with a linear regression overlaid:\n\nggplot(data, aes(avg_tuition_14_15, avg_tuition_15_16)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(size = 2) +\n  labs(title = \"Average College Tuition\",\n       subtitle = \"Each point represents one state\",\n       y = \"2015-2016 Average Tuition (dollars)\",\n       x = \"2014-2015 Average Tuition (dollars)\") +\n  theme_classic()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nTo assess model performance, we’ll use squared error, which we calculate as:\n\\[\\mathcal{L(y_i, f(x_i))} = (y_i - f(x_i))^2\\]\nwhere we treat \\(y\\) as a vector in \\(\\mathbb{R}^n\\). Our first measure of model performance is the in sample performance, which is the mean squared error (MSE) on all the training data:\n\\[\\texttt{in sample error} = {1 \\over n} \\sum_{i = 1}^n \\mathcal{L}(y_i, f(x_i))\\]\nWe can calculate this in R like so:\n\nformula <- avg_tuition_15_16 ~ avg_tuition_14_15\nmodel <- lm(formula, data)\n\n# utility to extract response vector from formula and dataframe\nresponse <- function(formula, data) model.response(model.frame(formula, data))\n\npred <- predict(model, data)\ntrue <- response(formula, data)\n\n# in the code I use root mean squared error (RMSE) as a metric rather than \n# MSE. this keeps the presentation of the math slightly nicer, since I can\n# omit square roots, but keeps the errors on a more interpretable scale.\n\n# I use `Metrics::rmse` for predictions that live in vectors\n# and `yardstick::rmse` for predictions that live in dataframes\nin_sample <- Metrics::rmse(pred, true)\n\nThe in sample error on the original data is 186 dollars.\n\nApparent and out of sample bootstrap error\nThere are several ways to calculate the error on our bootstrap samples. One first step is to calculate the bootstrap in sample error by estimating the performance of a model fit on each bootstrap sample on the each bootstrap sample \\(X^*_b\\) itself.\nTo write this out more formally, let \\(f_b\\) be the model fit to the \\(b^{th}\\) bootstrapped dataset, let \\(I_b\\) be the data points that made it into the \\(b^{th}\\) bootstrap sample and let \\(n_b\\) be the total number of data points in the \\(b^{th}\\) bootstrap sample\n\\[\\texttt{bootstrap in sample error} = \\\\ {1 \\over B } \\sum_{b = 1}^B {1 \\over n_b} \\sum_{i \\in I_b} \\mathcal{L}(y_i, f_b(x_i))\\]\nHowever, we also know that for example bootstrap sample, some of the original data didn’t get used to fit \\(f_b\\). We can use that data to calculate bootstrap out of sample error:\n\\[\\texttt{bootstrap out of sample error} = \\\\ {1 \\over B } \\sum_{b = 1}^B {1 \\over n - n_b} \\sum_{i \\notin I_b} \\mathcal{L}(y_i, f_b(x_i))\\]\nWhen someone tells me that they used the bootstrap to evaluate their model, I typically assume that they’re reporting the bootstrap out of sample error, especially if they’re from the machine learning community.\nThe bootstrap in sample error typically underestimates prediction error, and the bootstrap out of sample error typically overestimates prediction error. There are several variants of the bootstrap that try to correct these biases.\n\n\nThe optimism bootstrap\nThe first of these is the optimism bootstrap. First we define the optimism of the \\(b^{th}\\) bootstrap model.\n\\[\\mathcal{O}_b = {1 \\over n} \\sum_{i = 1}^n \\mathcal{L}(y_i, f_b(x_i)) - {1 \\over n_b} \\sum_{i \\in I_b} \\mathcal{L}(y_i, f_b(x_i))\\]\nThis is the same as calculating the average error of \\(f_b\\) on the entire original sample, and then subtracting the bootstrap in sample error. To get a better estimate of the overall error we take the average optimism and add it to the in sample error estimate:\n\\[\\texttt{optimism bootstrap error} = \\\\ \\texttt{in sample error} + {1 \\over B} \\sum_{b=1}^B \\mathcal{O}_b\\]\n\n\nThe 0.632 and 0.632+ bootstrap\nInterestingly, the bootstrap out of sample error is somewhat pessimistic (Efron and Tibshirani (1994), Efron and Tibshirani (1997)). The 0.632 bootstrap estimator tries to address this problem by combining the in sample performance estimate with the bootstrap out of sample performance estimate:\n\\[\\texttt{0.632 bootstrap error} = \\\\ \\quad \\\\ 0.632 \\cdot \\texttt{bootstrap out of sample error} + \\\\ 0.368 \\cdot \\texttt{in sample error}\\]\nWhere does the 0.632 come from? On average a bootstrap sample contains 63.2% of the data points in the original dataset. This Cross Validated answer has some nice discussion.\nSimilarly, the 0.632+ bootstrap estimator tries to find a good balance between the in sample error and the bootstrap out of sample error. To do this, it considers the no information error rate:\n\\[\\texttt{no info error rate} = {1 \\over n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\mathcal{L}(y_i, f(x_j))\\]\nwhich is the expected error rate when data points and responses are randomly assigned. Averaging over all \\(i, j\\) you get a measure of how well you can predict when you know pretty much nothing. Then you can estimate the relative overfitting of a model with:\n\\[\\texttt{overfit} = {\\texttt{bootstrap out of sample error} - \\texttt{in sample error} \\over \\texttt{no info error rate} - \\texttt{in sample error}}\\]\nThe 0.632+ uses this to weight the bootstrap out of sample error and the in sample error according to\n\\[\\texttt{w} = \\texttt{weight on out of sample bootstrap error} \\\\ = {0.632 \\over 1 - 0.368 \\cdot \\texttt{overfit}}\\]\nand the final 0.632+ estimator is\n\\[\\texttt{0.632+ bootstrap error} = \\\\ \\quad \\\\ \\texttt{w} \\cdot \\texttt{out of sample bootstrap error} + \\\\ (1 - \\texttt{w}) \\cdot \\texttt{in sample error}\\]\nThe idea is that the 0.632 estimator can be optimistic, and to take overfitting into account to correct this. If the relative overfitting is zero, the 0.632+ estimator reduces to the 0.632 estimator."
  },
  {
    "objectID": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#actually-calculating-these-things-in-r",
    "href": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#actually-calculating-these-things-in-r",
    "title": "predictive performance via bootstrap variants",
    "section": "Actually calculating these things in R",
    "text": "Actually calculating these things in R\nBefore anything else, we need bootstrap samples. Luckily, the rsample package makes this super convenient.\n\n# create an `rset` object: a data frame with a list-column full\n# of bootstrap resamples\nboots <- bootstraps(data, times = 25)\nboots\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits          id         \n   <list>          <chr>      \n 1 <split [50/19]> Bootstrap01\n 2 <split [50/20]> Bootstrap02\n 3 <split [50/17]> Bootstrap03\n 4 <split [50/18]> Bootstrap04\n 5 <split [50/14]> Bootstrap05\n 6 <split [50/13]> Bootstrap06\n 7 <split [50/15]> Bootstrap07\n 8 <split [50/20]> Bootstrap08\n 9 <split [50/17]> Bootstrap09\n10 <split [50/13]> Bootstrap10\n# … with 15 more rows\n\n\nThe individual bootstrap samples are contained in the rsplit objects:\n\none_boot <- boots$splits[[1]]\none_boot\n\n<Analysis/Assess/Total>\n<50/19/50>\n\n\nThis print method is to help track which data went into the bootstrap sample. The format here is <# data points in resampled data set / # original data points not in the resampled data set / # original data points>.\nIf we want to see the bootstrap sample itself, we can:\n\nanalysis(one_boot)\n\n# A tibble: 50 × 3\n   state          avg_tuition_15_16 avg_tuition_14_15\n   <chr>                      <dbl>             <dbl>\n 1 California                  9270              9187\n 2 Wyoming                     4891              4654\n 3 Florida                     6360              6345\n 4 South Carolina             11816             11470\n 5 Maine                       9573              9560\n 6 Kansas                      8530              8270\n 7 North Carolina              6973              6685\n 8 Missouri                    8564              8409\n 9 North Carolina              6973              6685\n10 Alabama                     9751              9496\n# … with 40 more rows\n\n\nIf we want to see all the data points that didn’t go into the bootstrap sample, we can use:\n\nassessment(one_boot)\n\n# A tibble: 19 × 3\n   state         avg_tuition_15_16 avg_tuition_14_15\n   <chr>                     <dbl>             <dbl>\n 1 Alaska                     6571              6149\n 2 Colorado                   9748              9299\n 3 Connecticut               11397             10664\n 4 Delaware                  11676             11515\n 5 Georgia                    8447              8063\n 6 Massachusetts             11588             10987\n 7 Michigan                  11991             11618\n 8 Nebraska                   7608              7348\n 9 New Jersey                13303             13027\n10 New York                   7644              7306\n11 Ohio                      10196             10104\n12 Oregon                     9371              8949\n13 Pennsylvania              13395             13157\n14 Rhode Island              11390             10977\n15 Tennessee                  9263              8941\n16 Vermont                   14993             14501\n17 Virginia                  11819             11202\n18 Washington                10288             10703\n19 West Virginia              7171              6698\n\n\nNow we want to fit models on each of the bootstrap samples and assess various performance metrics. We write some helper functions to smooth things along.\n\n# evaluate model performance on a particular dataset\nperformance <- function(model, data, metric = Metrics::rmse) {\n  \n  # aside: it would be nice to be able to extract a data/design object from\n  # the model, and then extract the response from that. someday.\n  \n  true <- response(formula, data)\n  pred <- predict(model, data)\n  metric(true, pred)\n}\n\n# get the no info error rate. should be used on the model\n# fit to full original data\nno_info_error_rate <- function(model, formula, data, metric = Metrics::rmse) {\n  \n  true <- response(formula, data)\n  pred <- predict(model, data)\n  \n  crossed <- crossing(true, pred)\n  with(crossed, metric(true, pred))\n}\n\n# NOTE: `bs` in variable names is an abbreviation for `bootstrap`\n\n# 0.632 bootstrap estimate of performance\nbs_632 <- function(in_sample, bs_out_of_sample) {\n  0.368 * in_sample + 0.632 * bs_out_of_sample\n}\n\n# 0.632+ bootstrap estimate of performance\nbs_632p <- function(in_sample, bs_out_of_sample, no_info_error_rate) {\n  relative_overfit <- (bs_out_of_sample - in_sample) /\n    (no_info_error_rate - in_sample)\n  w <- 0.632 / (1 - 0.368 * relative_overfit)\n  w * bs_out_of_sample + (1 - w) * in_sample\n}\n\nNow we can fit models on each bootstrapped dataset:\n\nno_info_perf <- no_info_error_rate(model, formula, data)\n\nboot_performance <- boots %>% \n  mutate(\n    # fit a model on *bootstrap sample* of data\n    model = map(splits, ~lm(formula, analysis(.x))),\n    \n    # bootstrap in sample error\n    bs_is_perf = map2_dbl(model, splits, ~performance(.x, analysis(.y))),\n    \n    # bootstrap out of sample error\n    bs_os_perf = map2_dbl(model, splits, ~performance(.x, assessment(.y))),\n    \n    # bootstrap error on full data\n    data_perf = map_dbl(model, ~performance(.x, data)),\n    \n    # optimism\n    optimism = bs_is_perf - data_perf,\n    \n    # optimism corrected error estimate\n    bs_optimism = in_sample - optimism,\n    \n    # 0.632 bootstrap error estimate\n    bs_632_perf = bs_632(bs_is_perf, bs_os_perf),\n    \n    # 0.632+ bootstrap error estimate\n    bs_632p_perf = bs_632p(bs_is_perf, bs_os_perf, no_info_perf))\n\nWe can calculate the point estimates we discussed above:\n\nclean_performance <- boot_performance %>% \n  select_if(is.numeric) %>% \n  select(-optimism, -data_perf) %>% \n  gather(model, statistic) %>% \n  transmute(measure = recode(\n    model,\n    \"bs_is_perf\" = \"bootstrap in sample\",\n    \"bs_os_perf\" = \"bootstrap out of sample\" ,\n    \"bs_optimism\" = \"optimism bootstrap\",\n    \"bs_632_perf\" = \"0.632 bootstrap\",\n    \"bs_632p_perf\" = \"0.632+ bootstrap\"),\n    error = statistic\n  )\n\nclean_performance %>% \n  group_by(measure) %>% \n  summarize_if(is.numeric, mean) %>% \n  mutate_if(is.numeric, round) %>% \n  knitr::kable()\n\n\n\n\nmeasure\nerror\n\n\n\n\n0.632 bootstrap\n188\n\n\n0.632+ bootstrap\n188\n\n\nbootstrap in sample\n180\n\n\nbootstrap out of sample\n192\n\n\noptimism bootstrap\n195\n\n\n\n\n\nHowever, point estimates aren’t good ways to compare the performance of two models because they don’t give us a sense of how much model performance might vary on different data sets. Visualizing the sampling distribution of each of the metrics gives us a more complete view of model performance:\n\nclean_performance %>% \n  ggplot(aes(measure, error, color = measure)) +\n  geom_boxplot() +\n  geom_jitter() +\n  scale_color_brewer(type = \"qual\") +\n  coord_flip() +\n  labs(\n    title = \"Sampling distributions of bootstrap performance metrics\",\n    y = \"rmse error (dollars)\"\n  ) + \n  theme_classic() +\n  theme(\n    axis.title.y = element_blank(),\n    legend.position = \"none\"\n  )\n\n\n\n\nFor this particular model, we see that the 0.632 and 0.632+ estimators are pretty much the same. This makes sense because a linear model fits the data quick well and so there should be little overfitting. We can also see that the bootstrap in sample error rate has the lowest median.\nGiven how it can be difficult to keep track of which metric should be calculated on which dataset, I should probably write some tests to confirm that my code does what I want it to. Since this is a blog post, I’m going to pass on that for the moment."
  },
  {
    "objectID": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#using-the-bootstrap-in-practice",
    "href": "post/2018-05-03_performance-assessments-via-bootstrap-variants/index.html#using-the-bootstrap-in-practice",
    "title": "predictive performance via bootstrap variants",
    "section": "Using the bootstrap in practice",
    "text": "Using the bootstrap in practice\nWhen you have fewer than 20,000 data points, it’s reasonable to use the optimism bootstrap with \\(B = 200\\) to \\(B = 400\\). When you have more data, cross-validation4 or simply splitting the data becomes more reliable. Any steps taken to develop a model should be performed on each bootstrap dataset. For example, if you use LASSO for feature selection and then fit a model, the feature selection step needs to be included in the bootstrap procedure.\nIn some studies (Steyerberg et al. (2001)) the 0.632 and 0.632+ estimators have similar performance. Asymptotically, the 0.632 estimator is equivalent to the optimism bootstrap (Efron and Tibshirani (1994))5. It isn’t clear if one is preferrable over the other in finite samples, so I’d just stick with the estimator your audience is most likely to already be familiar with. You should be fine so long as you avoid using the bootstrap in sample error estimate.\nIf you are interested in comparing several models, it probably isn’t sufficient to compare the sampling distributions of these error measures. If you fit multiple models on the same bootstrap datasets, the bootstrap samples will have an impact on model performance. Hierarchical models are a good way to model this. Once you have your bootstrapped error estimates, the tidyposterior package can help you take the next step when comparing models.\nIf you’re interested, stick around for future posts, where I’ll cover the process of building predictive models and how to select from several predictive models.\nFeedback is always welcome!"
  },
  {
    "objectID": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html",
    "href": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html",
    "title": "comparing runs with riegel’s formula and gams",
    "section": "",
    "text": "Runners often vary the distance and intensity of their workouts. In this post I demonstrate how to compare runs of different lengths using Riegel’s formula. The formula accurately describes the trade-off between run distance and average speed for aerobic runs up to about a half-marathon in length. Using my Strava data, I demonstrate how to use Riegel’s formula to measure the difficulty of runs on a standardized scale and briefly investigate how my fitness has changed over time with GAMs."
  },
  {
    "objectID": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#riegels-formula-a-measure-of-running-ability",
    "href": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#riegels-formula-a-measure-of-running-ability",
    "title": "comparing runs with riegel’s formula and gams",
    "section": "Riegel’s formula: a measure of running ability",
    "text": "Riegel’s formula: a measure of running ability\nRiegel (1981) proposed that aerobic exercise can be modeled via the power law equation:\n\\[t = a d^b\\]\nwhere \\(t\\) is the time it takes to travel distance \\(d\\). Here \\(a\\) and \\(b\\) are coefficients that depend on the activity (typically \\(a\\) and \\(b\\) are estimated separately for different ages and genders). \\(b = 1.06\\) is a typical estimate for recreational runners, although \\(b\\) might be as high as \\(1.08\\) for elite runners. Using data from over two thousand runners, Vickers and Vertosick (2016) showed that this formula is well-calibrated for runs ranging from one mile to a half-marathon in length. For runs longer than a half marathon, the formula tends be too optimistic.\nWe can also reformulate the equation to deal with speed rather than time. Letting \\(b = 1 + k\\) we have:\n\\[s = {d \\over t} = {1 \\over a d^k}\\]\nwhere \\(s\\) is speed. We can also estimate the time it takes to complete a run of length \\(d_2\\) given a run of length \\(d_1\\) in time \\(t_1\\) (it’s typical to estimate a Riegel curve based on a single best effort1):\n\\[t_2 = t_1 \\cdot \\left(d_2 \\over d_1 \\right)^b\\]\nSimilarly we can rearrange to calculate speed instead:\n\\[s_2 = s_1 \\cdot \\left(d_1 \\over d_2 \\right)^k\\]\nUsing this equation for a set of distances, we can estimate a curve describing maximal speeds at each distance. For example, my best recent effort was a 5K that I completed in 18:52, which results in the following curve:\n\n\n\n\n\nNotice the exponential decay in speed with run distance. This plot also includes a visualization of exercise types at various speeds2, using a rough categorization from Strava again based on the 5K effort. The takeaway is that exercise at different intensities and durations uses different muscular mechanisms. The vertical lines mark the region where Riegel’s curve is well calibrated.\nWithin the aerobic region, we can treat points on the Riegel curve as equally difficult. The entire Riegel curve serves as a measure of fitness, but we can describe the entire curve by choosing a single point to act as a reference. I choose to standardize my runs to their equivalent 5K times."
  },
  {
    "objectID": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#data",
    "href": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#data",
    "title": "comparing runs with riegel’s formula and gams",
    "section": "Data",
    "text": "Data\nI record my runs using Strava. Strava can analyze data recorded on a number of devices, but I just run Strava directly on my phone. During a run, Strava records my latitude and longitude once per second. These measurements are typically accurate to within 10 meters. For this analysis I bulk exported my Strava data.\nIn total I have recordings of 45 runs from November 2017 to April 20183. The trajectory of one of these runs is below:\n\n\n\n\n\nRun in black.\n\n\n\n\nThis is a fairly typical run of mine around Rice’s outer loop4. In addition to plotting individual runs, we can look at how my runs have evolved over time by visualizing some summary statistics for each run:"
  },
  {
    "objectID": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#changing-fitness-over-time-with-gams",
    "href": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#changing-fitness-over-time-with-gams",
    "title": "comparing runs with riegel’s formula and gams",
    "section": "Changing fitness over time with GAMs",
    "text": "Changing fitness over time with GAMs\nNow I use a Generalized Additive Model to understand how my fitness changes over time, following Simpson (2018). Using mgcv I fit a model of the form:\n\\[\\mathrm{riegel\\_5k\\_time}_i = f(t_i) + \\varepsilon_i \\qquad \\varepsilon_i \\sim \\mathrm{Normal}(0, \\sigma^2)\\] where \\(t_i\\) is the number of days since my first run and \\(f\\) is a smooth function. I also fit a model of the same form but with additional continuous autoregressive (1) structure in case there is residual autocorrelation in the time series after fitting the model.\nBoth models fit the data well, but the CAR(1) structure turns out to be unnecessary. We briefly inspect the original model:\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nriegel_5k_time ~ s(t)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  21.5884     0.2038     106   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n       edf Ref.df   F  p-value    \ns(t) 3.183  3.183 9.1 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.388   \n  Scale est. = 1.8267    n = 45\n\n\n\n\n\nWe also plot the fit model:\n\n\n\n\n\nMy estimated fitness passes some sanity checks: (1) I lose fitness over winter break when I’m not working out, (2) toward the end of the spring semester after several months of training, I start to plateau. My fitness should probably be improving in November, when I was training fairly hard, but I had one spectacularly slow long run that that seems to be throwing the estimates off.\n\nWhat I’m really interested in here is how my fitness changes over time, or the derivative of my smoothed Riegel 5K times. To estimate this, we can draw simulations from the posterior of the GAM, and then approximate the derivative via finite differences. Gavin Simpson’s wonderful gratia package provides this functionality, and we plot the first derivative of the GAM below:\n\n\n\n\n\nHere we see the same trends as before: losing fitness over winter break, big improvements in January, my first month of serious training, and a plateau towards the end of the semester. Interestingly, my changing in fitness level is never significant. This doesn’t match with my perceived experience, and I attribute this again to measuring mean effort rather than maximal effort. I ran my recovery and long runs at about a 7:30 pace all semester, and really pushed myself only once in every three or four runs. So there’s a lot of runs in there that make it look like I’m not doing much."
  },
  {
    "objectID": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#takeaways",
    "href": "post/2018-05-16_comparing-runs-with-riegels-formula-and-gams/index.html#takeaways",
    "title": "comparing runs with riegel’s formula and gams",
    "section": "Takeaways",
    "text": "Takeaways\nRiegel’s formula provides a nice way to standardize runs, and GAMs are a satisfying and interpretable way to investigate how run capacity changes over time. An interesting problem is to model best efforts rather than mean efforts. I’m brainstorming on this at the moment. In a future blog post I’ll show how to efficiently process and tidy Strava GPX files. I’m also curious to replicate Vickers and Vertosick (2016), or to repeat this analysis using Gaussian processes or state space models."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "",
    "text": "This post is a casual case study in speeding up R code. I work through several iterations of a function to read and process GPS running data from Strava stored in the GPX format. Along the way I describe how to visualize code bottlenecks with profvis and briefly touch on fast compiled code with Rcpp and parallelization with furrr."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#the-problem-tidying-trajectories-in-gpx-files",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#the-problem-tidying-trajectories-in-gpx-files",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "The problem: tidying trajectories in GPX files",
    "text": "The problem: tidying trajectories in GPX files\nI record my runs on my phone using Strava. Strava stores the GPS recordings in GPX files, which are XML files that follow some additional conventions. They start with some metadata and then contain a list of GPS readings taken at one second intervals with longitude, latitude, elevation and timestap information. I wanted to approximate my speed at each timestamp in the GPS record, as well as my distance traveled since the previous GPS recordings.\nBelow I have an example of a GPX file that contains three GPS readings. First I create a vector that contains the names off my GPX files, and then I subset to the files that contain running data. I choose to work with the third run as a canonical example, and show a subset of the recording with three GPS readings.\n\nlibrary(tidyverse)\nlibrary(here)\n\n# file contain run data\nact_files <- dir(here::here(\"post\", \"2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr\", \"2018-04-17-activities-alex\"),\n  full.names = TRUE\n)\nrun_files <- act_files[str_detect(act_files, \"Run\")]\n\n# example file we'll work with\nfname <- run_files[3]\n\n# subset of example\nall <- read_lines(fname)\nmini_idx <- c(1:20, 5897:5899)\ncat(all[mini_idx], sep = \"\\n\")\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<gpx creator=\"StravaGPX Android\" version=\"1.1\" xmlns=\"http://www.topografix.com/GPX/1/1\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.topografix.com/GPX/1/1 http://www.topografix.com/GPX/1/1/gpx.xsd\">\n <metadata>\n  <time>2017-10-31T17:58:22Z</time>\n </metadata>\n <trk>\n  <name>analytically slow</name>\n  <trkseg>\n   <trkpt lat=\"29.7169490\" lon=\"-95.3978210\">\n    <ele>14.1</ele>\n    <time>2017-10-31T17:58:22Z</time>\n   </trkpt>\n   <trkpt lat=\"29.7168040\" lon=\"-95.3977180\">\n    <ele>14.4</ele>\n    <time>2017-10-31T17:58:29Z</time>\n   </trkpt>\n   <trkpt lat=\"29.7167480\" lon=\"-95.3976890\">\n    <ele>14.5</ele>\n    <time>2017-10-31T17:58:30Z</time>\n   </trkpt>\n  </trkseg>\n </trk>\n</gpx>\n\n\nThe part we want is in the <trkseg> tags. We’d like to turn this into a tidy dataframe where each row represents one GPS reading and the columns contain information like speed, distance, traveled, elevation gained, etc."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-0-using-plotkmlreadgpx",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-0-using-plotkmlreadgpx",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "GPX reader version 0: using plotKML::readGPX",
    "text": "GPX reader version 0: using plotKML::readGPX\n\n\n\n\n\n\nNote\n\n\n\nplotKML was archived from CRAN on 2022-04-18 and the archived version isn’t easy to install. I’ve pulled the source for the readGPX() function and inserted it below to avoid depending on the plotKML package as of 2022-04-29.\n\n\n\n\nCode\nlibrary(XML)\n\nreadGPX <- function(gpx.file,\n                    metadata = TRUE,\n                    bounds = TRUE,\n                    waypoints = TRUE,\n                    tracks = TRUE,\n                    routes = TRUE) {\n  opt <- options(warn = -1)\n  if (!file.exists(gpx.file)) stop(\"The file '\", gpx.file, \"'\\n  does not exist in \", getwd())\n\n  if (metadata == TRUE) {\n    metadata <- .readGPX.element(gpx.file, \"name\")\n  }\n  if (bounds == TRUE) {\n    bounds <- .readGPX.element(gpx.file, \"bounds\")\n  }\n  if (waypoints == TRUE) {\n    waypoints <- .readGPX.element(gpx.file, \"wpt\")\n  }\n  if (tracks == TRUE) {\n    tracks <- .readGPX.element(gpx.file, \"trk\")\n  }\n  if (routes == TRUE) {\n    routes <- .readGPX.element(gpx.file, \"rte\")\n  }\n\n  gpx <- list(metadata = metadata, bounds = bounds, waypoints = waypoints, tracks = tracks, routes = routes)\n  return(gpx)\n  on.exit(options(opt))\n}\n\n## Read various elements from a *.gpx file:\n\n.readGPX.element <- function(gpx.file, element) {\n  # element = \"metadata\", \"wpt\", \"rte\", \"trk\"\n\n  ret <- xmlTreeParse(gpx.file, useInternalNodes = TRUE)\n  # top structure:\n  top <- xmlRoot(ret)\n\n  # check if there is any content:\n  if (any(grep(element, names(top)))) {\n\n    # tracks:\n    if (element == \"trk\") {\n      ret <- NULL\n      nu <- which(names(top) %in% element)\n      for (c in seq_along(nu)) {\n        lst <- which(names(top[[nu[c]]]) %in% \"trkseg\")\n        nm <- names(top[[nu[c]]][[lst[1]]][[1]])\n        ret[[c]] <- list(NULL)\n        for (i in seq_along(lst)) {\n          trkpt <- top[[nu[c]]][[lst[i]]]\n          ret[[c]][[i]] <- data.frame(NULL)\n          ## get columns (https://www.topografix.com/GPX/1/1/#type_wptType)\n          lon <- as.numeric(xmlSApply(trkpt, xmlGetAttr, \"lon\"))\n          lat <- as.numeric(xmlSApply(trkpt, xmlGetAttr, \"lat\"))\n          ret[[c]][[i]][1:length(lon), \"lon\"] <- lon\n          ret[[c]][[i]][1:length(lat), \"lat\"] <- lat\n          if (!nm[[1]] == \"NULL\") {\n            for (j in 1:length(nm)) {\n              xm <- as.character(sapply(sapply(xmlChildren(trkpt), function(x) x[[nm[[j]]]]), xmlValue))\n              ret[[c]][[i]][1:length(xm), nm[[j]]] <- xm\n            }\n          }\n        }\n        names(ret[[c]]) <- xmlValue(top[[nu[c]]][[\"name\"]])\n      }\n    }\n\n    if (element == \"wpt\") {\n      ret <- data.frame(NULL)\n      nu <- which(names(top) %in% element)\n      nm <- names(top[[nu[1]]])\n      for (i in seq_along(nu)) {\n        # coordinates:\n        ret[i, \"lon\"] <- as.numeric(xmlGetAttr(top[[nu[i]]], \"lon\"))\n        ret[i, \"lat\"] <- as.numeric(xmlGetAttr(top[[nu[i]]], \"lat\"))\n        if (!nm[[1]] == \"NULL\") {\n          for (j in 1:length(nm)) {\n            ret[i, nm[[j]]] <- xmlValue(xmlChildren(top[[nu[i]]])[[nm[[j]]]])\n          }\n        }\n      }\n    }\n\n    if (element == \"rte\") {\n      ret <- NULL\n      nu <- which(names(top) %in% element)\n      for (c in seq_along(nu)) {\n        ret[[c]] <- data.frame(NULL)\n        lst <- which(names(top[[nu[c]]]) %in% \"rtept\")\n        nm <- names(top[[nu[c]]][[lst[1]]])\n        for (i in seq_along(lst)) {\n          rtept <- top[[nu[c]]][[lst[i]]]\n          ret[[c]][i, \"lon\"] <- as.numeric(xmlGetAttr(rtept, \"lon\"))\n          ret[[c]][i, \"lat\"] <- as.numeric(xmlGetAttr(rtept, \"lat\"))\n          if (!nm[[1]] == \"NULL\") {\n            for (j in c(\"name\", \"cmt\", \"desc\", \"sym\", \"type\")) {\n              try(ret[[c]][i, j] <- xmlValue(rtept[[j]]), silent = TRUE)\n            }\n          }\n        }\n        names(ret)[c] <- xmlValue(top[[nu[c]]][[\"name\"]])\n      }\n    }\n\n    # bounds\n    if (element == \"bounds\") {\n      nu <- which(names(top) %in% element)\n      ret <- matrix(rep(NA, 4), nrow = 2, dimnames = list(c(\"lat\", \"lon\"), c(\"min\", \"max\")))\n      # coordinates:\n      ret[1, 1] <- as.numeric(xmlGetAttr(top[[nu[1]]], \"minlon\"))\n      ret[1, 2] <- as.numeric(xmlGetAttr(top[[nu[1]]], \"maxlon\"))\n      ret[2, 1] <- as.numeric(xmlGetAttr(top[[nu[1]]], \"minlat\"))\n      ret[2, 2] <- as.numeric(xmlGetAttr(top[[nu[1]]], \"maxlat\"))\n    }\n\n    # metadata\n    if (element == \"name\") {\n      lst <- c(\"name\", \"desc\", \"author\", \"email\", \"url\", \"urlname\", \"time\")\n      nu <- which(names(top) %in% lst)\n      if (!nu[[1]] == \"NULL\") {\n        ret <- data.frame(NULL)\n        for (i in seq_along(lst)) {\n          try(ret[1, lst[i]] <- xmlValue(top[[nu[[i]]]]), silent = TRUE)\n        }\n      }\n    }\n  } else {\n    ret <- NULL\n  }\n\n  return(ret)\n}\n\n\nUsing plotKML::readGPX we can read the representative file into R.\n\ngps_raw <- readGPX(fname)$tracks[[1]][[1]]  |> \n  as_tibble()\n\ngps_raw\n\n# A tibble: 1,472 × 4\n     lon   lat ele   time                \n   <dbl> <dbl> <chr> <chr>               \n 1 -95.4  29.7 14.1  2017-10-31T17:58:22Z\n 2 -95.4  29.7 14.4  2017-10-31T17:58:29Z\n 3 -95.4  29.7 14.5  2017-10-31T17:58:30Z\n 4 -95.4  29.7 14.6  2017-10-31T17:58:31Z\n 5 -95.4  29.7 14.6  2017-10-31T17:58:32Z\n 6 -95.4  29.7 14.7  2017-10-31T17:58:33Z\n 7 -95.4  29.7 14.7  2017-10-31T17:58:34Z\n 8 -95.4  29.7 14.7  2017-10-31T17:58:36Z\n 9 -95.4  29.7 14.7  2017-10-31T17:58:37Z\n10 -95.4  29.7 14.7  2017-10-31T17:58:38Z\n# … with 1,462 more rows\n\n\nNow we can we correct the type information:\n\nlibrary(lubridate)\n\nretyped <- gps_raw |>\n  mutate_at(vars(lon, lat, ele), as.numeric) |>\n  mutate_at(vars(time), lubridate::ymd_hms)\n\nretyped\n\n# A tibble: 1,472 × 4\n     lon   lat   ele time               \n   <dbl> <dbl> <dbl> <dttm>             \n 1 -95.4  29.7  14.1 2017-10-31 17:58:22\n 2 -95.4  29.7  14.4 2017-10-31 17:58:29\n 3 -95.4  29.7  14.5 2017-10-31 17:58:30\n 4 -95.4  29.7  14.6 2017-10-31 17:58:31\n 5 -95.4  29.7  14.6 2017-10-31 17:58:32\n 6 -95.4  29.7  14.7 2017-10-31 17:58:33\n 7 -95.4  29.7  14.7 2017-10-31 17:58:34\n 8 -95.4  29.7  14.7 2017-10-31 17:58:36\n 9 -95.4  29.7  14.7 2017-10-31 17:58:37\n10 -95.4  29.7  14.7 2017-10-31 17:58:38\n# … with 1,462 more rows\n\n\nWe want to compare location at \\(t\\) and \\(t - 1\\), so we create a lagged column of longitudes and latitudes. We put longitude and latitude together into a vector to play well with raster::pointDistance, which we’ll use to compute the great circle distance between two points.\n\nlagged <- retyped |>\n  mutate(\n    x = map2(lon, lat, c), # create lagged position, this means the\n    x_old = lag(x),        # first row isn't complete\n    t_old = lag(time)\n  ) |>\n  slice(-1) # remove incomplete first row\n\nlagged\n\n# A tibble: 1,471 × 7\n     lon   lat   ele time                x         x_old     t_old              \n   <dbl> <dbl> <dbl> <dttm>              <list>    <list>    <dttm>             \n 1 -95.4  29.7  14.4 2017-10-31 17:58:29 <dbl [2]> <dbl [2]> 2017-10-31 17:58:22\n 2 -95.4  29.7  14.5 2017-10-31 17:58:30 <dbl [2]> <dbl [2]> 2017-10-31 17:58:29\n 3 -95.4  29.7  14.6 2017-10-31 17:58:31 <dbl [2]> <dbl [2]> 2017-10-31 17:58:30\n 4 -95.4  29.7  14.6 2017-10-31 17:58:32 <dbl [2]> <dbl [2]> 2017-10-31 17:58:31\n 5 -95.4  29.7  14.7 2017-10-31 17:58:33 <dbl [2]> <dbl [2]> 2017-10-31 17:58:32\n 6 -95.4  29.7  14.7 2017-10-31 17:58:34 <dbl [2]> <dbl [2]> 2017-10-31 17:58:33\n 7 -95.4  29.7  14.7 2017-10-31 17:58:36 <dbl [2]> <dbl [2]> 2017-10-31 17:58:34\n 8 -95.4  29.7  14.7 2017-10-31 17:58:37 <dbl [2]> <dbl [2]> 2017-10-31 17:58:36\n 9 -95.4  29.7  14.7 2017-10-31 17:58:38 <dbl [2]> <dbl [2]> 2017-10-31 17:58:37\n10 -95.4  29.7  14.7 2017-10-31 17:58:39 <dbl [2]> <dbl [2]> 2017-10-31 17:58:38\n# … with 1,461 more rows\n\n\nIt turns out this data is not contiguous. Strava has a feature called autopause which detects pauses in runs (for example, at a stoplight), and GPS readings during paused periods are not include in the GPX files1. GPS readings typically happen once every second. I plotted the time gaps between readings and realized that time gaps greater than three seconds between two GPS recordings indicated a pause. This lets me break the run down into a series of contigous segments:\n\nsegmented <- lagged |>\n  mutate(\n    rest = as.numeric(time - t_old), # seconds\n    new_segment = as.numeric(rest > 3),\n    segment = cumsum(new_segment)\n  ) |>\n  # don't want t_old to be from previous segment\n  group_by(segment) |>\n  slice(-1)\n\nsegmented\n\n# A tibble: 1,464 × 10\n# Groups:   segment [5]\n     lon   lat   ele time                x      x_old  t_old                rest\n   <dbl> <dbl> <dbl> <dttm>              <list> <list> <dttm>              <dbl>\n 1 -95.4  29.7  14.5 2017-10-31 17:58:30 <dbl>  <dbl>  2017-10-31 17:58:29     1\n 2 -95.4  29.7  14.6 2017-10-31 17:58:31 <dbl>  <dbl>  2017-10-31 17:58:30     1\n 3 -95.4  29.7  14.6 2017-10-31 17:58:32 <dbl>  <dbl>  2017-10-31 17:58:31     1\n 4 -95.4  29.7  14.7 2017-10-31 17:58:33 <dbl>  <dbl>  2017-10-31 17:58:32     1\n 5 -95.4  29.7  14.7 2017-10-31 17:58:34 <dbl>  <dbl>  2017-10-31 17:58:33     1\n 6 -95.4  29.7  14.7 2017-10-31 17:58:36 <dbl>  <dbl>  2017-10-31 17:58:34     2\n 7 -95.4  29.7  14.7 2017-10-31 17:58:37 <dbl>  <dbl>  2017-10-31 17:58:36     1\n 8 -95.4  29.7  14.7 2017-10-31 17:58:38 <dbl>  <dbl>  2017-10-31 17:58:37     1\n 9 -95.4  29.7  14.7 2017-10-31 17:58:39 <dbl>  <dbl>  2017-10-31 17:58:38     1\n10 -95.4  29.7  14.6 2017-10-31 17:58:40 <dbl>  <dbl>  2017-10-31 17:58:39     1\n# … with 1,454 more rows, and 2 more variables: new_segment <dbl>,\n#   segment <dbl>\n\n\nNow I calculate some information about each time point and segment that I’ll use in downstream analyses:\n\nlonlat_dist <- partial(raster::pointDistance, lonlat = TRUE)\n\nuseful <- segmented |>\n  mutate(\n    seg_length = max(time) - min(t_old),  # seconds\n    dx = map2_dbl(x, x_old, lonlat_dist), # meters\n    dx = 0.000621371 * dx,                # miles\n    dt = rest / 60^2,                     # hours\n    speed = dx / dt,                      # mph\n    pace = 60 * dt / dx,                  # min / mile\n    elev = as.numeric(ele)                # feet\n  ) |> \n  dplyr::select(-ele, -x, -x_old, -t_old, -new_segment, -rest) |>\n  ungroup()\n\nuseful\n\n# A tibble: 1,464 × 10\n     lon   lat time                seg…¹ seg…²      dx      dt speed  pace  elev\n   <dbl> <dbl> <dttm>              <dbl> <drt>   <dbl>   <dbl> <dbl> <dbl> <dbl>\n 1 -95.4  29.7 2017-10-31 17:58:30     1 510 … 0.00423 2.78e-4 15.2   3.94  14.5\n 2 -95.4  29.7 2017-10-31 17:58:31     1 510 … 0.00367 2.78e-4 13.2   4.54  14.6\n 3 -95.4  29.7 2017-10-31 17:58:32     1 510 … 0.00197 2.78e-4  7.11  8.44  14.6\n 4 -95.4  29.7 2017-10-31 17:58:33     1 510 … 0.00483 2.78e-4 17.4   3.45  14.7\n 5 -95.4  29.7 2017-10-31 17:58:34     1 510 … 0.00230 2.78e-4  8.28  7.25  14.7\n 6 -95.4  29.7 2017-10-31 17:58:36     1 510 … 0.00410 5.56e-4  7.38  8.13  14.7\n 7 -95.4  29.7 2017-10-31 17:58:37     1 510 … 0.00243 2.78e-4  8.75  6.86  14.7\n 8 -95.4  29.7 2017-10-31 17:58:38     1 510 … 0.00316 2.78e-4 11.4   5.27  14.7\n 9 -95.4  29.7 2017-10-31 17:58:39     1 510 … 0.00415 2.78e-4 15.0   4.01  14.7\n10 -95.4  29.7 2017-10-31 17:58:40     1 510 … 0.00363 2.78e-4 13.1   4.60  14.6\n# … with 1,454 more rows, and abbreviated variable names ¹​segment, ²​seg_length\n\n\nWe can quickly visualize instantaneous speed throughout the run:\n\nggplot(useful, aes(time, speed, group = segment)) +\n  geom_point() +\n  geom_line(alpha = 0.5) +\n  labs(\n    title = \"Speed throughout example run\",\n    y = \"Speed (mph)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.x = element_blank())\n\n\n\n\nWe can see two short pauses present in the run at around 18:08 and 18:17.\nWe’re going to use the code above a whole bunch, so we wrap it up into a helper function. I’m not sure that raster::pointDistance is the best option for calculating the distance between two points, so we use a dist_func argument to make it easy to switch out.\n\nget_metrics <- function(gps_df, dist_func = lonlat_dist) {\n  gps_df |>\n    mutate_at(vars(lon, lat, ele), as.numeric) |>\n    mutate_at(vars(time), lubridate::ymd_hms) |>\n    mutate(\n      x = map2(lon, lat, c),\n      x_old = lag(x),\n      t_old = lag(time)\n    ) |>\n    slice(-1) |>\n    mutate(\n      rest = as.numeric(time - t_old),\n      new_segment = as.numeric(rest > 3),\n      segment = cumsum(new_segment) + 1\n    ) |>\n    group_by(segment) |>\n    slice(-1) |>\n    mutate(\n      seg_length = max(time) - min(t_old),\n      dx = map2_dbl(x, x_old, dist_func),\n      dx = 0.000621371 * dx,\n      dt = rest / 60^2,\n      speed = dx / dt,\n      pace = 60 * dt / dx,\n      elev = as.numeric(ele)\n    ) |>\n    dplyr::select(-ele, -x, -x_old, -t_old, -new_segment, -rest) |>\n    ungroup()\n}\n\nThis means our initial read_gpx function is just two lines:\n\nread_gpx0 <- function(fname) {\n  gps_df <- readGPX(fname)$tracks[[1]][[1]]\n  get_metrics(gps_df)\n}\n\nWe can use profvis::profvis to create an interactive visualization of how long it takes to read the example file.\n\nlibrary(profvis)\n\nprofvis(read_gpx0(fname))\n\n\n\n\n\nIn the default view, the horizontal axis represents time and the box represents the call stack. All the boxes above plotKML::readGPX are functions called by plotKML::readGPX. Here it seems like plotKML::readGPX takes about 400 milliseconds to run. So about half the time is spent reading in the file, and half calculating metrics. Most of the time calculating metrics is in raster::pointDistance, which is fairly up the call stack - you may have to click and drag the plot to see it."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-1-no-more-plotkmlgpx",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-1-no-more-plotkmlgpx",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "GPX reader version 1: no more plotKML::GPX",
    "text": "GPX reader version 1: no more plotKML::GPX\nThen I broke my R library and couldn’t use plotKML::readGPX for a little while. Since GPX files are XML files, I used the xml2 package as a replacement. xml2 has a function as_list that let me treat the XML as an R list. We extract the relevant portion of the list and purrr::map_dfr each GPS recording into a row of a tibble.\n\nlibrary(xml2)\n\nrun_xml <- read_xml(fname)\nrun_list <- as_list(run_xml)\ngps_pts <- run_list$gpx$trk$trkseg\n\nextract_gps_point <- function(point) {\n  tibble(\n    lon = attr(point, \"lon\"),\n    lat = attr(point, \"lat\"),\n    ele = point$ele[[1]],\n    time = point$time[[1]]\n  )\n}\n\nmap_dfr(gps_pts, extract_gps_point)\n\n# A tibble: 1,472 × 4\n   lon         lat        ele   time                \n   <chr>       <chr>      <chr> <chr>               \n 1 -95.3978210 29.7169490 14.1  2017-10-31T17:58:22Z\n 2 -95.3977180 29.7168040 14.4  2017-10-31T17:58:29Z\n 3 -95.3976890 29.7167480 14.5  2017-10-31T17:58:30Z\n 4 -95.3976530 29.7167050 14.6  2017-10-31T17:58:31Z\n 5 -95.3976600 29.7166770 14.6  2017-10-31T17:58:32Z\n 6 -95.3976330 29.7166110 14.7  2017-10-31T17:58:33Z\n 7 -95.3976090 29.7165850 14.7  2017-10-31T17:58:34Z\n 8 -95.3975830 29.7165300 14.7  2017-10-31T17:58:36Z\n 9 -95.3975780 29.7164950 14.7  2017-10-31T17:58:37Z\n10 -95.3975630 29.7164510 14.7  2017-10-31T17:58:38Z\n# … with 1,462 more rows\n\n\nThen we wrap this in a function.\n\nread_gpx1 <- function(fname) {\n  run_xml <- read_xml(fname)\n  run_list <- as_list(run_xml)\n\n  extract_gps_point <- function(point) {\n    tibble(\n      lon = attr(point, \"lon\"),\n      lat = attr(point, \"lat\"),\n      ele = point$ele[[1]],\n      time = point$time[[1]]\n    )\n  }\n\n  gps_df <- map_dfr(run_list$gpx$trk$trkseg, extract_gps_point)\n  get_metrics(gps_df)\n}\n\nThe next part is critical when trying to speed up code: test that the new code does the same thing as the old code.\n\nlibrary(testthat)\n\nexpected <- read_gpx0(fname)\nresult_1 <- read_gpx1(fname)\n\n# silence means everything went well\nexpect_equal(expected, result_1)\n\nThis turned out to be too slow, so we profile and see which lines are taking the most amount of time.\n\nprofvis(read_gpx1(fname))\n\n\n\n\n\nHere we see that we spend most of our time on the functions as_list and tibble."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-2-no-more-tibble",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-2-no-more-tibble",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "GPX reader version 2: no more tibble",
    "text": "GPX reader version 2: no more tibble\ntibbles are somewhat heavy objects, and we can bind lists together instead of tibbles, so let’s try that next. We only change one line from read_gpx1.\n\nread_gpx2 <- function(fname) {\n  run_xml <- read_xml(fname)\n  run_list <- as_list(run_xml)\n\n  extract_gps_point <- function(point) {\n    list(\n      lon = attr(point, \"lon\"),\n      lat = attr(point, \"lat\"),\n      ele = point$ele[[1]],\n      time = point$time[[1]]\n    )\n  }\n\n  gps_df <- map_dfr(run_list$gpx$trk$trkseg, extract_gps_point)\n  get_metrics(gps_df)\n}\n\nresult_2 <- read_gpx2(fname)\nexpect_equal(expected, result_2)\n\nOur results are still as expected, which is good. We profile again to see if we’ve done any better, which we have. Now we’re at about 1.5 seconds instead of 2.5 seconds.\n\nprofvis(read_gpx2(fname))\n\n\n\n\n\nI needed to this for about fifty files though, so this was still slow enough to be somewhat frustrating. Now xml2::as_list is really killing us."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-3-now-with-more-xml2",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-3-now-with-more-xml2",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "GPX reader version 3: now with more xml2",
    "text": "GPX reader version 3: now with more xml2\nLuckily, we can use xml2 to manipulate the XML via a fast C package instead. For this next part I tried functions exported by xml2 until they worked and occasionally read the documentation.\n\nread_gpx_xml <- function(fname) {\n  # get the interested nodes\n  run_xml <- read_xml(fname)\n  trk <- xml_child(run_xml, 2)\n  trkseg <- xml_child(trk, 2)\n  trkpts <- xml_children(trkseg) # nodeset where each node is a GPS reading\n\n  # get the longitude and latitude for each node\n  latlon_list <- xml_attrs(trkpts)\n  latlon <- do.call(rbind, latlon_list)\n\n  # get the time and elevation for each node\n  ele_time_vec <- xml_text(xml_children(trkpts))\n  ele_time <- matrix(ele_time_vec, ncol = 2, byrow = TRUE)\n  colnames(ele_time) <- c(\"ele\", \"time\")\n\n  as_tibble(cbind(latlon, ele_time))\n}\n\nread_gpx3 <- function(fname) {\n  gps_df <- read_gpx_xml(fname) |> \n    select(lon, lat, everything())\n  get_metrics(gps_df)\n}\n\nresult_3 <- read_gpx3(fname)\nexpect_equal(expected, result_3) \n\nAgain we see if there’s anywhere else we can speed things up:\n\nprofvis(read_gpx3(fname))\n\n\n\n\n\nWe’re way faster, taking less than half a second! Now the most time is spent on raster::pointDistance, which we call a ton of times. What does pointDistance do? It takes two pairs (lat1, lon1) and (lat2, lon2) the distance between them2."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-4-drop-into-rcpp",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#gpx-reader-version-4-drop-into-rcpp",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "GPX reader version 4: drop into Rcpp",
    "text": "GPX reader version 4: drop into Rcpp\nNext I Googled how to perform this calculation myself and found this and this. The Rcpp implementation looks like:\n\n#include <Rcpp.h>\n\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble haversine_dist(const NumericVector p1, const NumericVector p2) {\n  \n  double lat1 = p1[0] * M_PI / 180;\n  double lon1 = p1[1] * M_PI / 180;\n  double lat2 = p2[0] * M_PI / 180;\n  double lon2 = p2[1] * M_PI / 180;\n  \n  double d_lat = lat2 - lat1;\n  double d_lon = lon2 - lon1;\n  \n  double a = pow(sin(d_lat / 2.0), 2) + \n    cos(lat1) * cos(lat2) * pow(sin(d_lon / 2.0), 2);\n  double c = 2 * asin(std::min(1.0, sqrt(a)));\n  \n  return 6378137 * c; // 6378137 is the radius of the earth in meters\n}\n\nThe haversine distance is fast to calculate at the cost of some small error, which we can see below:\n\np1 <- c(0, 0)\np2 <- c(1, 1)\n\ndist_expected <- raster::pointDistance(p1, p2, lonlat = TRUE)\ndist_result <- haversine_dist(p1, p2)\n\ndist_result - dist_expected\n\n[1] 525.9688\n\n\nIt turns out that “small error” on the geological scale is big error on the neighborhood run scale. Put all together, the C++ version looks like:\n\nread_gpx4 <- function(fname) {\n  gps_df <- read_gpx_xml(fname)\n  get_metrics(gps_df, dist_func = haversine_dist)\n}\n\nWe profile one more time:\n\nprofvis(read_gpx4(fname))\n\n\n\n\n\nNow it takes only about 0.1 seconds, but the result isn’t accurate enough anymore. I wasn’t in the mood to implement a more precise great circle distance calculation, but hopefully this illustrates the general principle of dropping into Rcpp and also why it’s important to test when profiling."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#comparing-the-various-gpx-readers",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#comparing-the-various-gpx-readers",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "Comparing the various GPX readers",
    "text": "Comparing the various GPX readers\nNow we can compare how long each version takes using the bench package.\n\nlibrary(bench)\n\nmark(\n  read_gpx0(fname),\n  read_gpx1(fname),\n  read_gpx2(fname),\n  read_gpx3(fname),\n  read_gpx4(fname),\n  iterations = 5, # how many times to run everything. 5 is very low.\n  relative = TRUE,\n  check = FALSE # since readgpx4 isn't right, will error without this\n)\n\n# A tibble: 5 × 6\n  expression         min median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>       <dbl>  <dbl>     <dbl>     <dbl>    <dbl>\n1 read_gpx0(fname)  2.88   3.03      5.64      1.09     1.06\n2 read_gpx1(fname) 17.1   18.3       1         1.27     1   \n3 read_gpx2(fname)  4.97   4.97      3.65      1.03     1.14\n4 read_gpx3(fname)  1.59   1.60     11.2       1.00     1.40\n5 read_gpx4(fname)  1      1        16.9       1        1.05\n\n\nHere timings are relative. We see that read_gpx4 is about ten times faster than read_gpx1 and two times faster than read_gpx0."
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#embarrassing-parallelization-with-furrr",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#embarrassing-parallelization-with-furrr",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "Embarrassing parallelization with furrr",
    "text": "Embarrassing parallelization with furrr\nIn the end, I needed to do this for about fifty files. Since we can process each file independently of the other files, this operation is embarrassingly parallel. I actually wanted to use this data, so I didn’t use the C++ haversine distance function. We can write with a single map call to process all the files at once:\n\nrun_files_subset <- run_files[1:10]\n\nmap_dfr(run_files_subset, read_gpx3, .id = \"run\")\n\n# A tibble: 11,780 × 11\n   run     lon   lat time                seg…¹ seg…²      dx      dt speed  pace\n   <chr> <dbl> <dbl> <dttm>              <dbl> <drt>   <dbl>   <dbl> <dbl> <dbl>\n 1 1     -95.4  29.7 2017-10-29 18:31:00     2 907 … 0.00250 2.78e-4  9.00  6.67\n 2 1     -95.4  29.7 2017-10-29 18:31:01     2 907 … 0.00245 2.78e-4  8.81  6.81\n 3 1     -95.4  29.7 2017-10-29 18:31:02     2 907 … 0.00234 2.78e-4  8.41  7.13\n 4 1     -95.4  29.7 2017-10-29 18:31:03     2 907 … 0.00289 2.78e-4 10.4   5.77\n 5 1     -95.4  29.7 2017-10-29 18:31:04     2 907 … 0.00341 2.78e-4 12.3   4.88\n 6 1     -95.4  29.7 2017-10-29 18:31:05     2 907 … 0.00315 2.78e-4 11.4   5.29\n 7 1     -95.4  29.7 2017-10-29 18:31:06     2 907 … 0.00761 2.78e-4 27.4   2.19\n 8 1     -95.4  29.7 2017-10-29 18:31:08     2 907 … 0.00244 5.56e-4  4.39 13.7 \n 9 1     -95.4  29.7 2017-10-29 18:31:09     2 907 … 0.00322 2.78e-4 11.6   5.18\n10 1     -95.4  29.7 2017-10-29 18:31:11     2 907 … 0.00349 5.56e-4  6.28  9.55\n# … with 11,770 more rows, abbreviated variable names ¹​segment, ²​seg_length,\n#   and 1 more variable: elev <dbl>\n\n\nWhich means we can also write this as a parallelized map call with furrr like so:\n\nlibrary(furrr)\nplan(multiprocess, workers = 12)\n\nfuture_map_dfr(run_files, read_gpx3, .id = \"run\")\n\n# A tibble: 59,833 × 11\n   run     lon   lat time                seg…¹ seg…²      dx      dt speed  pace\n   <chr> <dbl> <dbl> <dttm>              <dbl> <drt>   <dbl>   <dbl> <dbl> <dbl>\n 1 1     -95.4  29.7 2017-10-29 18:31:00     2 907 … 0.00250 2.78e-4  9.00  6.67\n 2 1     -95.4  29.7 2017-10-29 18:31:01     2 907 … 0.00245 2.78e-4  8.81  6.81\n 3 1     -95.4  29.7 2017-10-29 18:31:02     2 907 … 0.00234 2.78e-4  8.41  7.13\n 4 1     -95.4  29.7 2017-10-29 18:31:03     2 907 … 0.00289 2.78e-4 10.4   5.77\n 5 1     -95.4  29.7 2017-10-29 18:31:04     2 907 … 0.00341 2.78e-4 12.3   4.88\n 6 1     -95.4  29.7 2017-10-29 18:31:05     2 907 … 0.00315 2.78e-4 11.4   5.29\n 7 1     -95.4  29.7 2017-10-29 18:31:06     2 907 … 0.00761 2.78e-4 27.4   2.19\n 8 1     -95.4  29.7 2017-10-29 18:31:08     2 907 … 0.00244 5.56e-4  4.39 13.7 \n 9 1     -95.4  29.7 2017-10-29 18:31:09     2 907 … 0.00322 2.78e-4 11.6   5.18\n10 1     -95.4  29.7 2017-10-29 18:31:11     2 907 … 0.00349 5.56e-4  6.28  9.55\n# … with 59,823 more rows, abbreviated variable names ¹​segment, ²​seg_length,\n#   and 1 more variable: elev <dbl>\n\n\nNote that other than loading furrr and calling plan(multiprocess) all we’ve had to do to get parallelism is to call furrr::future_map_dfr, which has exactly the same API as purrr::map_dfr. My computer has twelve cores, meaning there’s a maximum possible speedup of twelve.\n\nmark(\n  sequential = map_dfr(run_files, read_gpx3, .id = \"run\"),\n  parallel = future_map_dfr(run_files, read_gpx3, .id = \"run\"),\n  iterations = 5,\n  memory = FALSE,\n  relative = TRUE\n)\n\n# A tibble: 2 × 6\n  expression   min median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <dbl>  <dbl>     <dbl>     <dbl>    <dbl>\n1 sequential  5.00   4.55      1           NA     1   \n2 parallel    1      1         4.50        NA     4.10"
  },
  {
    "objectID": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#wrap-up",
    "href": "post/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/index.html#wrap-up",
    "title": "speeding up GPX ingest: profiling, Rcpp and furrr",
    "section": "Wrap Up",
    "text": "Wrap Up\nThis was a low stakes exercise in speeding up R code. By the time I’d written all of these it would have been several hundred times faster to use read_gpx0 and just save the results to a .rds file. Still, it was fun to work through the profiling workflow and I look forward to enterprising strangers on the internet pointing out places where things can get faster still.\nSee also gpx for a more modern approach to reading .gpx files in R that did not exist at the time I originally wrote this blogpost."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html",
    "title": "a summer with rstudio",
    "section": "",
    "text": "Today is the last day of my summer internship with RStudio. This is the first year that RStudio has had an official internship program, and I couldn’t be happier to have been a part of it.\nMy mandate for the summer has been to make broom better. My project was advised by both Dave Robinson and Max Kuhn. Dave originally wrote the broom package and acted as my primary mentor. Max got to sign all my paperwork and answer an unending stream of questions over Slack while he was trying to get work done."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#logistics",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#logistics",
    "title": "a summer with rstudio",
    "section": "Logistics",
    "text": "Logistics\nAfter submitting my application in March, I had video interviews with both Dave and Max1. Apparently things went well because a few days later RStudio sent me some paperwork to sign. Onboarding was all done online and was quick and painless. I started work in early June. I spent my second week on the job in Nashville for RStudio’s work week, where I met most of the RStudio employees and other interns. The rest of the summer I worked from home."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#rstudio-work-week",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#rstudio-work-week",
    "title": "a summer with rstudio",
    "section": "RStudio & Work Week",
    "text": "RStudio & Work Week\nRStudio work week was a blast. I spent the first day star-struck. For the most part we were working, but there were interludes for an intern lunch and a couple short presentations. Each work day finished with a round of lightning talks from RStudio employees and Vandy biostats professors and students. Never before in my life have I seen so many fantastic and hilarious presentations in such a short period of time.\nRStudio also had entertainment and dinner planned at a Nashville venue each evening. This was a nice way to put faces to names given that the company is mostly remote.\nMore than anything, I was surprised by how personable and empathetic everyone was. I’ve never worked for a tech company before, and I had originally envisioned a somewhat introverted and nerdy cast. This could not have been further from the truth. As I talked with other employees and tried to figure out exactly what RStudio does, people kept emphasizing the importance of empathy paired with technical skills.\nI also finally learned how RStudio makes money: they sell professional products to other companies. These products are RStudio Server Pro, Shiny Server Pro, and RStudio Connect, all of which facilitate some form of shared internal data science infrastructure. I still haven’t used them but I have a vague idea of when they might be useful."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#development-approach",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#development-approach",
    "title": "a summer with rstudio",
    "section": "Development approach",
    "text": "Development approach\nWorking with Dave was incredibly empowering. I highly recommend working with Dave if you ever get the chance.\nOur general strategy was to have one or two video calls each week. Typically we each had some priorities for broom development and would set priorities and hash out any major decisions together. Occasionally I’d pitch Dave on some overambitious project – he pretty much was always all in.\nFor the most part I had free rein to make pretty much whatever changes I felt like. Dave would occasionally comment on issues, but for the most part I was just having at on my own. This freedom was awesome."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#what-i-worked-on",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#what-i-worked-on",
    "title": "a summer with rstudio",
    "section": "What I worked on",
    "text": "What I worked on\nThe big win of the summer was the broom 0.5.0 release. broom 0.5.0 featured:\n\nA move to tibble output instead of data frames\nAn entirely new test suite\nA complete documentation overhaul\nSeveral new vignettes\n~10 new tidiers (mostly contributed)\nTons of bug fixes (mostly contributed)\n\nI won’t rehash the details, which you can find at the tidyverse blog post I linked to above, but suffice it to say the release was pretty big. A couple weeks ago I gave a presentation to the tidyverse team on my work; those slides (rough) are here."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#next-steps-for-broom",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#next-steps-for-broom",
    "title": "a summer with rstudio",
    "section": "Next steps for broom",
    "text": "Next steps for broom\nSince the broom 0.5.0 release I’ve been focused on making broom more extensible and consistent. So far this has included:\n\nMoving broom into the tidymodels organization\nA new approach to exporting generics\nMoving the test suite to the modeltests package\nAnother vignette on extending broom.\nA new approach to standardized return documentation\nExperiments with rewriting the core augment() logic for increased consistency\n\nYou can track broom’s long term goals in this thread."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#things-i-learned",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#things-i-learned",
    "title": "a summer with rstudio",
    "section": "Things I learned",
    "text": "Things I learned\n\nTechnical takeaways\nBroom is basically three R generics plus a hundred or so methods for each of those generics. None of the methods are especially long – I’d be surprised if any is more than 100 lines of code. This means that most of the technical problems in broom are isolated and easy to fix, because they don’t interact with other moving components (Dana and Tim, the ggplot2 and Shiny interns would sometimes spends several days on the same heinous bug, which I fortunately avoided!).\nThe hard part of broom is not the technical aspect of writing the tidiers, but rather the design aspect of deciding what tidiers should do. Especially for models that you’ve never used before, this can be unclear.\nI will say that working with model objects in R is a special kind of frustrating in which everybody partially follows some conventions, but you never know which ones2. For the first time in my life I found myself using test driven development.\nI didn’t produce the cleanest git history of all time, but I also didn’t bork anything up massively either, which I consider a success. I did break the build twice (?), so apologies if that affected you.\nAt the beginning of the summer, RStudio offered me a laptop to develop on, which I declined. That was a mistake. My personal computer is getting old and, more critically, runs Windows, which made some development tasks painful (revdep checks for example). The time to get to familiar with a fast new MBP from RStudio was definitely work it.\n\n\nOpen source\nI spent the first two weeks of my internship on full time bug-resolution and PR duty. broom relies on contributors for bug reports, feedback on documentation, development ideas and sanity checks. Perhaps most importantly, writing code is more fun when you know people are using it3.\n\n\n\n\n\nMy takeaways from this time was that community matters. A lot. I was surprised by the number of PRs from 12+ months ago that came back to glorious life. In general, I tried to bring enthusiasm to all my Github activity. I also embraced the emoji more fully as a tool for clarifying tone.\nI’m realizing that my willingness to contributor to open source projects (and even to use other packages) has a lot to do with how I feel about the community. Life is short, and I like writing code for kind people. The broom contributors have been fantastic to work with, and I look forward to continued collaboration.\nI’ve adopted a philosophy of Merge now, refactor later. Merged PRs generate enthusiasm, and if you need to go back and make some changes down the line, that’s okay in a project like broom. Similarly, I’ve tried to remember to have all contributors list themselves as contributors in the package description: if you put in the time to make a PR, you should get credit for it.\nIt’s become increasingly clear that broom’s success depends on high quality PRs. The people who write new tidiers typically have domain knowledge that Dave and I don’t, and it’s best to have a frequent user of a model write the tidying methods. A large portion of the summer has been spent writing documentation to make it easier to contribute these tidiers.\n\n\nWorking remotely\nI’m a fan of working remotely. Personally, I tried to stick to the 9 - 5 schedule fairly closely, with exceptions for errands and bike rides. It’s nice to not have someone looking over your shoulder, but I definitely found that I needed to be proactive about get work done. When I deviated too far from my schedule I found that I wasn’t as productive and that my stress levels went up.\nThis internship was also my first 40 hour / week job. Writing code for 8 hours a day is exhausting. I started drinking a second coffee at 5 pm to kick my brain back into gear for socializing and getting out of the house. I worked in a coffee-shop one day a week or so.\nI also started scheduling social time a few days ahead of time, because if I didn’t I found I wasn’t leaving the house enough. I was lucky to be living with my hyperactive family this summer, but anticipate that I would have needed to be even more socially proactive if I were working remotely in an apartment on my own."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#what-was-most-rewarding",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#what-was-most-rewarding",
    "title": "a summer with rstudio",
    "section": "What was most rewarding",
    "text": "What was most rewarding\nThe best of my summer was the success of the Beginner Friendly tag on Github issues. A number of people make their first ever PRs to broom, and that made me really happy. I remember having a hard time getting into open source, and I’d like to make that transition easier for others if I can (keep an eye for an upcoming post on how to make your first PR!).\n\n\n\n\n\nI also enjoyed feeling empowered to make design decisions for broom and the level of freedom I had in setting my own goals and playing around in the code base.\nSpending time with the other interns was another highlight. We had a weekly intern-only coffee chat and private slack channel that featured a lot emotional support, technical support and the party parrot emoji.\nFinally, working with broom has exposed me to a broad variety of statistical models. Over the course of the summer I’ve been writing up and slowly revising an essay on software abstractions for modeling software. Working on broom has inspired a number of interesting conversations in this vein that I look forward to continuing."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#what-could-have-gone-differently",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#what-could-have-gone-differently",
    "title": "a summer with rstudio",
    "section": "What could have gone differently",
    "text": "What could have gone differently\nIn somewhat typical fashion, I started way more projects than I could finish. Trying to wrap these up, or even just document their current status, has been somewhat stressful.\nThe one downside of my particular supervision situation was a lack of code review. It would have been nice to get some feedback on my code.\nReally, though, I have no complaints and would do it all again in a heartbeat. Working on modeling interfaces for RStudio has been the dream job and I hope I have opportunities to do similar work in the future."
  },
  {
    "objectID": "post/2018-08-10_a-summer-with-rstudio/index.html#thank-you",
    "href": "post/2018-08-10_a-summer-with-rstudio/index.html#thank-you",
    "title": "a summer with rstudio",
    "section": "Thank you!",
    "text": "Thank you!\nMy summer would not have been possible without RStudio’s internship program. Thanks to everyone who answered my questions in the RStudio slack and on Github, especially Hadley, Jenny and Jim. Dave and Max deserve special mention not only for being fantastic mentors, but also for putting up with a large number of my uninformed but nonetheless strongly held opinions.\nIf you find yourself applying to an RStudio internship program in the future, feel free to reach out on Twitter or to shoot me an email."
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "",
    "text": "This post assumes you are familiar with logistic regression and that you just fit your first or second multinomial logistic regression model. While there is an interpretation for the coefficients in a multinomial regression, that interpretation is relative to a base class, which may not be the most useful. Partial dependence plots are an alternative way to understand multinomial regression, and in fact can be used to understand any predictive model. This post explains what partial dependence plots are and how to create them using R."
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#data",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#data",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "Data",
    "text": "Data\nI’ll use the built in iris dataset for this post. If you’ve already seen the iris dataset a hundred times, I apologize. Our goal will be to predict the Species of an iris flower based on four numerical measures of the flower: Sepal.Length, Speal.Width, Petal.Length and Petal.Width. There are 150 measurements and three species of iris: setosa, versicolor and virginica.\n\nlibrary(tidyverse)\nlibrary(skimr)\n\ndata <- as_tibble(iris)\nglimpse(data)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…"
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#the-multinomial-logistic-regression-model",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#the-multinomial-logistic-regression-model",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "The multinomial logistic regression model",
    "text": "The multinomial logistic regression model\nRecall that the probability of an event \\(y = 1\\) given data \\(x \\in \\mathbb R^p\\) in a logistic regression model is:\n\\[\nP(y = 1|x) = {1 \\over 1 + \\exp(-\\beta^T x)}\n\\] where \\(\\beta \\in \\mathbb R^p\\) is a coefficient vector. Multinomial logistic regression generalizes this relation by assuming that we have \\(y \\in \\{1, 2, ..., K\\}\\). Then we have coefficient vectors \\(\\beta_1, ..., \\beta_{k-1}\\) such that\n\\[\nP(y = k|x) = {\\exp(\\beta_k^T x) \\over 1 + \\sum_{k=1}^{K - 1} \\exp(\\beta_k^T x)}\n\\]\nand\n\\[\nP(y = K|x) = {1 \\over 1 + \\sum_{k=1}^{K - 1} \\exp(\\beta_k^T x)}\n\\]\nThere are only \\(K-1\\) coefficient vectors in order to prevent overparameterization1. The purpose here isn’t to describe the model in any meaningful detail, but rather to remind you of what it looks like. I strongly encourage you to read this fantastic derivation of multinomial logistic regression, which follows the work that lead to McFadden’s Noble prize in economics in 2000.\nIf you’d like to interpret the coefficients, I recommend reading the Stata page, but I won’t rehash that here. Instead we’ll explore partial dependence plots as a way of understanding the fit model."
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#partial-dependence-plots",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#partial-dependence-plots",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "Partial dependence plots",
    "text": "Partial dependence plots\nPartial dependence plots are a way to understand the marginal effect of a variable \\(x_s\\) on the response. The gist goes like this:\n\nPick some interesting grid of points in the \\(x_s\\) dimension\n\nTypically the observed values of \\(x_s\\) in the training set\n\nFor each point \\(x\\) in the grid:\n\nReplace the \\(x_s\\) with a bunch of repeated \\(x\\)s in the training set\nCalculate the average response (class probabilities in our case)\n\n\nMore formally, suppose that we have a data set \\(X = [x_s \\, x_c] \\in \\mathbb R^{n \\times p}\\) where \\(x_s\\) is a matrix of variables we want to know the partial dependencies for and \\(x_c\\) is a matrix of the remaining predictors. Suppose we estimate some fit \\(\\hat f\\).\nThen \\(\\hat f_s (x)\\), the partial dependence of \\(\\hat f\\) at \\(x\\) (here \\(x\\) lives in the same space as \\(x_s\\)), is defined as:\n\\[\\hat f_s(x) = {1 \\over n} \\sum_{i=1}^n \\hat f(x, x_{c_i})\\]\nThis says: hold \\(x\\) constant for the variables of interest and take the average prediction over all other combinations of other variables in the training set. So we need to pick variables of interest, and also to pick a region of the space that \\(x_s\\) lives in that we are interested in. Be careful extrapolating the marginal mean of \\(f(x)\\) outside of this region!\nHere’s an example implementation in R. We start by fitting a multinomial regression to the iris dataset.\n\nlibrary(nnet)\n\nfit <- multinom(Species ~ ., data, trace = FALSE)\nfit\n\nCall:\nmultinom(formula = Species ~ ., data = data, trace = FALSE)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    18.69037    -5.458424   -8.707401     14.24477   -3.097684\nvirginica    -23.83628    -7.923634  -15.370769     23.65978   15.135301\n\nResidual Deviance: 11.89973 \nAIC: 31.89973 \n\n\nNext we pick the feature we’re interested in estimating partial dependencies for:\n\nvar <- quo(Sepal.Length)\n\nNow we can split the dataset into this predictor and other predictors:\n\nx_s <- select(data, !!var)   # grid where we want partial dependencies\nx_c <- select(data, -!!var)  # other predictors\n\nThen we create a dataframe of all combinations of these datasets2:\n\n# if the training dataset is large, use a subsample of x_c instead\ngrid <- crossing(x_s, x_c)\n\nWe want to know the predictions of \\(\\hat f\\) at each point on this grid. I define a helper in the spirit of broom::augment() for this:\n\nlibrary(broom)\n\naugment.multinom <- function(object, newdata) {\n  newdata <- as_tibble(newdata)\n  class_probs <- predict(object, newdata, type = \"prob\")\n  bind_cols(newdata, as_tibble(class_probs))\n}\n\nau <- augment(fit, grid)\nau\n\n# A tibble: 5,005 × 8\n   Sepal.Length Sepal.Width Petal.Length Petal…¹ Spe…²   setosa versi…³ virgin…⁴\n          <dbl>       <dbl>        <dbl>   <dbl> <fct>    <dbl>   <dbl>    <dbl>\n 1          4.3         2            3.5     1   vers… 2.15e-11 1.00e+0 2.34e- 7\n 2          4.3         2.2          4       1   vers… 9.89e-14 1.00e+0 6.84e- 6\n 3          4.3         2.2          4.5     1.5 vers… 4.76e-17 1.27e-1 8.73e- 1\n 4          4.3         2.2          5       1.5 virg… 3.96e-22 1.31e-3 9.99e- 1\n 5          4.3         2.3          1.3     0.3 seto… 9.99e- 1 7.32e-4 6.71e-26\n 6          4.3         2.3          3.3     1   vers… 5.06e- 9 1.00e+0 4.82e- 9\n 7          4.3         2.3          4       1.3 vers… 5.98e-13 9.99e-1 8.33e- 4\n 8          4.3         2.3          4.4     1.3 vers… 1.94e-15 9.65e-1 3.48e- 2\n 9          4.3         2.4          3.3     1   vers… 1.21e- 8 1.00e+0 2.48e- 9\n10          4.3         2.4          3.7     1   vers… 4.05e-11 1.00e+0 1.07e- 7\n# … with 4,995 more rows, and abbreviated variable names ¹​Petal.Width,\n#   ²​Species, ³​versicolor, ⁴​virginica\n\n\nNow we have the predictions and we marginalize by taking the average for each point in \\(x_s\\):\n\npd <- au %>%\n  gather(class, prob, setosa, versicolor, virginica) %>% \n  group_by(class, !!var) %>%\n  summarize(marginal_prob = mean(prob))\npd\n\n# A tibble: 105 × 3\n# Groups:   class [3]\n   class  Sepal.Length marginal_prob\n   <chr>         <dbl>         <dbl>\n 1 setosa          4.3         0.322\n 2 setosa          4.4         0.322\n 3 setosa          4.5         0.322\n 4 setosa          4.6         0.322\n 5 setosa          4.7         0.322\n 6 setosa          4.8         0.322\n 7 setosa          4.9         0.322\n 8 setosa          5           0.322\n 9 setosa          5.1         0.322\n10 setosa          5.2         0.322\n# … with 95 more rows\n\n\nWe can visualize this as well:\n\npd %>%\n  ggplot(aes(!!var, marginal_prob, color = class)) +\n  geom_line(size = 1) +\n  scale_color_viridis_d() +\n  labs(title = paste(\"Partial dependence plot for\", quo_name(var)),\n       y = \"Average class probability across all other predictors\",\n       x = quo_name(var)) +\n  theme_classic()\n\n\n\n\nI won’t show it here, but these values agree exactly with the implementation in the pdp package, which is a good sanity check on our code."
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#partial-dependence-plots-for-all-the-predictors-at-once",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#partial-dependence-plots-for-all-the-predictors-at-once",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "Partial dependence plots for all the predictors at once",
    "text": "Partial dependence plots for all the predictors at once\nIn practice it’s useful to look at partial dependence plots for all of the predictors at once. We can do this by wrapping the code we’ve written so far into a helper function and then mapping over all the predictors.\n\npartial_dependence <- function(predictor) {\n  \n  var <- ensym(predictor)\n  x_s <- select(data, !!var)\n  x_c <- select(data, -!!var)\n  grid <- crossing(x_s, x_c)\n\n  augment(fit, grid) %>% \n    gather(class, prob, setosa, versicolor, virginica) %>% \n    group_by(class, !!var) %>%\n    summarize(marginal_prob = mean(prob))\n}\n\nall_dependencies <- colnames(iris)[1:4] %>% \n  map_dfr(partial_dependence) %>% \n  gather(feature, feature_value, -class, -marginal_prob) %>% \n  na.omit()\n\nall_dependencies\n\n# A tibble: 369 × 4\n# Groups:   class [3]\n   class  marginal_prob feature      feature_value\n   <chr>          <dbl> <chr>                <dbl>\n 1 setosa         0.322 Sepal.Length           4.3\n 2 setosa         0.322 Sepal.Length           4.4\n 3 setosa         0.322 Sepal.Length           4.5\n 4 setosa         0.322 Sepal.Length           4.6\n 5 setosa         0.322 Sepal.Length           4.7\n 6 setosa         0.322 Sepal.Length           4.8\n 7 setosa         0.322 Sepal.Length           4.9\n 8 setosa         0.322 Sepal.Length           5  \n 9 setosa         0.322 Sepal.Length           5.1\n10 setosa         0.322 Sepal.Length           5.2\n# … with 359 more rows\n\n\nThen we can plot everything at once!\n\nall_dependencies %>% \n  ggplot(aes(feature_value, marginal_prob, color = class)) +\n  geom_line(size = 1) +\n  facet_wrap(vars(feature), scales = \"free_x\") +\n  scale_color_viridis_d() +\n  labs(title = \"Partial dependence plots for all features\",\n       y = \"Marginal probability of class\",\n       x = \"Value of feature\") +\n  theme_classic()\n\n\n\n\nHere we see that Sepal.Length and Sepal.Width don’t influence class probabilites that much on average, but that Petal.Length and Petal.Width do."
  },
  {
    "objectID": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#takeaways",
    "href": "post/2018-10-23_understanding-multinomial-regression-with-partial-dependence-plots/index.html#takeaways",
    "title": "understanding multinomial regression with partial dependence plots",
    "section": "Takeaways",
    "text": "Takeaways\nPartial dependence plots are useful tool to understand the marginal behavior of models. The plots are especially helpful when telling a story about what your model means. In this post, I’ve only worked with continuous predictors, but you can calculate partial dependencies for categorical predictors as well, although you’ll probably want to plot them slightly differently. Additionally, it’s natural to consider the partial dependencies of a model when \\(x_s\\) is multidimensional, in which case you can visualize marginal response surfaces.\nI recommend using the pdp package to calculate partial dependencies in practice, and refer you to Christoph Molnar’s excellent book on interpretable machine learning for additional reading."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html",
    "title": "some things i’ve learned about stan",
    "section": "",
    "text": "Yesterday, for the first time ever, I coded up a model in Stan and it actually did what I wanted. My current knowledge of Stan is, at best, nascent, but I’ll show you the process I went through to write my first Stan program, pointing out what I wish I’d known along the way.\nMy goal is to provide a quick and dirty introduction to Stan, hopefully enough to get you started without having to dig into the manual yourself. My focus here is on the language, not Bayesian inference.\nIf you’re looking for the tutorial portion, scroll down to the section called The bare minimum to get started with Stan."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#what-is-stan-and-why-might-you-want-to-use-it",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#what-is-stan-and-why-might-you-want-to-use-it",
    "title": "some things i’ve learned about stan",
    "section": "What is Stan and why might you want to use it?",
    "text": "What is Stan and why might you want to use it?\nStan is Tensorflow for generative statistical models. Just like Tensorflow lets you write the forward pass of a neural net and fits the net for you, Stan lets you write out a generative model and then gives you samples from the posterior.\nThe big win is flexibility, and creativity: if you can dream up a generative model and write it in Stan, much of the work to use that model in practice is done1.\nLike Tensorflow, the downside is the low-level interface, which can be inconvenient for day-to-day work. Luckily, Stan has brms, a much higher level interface that provides a huge amount of infrastructure to make repetitive tasks easy."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-long-road-to-actually-writing-stan",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-long-road-to-actually-writing-stan",
    "title": "some things i’ve learned about stan",
    "section": "The long road to actually writing Stan",
    "text": "The long road to actually writing Stan\nI first learned about Stan from Gelman’s blog about three years ago, and first used the Stan ecosystem when I was running into convergence issues with lme4 and my advisor suggested rstanarm:\n\n\n{{% tweet \"880583179593146368\" %}}\n\n\nMy senior year I took Daniel Kowal’s fantastic Bayes course. In that class we didn’t use Stan, but I got comfortable with Bayesian inference and MCMC and I started using brms, loo and bayesplot for personal projects.\nIntermittently I’ve tried to code up models in raw Stan. I’ve probably tried to code up a model in Stan ten or so times before this, always hitting some roadblock along the way.\nIn retrospect, I should have just sat down and read the manual sooner, but I really dislike learning new languages by reading dense technical documentation2.\nI also think there’d be a lot of value in a publicly available set of Stan exercises, where you would practice translating models from math to Stan, and from Stan to math."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-bare-minimum-to-get-started-with-stan",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-bare-minimum-to-get-started-with-stan",
    "title": "some things i’ve learned about stan",
    "section": "The bare minimum to get started with Stan",
    "text": "The bare minimum to get started with Stan\n\nWhat programs look like\nI’m not going to formally describe the structure of a Stan program. If you haven’t seen Stan code before, I think a much better approach is to go read a bunch of Stan, even if it doesn’t make sense. I’d start by taking a look through the example models in the Stan manual, then reading blog posts. The first thing you want to pay attention to is the different “blocks,” and what goes in data versus parameters versus model.\nTo get you started, here’s simple linear regression on a single predictor, taken from the Stan manual:\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\n\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\n\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\nNote that every line ends with a semi-colon (;).\n\n\nTypes\nThere are two scalar types in Stan programs: ints and reals. Integers are discrete and real numbers are continuous. You should think about these like single numbers.\nUnlike R, you’ll need to declare any objects you want to work with before you start working with them. For example\nreal alpha;\nsays that alpha is a number you want to use. Numbers can have upper and lower bounds (for example, you’ll want numbers representing variances to be non-negative):\nreal<lower=0> alpha;\nIntegers can promoted to reals, but reals are never demoted to integers. The other two objects you should know about are vectors and matrixs. vectors and matrixs contain real elements. When you declare vectors and matrixs, you have to tell Stan how big they are:\nvector[N] x;    // a vector with N elements\nmatrix[N, M] A; // an N x M matrix\nIf you declare variable bounds, you do that before the brackets:\nvector<lower=0, upper=1>[N] p;\nYou can index into vectors much like other mathematical libraries, and do things like matrix-vector multiplication:\nx[1]\nA[2, 3]\n\nA * x       // matrix vector multiplication\nx' * A * x  // calculating a quadratic form (aprostrophe means transpose)\nBasic arithmetic is probably enough to get you started, but when you need to know how to do more mathematical operations, you’ll want to consult the function reference. Be sure to bookmark both the function reference and the manual itself.\nThere are some other building blocks, but they are really just icing on top of this cake. For example, if you need a vector with elements constrained to sum to one, you’ll want to use simplex. But to understand simplex, you really only need to understand what a vector is.\n\n\nArrays & Dimension\nThe fundamental objects in Stan are ints, reals, vectors and matrixs. Oftentimes, we’ll want to work with collections of these objects. For this we need one final ingredient: arrays. Arrays in Stan work much like Numpy arrays, or MATLAB arrays, but declaring them is slightly more involved.\nSuppose we want to work with an integer array. Then we might write\nint Y[N, M];\nwhich means we want an N by M array, where each element of the array is an integer (each element in an array must be the same type). If you wanted integers between 0 and 5, you would write:\nint<lower=0, upper=5> Y[N, M];\nNote that type goes first, and the array dimensions come after the variable name. That is, if we write:\nvector[p] theta[N];\nit means we want an array with N elements, where each element is vector with p elements.\n\n\nWriting & Debugging Stan\nWhen you first get started, I recommend copy-pasting existing Stan code and modifying it, rather than writing the code from scratch. When I write Stan, I typically keep several tabs open that just have Stan models on them, and when I get stuck I scan through them for lines that look like they might work.\nI write Stan in RStudio. My best friend is the Check button, which runs the Stan parser and tells you if there are any syntax errors. Sometimes I can understand the error message, but I often end up Googling them.\nFor runtime errors, my only trick at the moment is to use the print() function, which works in Stan much like it does in R."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#success-a-first-stan-program",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#success-a-first-stan-program",
    "title": "some things i’ve learned about stan",
    "section": "Success: a first Stan program",
    "text": "Success: a first Stan program\nSuppose we have \\(N\\) survey respondents that each answer \\(Q\\) questions, where each question has \\(A\\) possible answers. There are two groups of respondents and we would like to: (1) compare how their response probabilities differ, and (2) given a new set of responses, predict which group they belong in.\nBefore you write any code at all, I highly recommend you write your model down on paper. Scribbling down something like the following always clarifies my thinking.\nLet \\(R_{i, j}\\) be the response of the \\(i^{th}\\) respondent to the \\(j^{th}\\) question, and let \\(y_i \\in \\{0, 1\\}\\) be their group membership. Then suppose\n\\[\\begin{align}\n&R_{i, j} | \\theta_j \\sim \\mathrm{Categorical}(\\theta_j) \\\\\n&\\theta_j | y_i \\sim \\mathrm{Dirichlet}(5)\n\\end{align}\\]\nSo we let each group have a different distribution of responses to each question, and shrink these distributions toward each other with a Dirichlet prior with 5 pseudo-counts in each response category. This regularization makes sense if a-priori you expect the two groups to respond in similar ways.\nLet’s start by generating some fake data from our proposed data generating process. First we input some problem size parameters and the Dirichlet prior:\n\nlibrary(tidyverse)\n\nset.seed(27)\n\nQ <- 15\nA <- 5\n\nalpha <- rep(5, A)\n\ntheta_0 <- gtools::rdirichlet(Q, alpha)\ntheta_1 <- gtools::rdirichlet(Q, alpha)\n\nNow we figure out how to sample once:\n\nsample_one <- function(theta) {\n\n  R <- numeric(Q)\n\n  for (q in 1:Q)\n    R[q] <- sample(1:A, 1, prob = theta[q, ])\n\n  names(R) <- paste0(\"q\", 1:Q)\n  as.list(R)\n}\n\nwhich naturally leads into sampling \\(n\\) times\n\nsample_n <- function(theta, n, id) {\n  samples <- map_dfr(1:n, ~sample_one(theta))\n  samples <- add_column(samples, y = id, .before = TRUE)\n  mutate_all(samples, as.integer)\n}\n\nThen we sample 35 times from group 0 and 35 times from group 1, and look at the resulting data:\n\ndf <- sample_n(theta_0, 35, id = 0) %>%\n  bind_rows(sample_n(theta_1, 35, id = 1))\n\ndf\n\n# A tibble: 70 × 16\n       y    q1    q2    q3    q4    q5    q6    q7    q8    q9   q10   q11   q12\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     0     4     1     1     5     4     5     1     1     5     4     1     3\n 2     0     2     2     4     3     4     3     2     2     3     3     2     2\n 3     0     4     2     5     3     5     1     2     4     3     3     2     2\n 4     0     2     2     1     3     1     4     2     4     4     1     2     4\n 5     0     5     1     4     4     4     1     4     4     4     1     2     1\n 6     0     3     2     5     2     3     3     4     4     1     5     4     5\n 7     0     1     4     2     1     3     3     2     1     4     1     4     1\n 8     0     4     1     3     2     1     3     3     3     3     1     2     5\n 9     0     5     1     1     1     3     1     4     4     3     1     1     3\n10     0     4     2     3     3     2     1     3     1     3     5     1     2\n# … with 60 more rows, and 3 more variables: q13 <int>, q14 <int>, q15 <int>\n\n\nNow we need some Stan code. I started by copy-pasting the code from this blog post by Jim Savage, which solves a related (but more complicated) problem. Then I blindly played with things and somehow ended up with this:\n// survey_0.stan\n\ndata {\n  int N;           // number of respondents\n  int Q;           // number of questions\n  int A;           // number of possible answers to each question\n\n  int y[N];        // group membership for each user\n  int R[N, Q];     // responses to questions\n}\n\nparameters {\n  vector[Q] alpha; // dirichlet prior\n\n  matrix[Q, A] theta_0;\n  matrix[Q, A] theta_1;\n}\n\nmodel {\n\n  for (q in 1:Q) {\n    to_vector(theta_0[q, ]) ~ dirichlet(alpha);\n    to_vector(theta_1[q, ]) ~ dirichlet(alpha);\n\n    for (i in 1:N) {\n\n      if (y[i] == 0) {\n        R[i, q] ~ multinomial(to_vector(theta_0[q, ]));\n      }\n\n      if (y[i] == 1) {\n        R[i, q] ~ multinomial(to_vector(theta_1[q, ]));\n      }\n\n      }\n    }\n  }\n}\nThis didn’t run at all, and in fact was not even syntactically correct, but it was a starting point:\n\nI got stuck here for quite a while, but the Modern Statistical Workflow slack kindly pointed out that:\n\ntheta_0 and theta_1 needed to be arrays of simplexs\nThere’s an extra } (oops!)\nThe correct way to index into a matrix is theta_1[q, :], not theta_1[q, :]\n\nAt this point I finally realized I needed to read the Stan manual in earnest, and I decided to solve a simpler problem, where both group 0 and group 1 have the same parameter theta. This eventually resulted in the following:\n\n// survey_1.stan\n\ndata {\n  int N;        // number of respondents\n  int Q;        // number of questions\n  int A;        // number of possible answers to each question\n\n  int R[N, Q];  // responses to questions\n}\n\nparameters {\n  vector<lower=0>[A] alpha;  // dirichlet prior parameter\n  simplex[A] theta[Q];       // response probabilities for each question\n}\n\nmodel {\n\n  for (q in 1:Q) {\n\n    theta[q] ~ dirichlet(alpha);\n\n    for (i in 1:N) {\n      R[i, q] ~ categorical(theta[q]);\n    }\n  }\n}\n\nNow we compile the model. Compilation takes around a minute or two, and if you mainly use R and Python like I do, it takes a while to get used to. Then we set up some Stan options:\n\nlibrary(rstan)\n\n# prevent tedious re-compilation during interactive Stan dev\nrstan_options(auto_write = TRUE)\n\n# use multiple cores during sampling. i typically leave one\n# core free to keep my potato of a computer from lagging wildly\n# during sampling\noptions(mc.cores = parallel::detectCores())\n\nFor this blog post, I’m using stan chunks in my .Rmd file, but if you’re following along, you should put the Stan code into a file survey_1.stan. In general, I recommend keeping each version of your model in a separate .stan file.\nAnyways, you would run:\n```{r}\nm1_path <- \"path/to/survey_1.stan\"\nm1 <- stan_model(m1_path)\n```\nNow that the model has compiled, we can shove some data into a list and sample from it.\n\ndata <- list(\n  R = as.matrix(select(df, -y)),\n  N = nrow(df),\n  y = df$y,\n  Q = Q,\n  A = A\n)\n\n# `refresh = 0` hides the highly verbose messages that are\n# the default during sampling. if you are using multiple cores\n# you may also want to set `show_progress = FALSE`, which\n# prevents the those messages from showing up in a pop-up window\n\nfit1 <- sampling(m1, data = data, chains = 2, iter = 1000, refresh = 0)\n\nWarning: There were 178 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: There were 129 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 2.25, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nprint(fit1, pars = \"alpha\", probs = c(0.025, 0.5, 0.975))\n\nInference for Stan model: 4c369caef54d55bc60c9ffa6271f25e2.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n             mean  se_mean       sd     2.5%      50%    97.5% n_eff Rhat\nalpha[1] 428044.5 149487.2 189861.7 158573.4 335130.0 768014.7     2 1.92\nalpha[2] 505425.2 176352.0 225541.6 190134.2 394165.7 906866.4     2 1.92\nalpha[3] 427145.8 173526.8 212211.0 144019.2 308301.4 804984.9     1 2.11\nalpha[4] 525718.6 192718.8 241149.8 187486.6 404614.1 951796.0     2 1.96\nalpha[5] 437993.4 210197.6 245454.9 128395.4 279577.4 860000.9     1 2.33\n\nSamples were drawn using NUTS(diag_e) at Fri Apr 29 15:24:19 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nIt works! That’s great, sampling is actually happening. Here I realized that I should actually pass alpha to sampling, and that I’d actually given it a hyperprior by accident. So I changed this, and gave group 0 and group 1 different parameters theta, resulting in:\n\n// survey_2.stan\n\ndata {\n  int N;  // number of respondents\n  int Q;  // number of questions\n  int A;  // number of possible answers to each question\n\n  int<lower=0, upper=1> y[N];     // binary feature for user\n  int<lower=1, upper=5> R[N, Q];  // responses to questions\n\n  vector<lower=0>[A] alpha;       // dirichlet prior parameter\n}\n\nparameters {\n  // response probabilities for each question\n  simplex[A] theta_0[Q];  // for group 0\n  simplex[A] theta_1[Q];  // for group 1\n}\n\nmodel {\n\n  for (q in 1:Q) {\n\n    theta_0[q] ~ dirichlet(alpha);\n    theta_1[q] ~ dirichlet(alpha);\n\n    for (i in 1:N) {\n\n      if (y[i] == 0) {\n        R[i, q] ~ categorical(theta_0[q]);\n      }\n\n      if (y[i] == 1) {\n        R[i, q] ~ categorical(theta_1[q]);\n      }\n\n    }\n  }\n}\n\nWe compile this and give it alpha, then take a look at the resulting theta_0:\n\ndata <- list(\n  R = as.matrix(select(df, -y)),\n  N = nrow(df),\n  y = df$y,\n  Q = Q,\n  A = A,\n  alpha = alpha\n)\n\nfit2 <- sampling(m2, data = data, chains = 2, iter = 1000, refresh = 0)\nprint(fit2, pars = \"theta_0\", probs = c(0.025, 0.5, 0.975))\n\nInference for Stan model: ab71355641fa4f454bfdbc6b4b405911.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n              mean se_mean   sd 2.5%  50% 97.5% n_eff Rhat\ntheta_0[1,1]  0.23       0 0.05 0.14 0.23  0.34  1563 1.00\ntheta_0[1,2]  0.20       0 0.05 0.12 0.20  0.30  1348 1.00\ntheta_0[1,3]  0.12       0 0.04 0.05 0.11  0.21  1345 1.00\ntheta_0[1,4]  0.32       0 0.06 0.20 0.31  0.44  1498 1.00\ntheta_0[1,5]  0.13       0 0.04 0.06 0.13  0.23  1290 1.00\ntheta_0[2,1]  0.23       0 0.05 0.14 0.23  0.34  1469 1.00\ntheta_0[2,2]  0.42       0 0.06 0.30 0.42  0.54  1466 1.00\ntheta_0[2,3]  0.10       0 0.04 0.04 0.10  0.19  1398 1.00\ntheta_0[2,4]  0.13       0 0.04 0.06 0.13  0.23  1235 1.00\ntheta_0[2,5]  0.12       0 0.04 0.04 0.11  0.21  1525 1.00\ntheta_0[3,1]  0.20       0 0.05 0.11 0.20  0.31  1504 1.00\ntheta_0[3,2]  0.17       0 0.05 0.09 0.16  0.27  1471 1.00\ntheta_0[3,3]  0.22       0 0.05 0.12 0.21  0.33  1565 1.00\ntheta_0[3,4]  0.17       0 0.05 0.08 0.16  0.27  1454 1.00\ntheta_0[3,5]  0.25       0 0.05 0.15 0.25  0.36  1500 1.00\ntheta_0[4,1]  0.20       0 0.05 0.10 0.20  0.30  1272 1.00\ntheta_0[4,2]  0.15       0 0.04 0.08 0.15  0.23  1544 1.00\ntheta_0[4,3]  0.35       0 0.06 0.23 0.35  0.49  1163 1.00\ntheta_0[4,4]  0.15       0 0.05 0.07 0.15  0.26  1295 1.00\ntheta_0[4,5]  0.15       0 0.05 0.07 0.15  0.25  1416 1.00\ntheta_0[5,1]  0.20       0 0.05 0.11 0.20  0.31  1397 1.00\ntheta_0[5,2]  0.25       0 0.06 0.15 0.25  0.37  1002 1.01\ntheta_0[5,3]  0.20       0 0.05 0.11 0.20  0.31  1598 1.00\ntheta_0[5,4]  0.18       0 0.05 0.09 0.18  0.29  1742 1.00\ntheta_0[5,5]  0.16       0 0.05 0.08 0.16  0.26  1198 1.00\ntheta_0[6,1]  0.26       0 0.06 0.16 0.26  0.38  1725 1.00\ntheta_0[6,2]  0.13       0 0.04 0.07 0.13  0.22  1613 1.00\ntheta_0[6,3]  0.29       0 0.06 0.19 0.28  0.41  1427 1.00\ntheta_0[6,4]  0.12       0 0.04 0.05 0.11  0.20  1092 1.00\ntheta_0[6,5]  0.20       0 0.05 0.11 0.20  0.31  1265 1.00\ntheta_0[7,1]  0.15       0 0.04 0.07 0.15  0.24  1489 1.00\ntheta_0[7,2]  0.34       0 0.07 0.21 0.34  0.47  1912 1.00\ntheta_0[7,3]  0.18       0 0.05 0.10 0.18  0.28  1361 1.00\ntheta_0[7,4]  0.22       0 0.06 0.12 0.21  0.34  1938 1.00\ntheta_0[7,5]  0.12       0 0.04 0.05 0.11  0.21  1392 1.00\ntheta_0[8,1]  0.20       0 0.05 0.11 0.19  0.30  1041 1.00\ntheta_0[8,2]  0.26       0 0.06 0.17 0.26  0.37  1172 1.00\ntheta_0[8,3]  0.17       0 0.05 0.08 0.16  0.27  1220 1.00\ntheta_0[8,4]  0.23       0 0.05 0.14 0.23  0.34  1216 1.00\ntheta_0[8,5]  0.14       0 0.05 0.06 0.13  0.23  1244 1.00\ntheta_0[9,1]  0.13       0 0.04 0.06 0.13  0.22  1282 1.00\ntheta_0[9,2]  0.15       0 0.05 0.07 0.15  0.25  1773 1.00\ntheta_0[9,3]  0.25       0 0.05 0.15 0.25  0.37  1884 1.00\ntheta_0[9,4]  0.23       0 0.06 0.14 0.23  0.35  1393 1.00\ntheta_0[9,5]  0.23       0 0.06 0.13 0.23  0.35  1833 1.00\ntheta_0[10,1] 0.37       0 0.06 0.26 0.37  0.50  1649 1.00\ntheta_0[10,2] 0.10       0 0.04 0.04 0.10  0.19  1524 1.00\ntheta_0[10,3] 0.20       0 0.05 0.11 0.20  0.31  1365 1.00\ntheta_0[10,4] 0.15       0 0.04 0.07 0.15  0.25  1735 1.00\ntheta_0[10,5] 0.18       0 0.05 0.10 0.18  0.29  1398 1.00\ntheta_0[11,1] 0.26       0 0.06 0.17 0.26  0.37  1942 1.00\ntheta_0[11,2] 0.28       0 0.06 0.18 0.28  0.41  1839 1.00\ntheta_0[11,3] 0.15       0 0.05 0.07 0.15  0.26  1435 1.00\ntheta_0[11,4] 0.19       0 0.05 0.10 0.18  0.28  1574 1.00\ntheta_0[11,5] 0.12       0 0.04 0.05 0.11  0.21  1496 1.00\ntheta_0[12,1] 0.25       0 0.06 0.15 0.25  0.37  1406 1.00\ntheta_0[12,2] 0.25       0 0.06 0.15 0.24  0.36  1420 1.00\ntheta_0[12,3] 0.22       0 0.05 0.13 0.21  0.33  1447 1.00\ntheta_0[12,4] 0.12       0 0.04 0.05 0.11  0.21  1465 1.00\ntheta_0[12,5] 0.17       0 0.05 0.09 0.16  0.28  1502 1.00\ntheta_0[13,1] 0.13       0 0.04 0.06 0.13  0.23  1557 1.00\ntheta_0[13,2] 0.22       0 0.05 0.12 0.22  0.33  1705 1.00\ntheta_0[13,3] 0.16       0 0.05 0.08 0.16  0.26  1796 1.00\ntheta_0[13,4] 0.30       0 0.06 0.19 0.30  0.43  1444 1.00\ntheta_0[13,5] 0.18       0 0.05 0.10 0.18  0.29  1483 1.00\ntheta_0[14,1] 0.17       0 0.05 0.09 0.16  0.26  1118 1.00\ntheta_0[14,2] 0.23       0 0.05 0.14 0.23  0.35  1917 1.00\ntheta_0[14,3] 0.18       0 0.05 0.10 0.18  0.30  1710 1.00\ntheta_0[14,4] 0.22       0 0.05 0.13 0.22  0.32  1404 1.00\ntheta_0[14,5] 0.20       0 0.05 0.11 0.20  0.30  1417 1.00\ntheta_0[15,1] 0.17       0 0.05 0.08 0.17  0.27  1443 1.00\ntheta_0[15,2] 0.12       0 0.04 0.05 0.11  0.20  1477 1.00\ntheta_0[15,3] 0.33       0 0.06 0.22 0.33  0.46  1523 1.00\ntheta_0[15,4] 0.20       0 0.05 0.11 0.20  0.31  1263 1.00\ntheta_0[15,5] 0.18       0 0.05 0.10 0.18  0.28  1479 1.00\n\nSamples were drawn using NUTS(diag_e) at Fri Apr 29 15:24:39 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nAt this point I has happy with the core logic of the model, so I used ShinyStan to check MCMC diagnostics and make sure that all the chains were mixing, etc:\nshinystan::launch_shinystan(fit2)\nThat all checked out, which isn’t a surprise, since we’re effectively estimating a bunch of proportions, so it would be weird if things started exploding.\nBut really, my original idea was to see how well this model would predict respondent group, so we need to do some more work.\nI wanted \\(P(y = 0)\\) given a person’s set of responses \\(R_{i1}, ..., R_{iA}\\). This was one of those instances were I was intimated by how to figure that out, but then it all became clear with a minute or two of pen and paper work.\nBayes rule gives us that \\(P(y_i = 0 | R_{i1}, ..., R_{iA})\\) equals\n\\[\\begin{align}\nP(R_{i1}, ..., R_{iA} | y_i = 0) P(y_i = 0) \\over\n   P(R_{i1}, ..., R_{iA} | y_i = 0) P(y_i = 0) +\n   P(R_{i1}, ..., R_{iA} | y_i = 1) P(y_i = 1)\n\\end{align}\\]\nto deal with \\(P(y_i = 0)\\) and \\(P(y_i = 1)\\) we need a prior on \\(y\\). I just assumed that \\(P(y_i = 0) = P(y_i = 1) = 0.5\\), so those terms cancel. Next we can also assume that questions are independent from other questions, and continue with:\n\\[\\begin{align}\n&= {\\prod_{j=1}^A P(R_{ij} | y_i = 0)  \\over\n  \\prod_{j=1}^A P(R_{ij} | y_i = 0) + \\prod_{j=1}^A P(R_{ij} | y_i = 1)} \\\\\n&= {\\prod_{j=1}^A \\theta_{j, 0} \\over\n  \\prod_{j=1}^A \\theta_{j, 0} + \\prod_{j=1}^A \\theta_{j, 1}}\n\\end{align}\\]\nThis is notationally intense because of all the indices, but hopefully the idea is clear. In any case, this doesn’t involve any sampling, so we code it up in the generated quantities block:\n\n// survey_3.stan\n\ndata {\n  int N;  // number of respondents\n  int Q;  // number of questions\n  int A;  // number of possible answers to each question\n\n  int<lower=0, upper=1> y[N];     // binary feature for user\n  int<lower=1, upper=5> R[N, Q];  // responses to questions\n\n  vector<lower=0>[A] alpha;       // dirichlet prior parameter\n}\n\nparameters {\n  // response probabilities for each question\n  simplex[A] theta_0[Q];  // for group 0\n  simplex[A] theta_1[Q];  // for group 1\n}\n\nmodel {\n\n  for (q in 1:Q) {\n\n    theta_0[q] ~ dirichlet(alpha);\n    theta_1[q] ~ dirichlet(alpha);\n\n    for (i in 1:N) {\n\n      if (y[i] == 0) {\n        R[i, q] ~ categorical(theta_0[q]);\n      }\n\n      if (y[i] == 1) {\n        R[i, q] ~ categorical(theta_1[q]);\n      }\n\n    }\n  }\n}\n\ngenerated quantities {\n\n  vector<lower=0, upper=1>[N] p; // probability each user is in class 0\n\n  for (i in 1:N) {\n\n    // probability of user's response for each response\n    vector[Q] pr_0;  // if they are in class 0\n    vector[Q] pr_1;  // if they are in class 1\n\n    for (q in 1:Q) {\n      \n      // get the actual response\n      int response = R[i, q];\n      \n      // get the corresponding theta, which is also\n      // the probability we're interested in\n      pr_0[q] = theta_0[q, response];\n      pr_1[q] = theta_1[q, response];\n\n    }\n\n    // multiply response probabilities for each question together\n    // and then normalize\n    p[i] = prod(pr_0) / (prod(pr_0) + prod(pr_1));\n  }\n}\n\nAnd we can throw this all at Stan and see what happens:\n\nfit3 <- sampling(m3, data = data, chains = 2, iter = 1000)\nprint(fit3, pars = \"p\", probs = c(0.025, 0.5, 0.975))\n\nInference for Stan model: fd4a6e98a27f86bc7d1b183e7c23c01a.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n      mean se_mean   sd 2.5%  50% 97.5% n_eff Rhat\np[1]  0.57    0.01 0.26 0.07 0.59  0.96  1772 1.00\np[2]  0.96    0.00 0.07 0.78 0.99  1.00   624 1.00\np[3]  0.98    0.00 0.03 0.91 0.99  1.00   949 1.00\np[4]  0.91    0.00 0.11 0.60 0.95  1.00  1110 1.00\np[5]  0.31    0.01 0.23 0.02 0.25  0.82  1296 1.00\np[6]  0.44    0.01 0.27 0.04 0.41  0.93  1354 1.00\np[7]  0.83    0.01 0.18 0.35 0.90  0.99  1058 1.00\np[8]  0.92    0.00 0.11 0.56 0.96  1.00   933 1.00\np[9]  0.73    0.01 0.23 0.20 0.80  0.99  1386 1.00\np[10] 0.95    0.00 0.07 0.76 0.98  1.00   803 1.00\np[11] 0.99    0.00 0.01 0.96 1.00  1.00   744 1.00\np[12] 0.83    0.01 0.18 0.29 0.89  0.99   855 1.00\np[13] 0.79    0.01 0.21 0.22 0.85  0.99  1153 1.00\np[14] 0.90    0.00 0.13 0.52 0.95  1.00   969 1.00\np[15] 0.86    0.01 0.17 0.34 0.93  1.00   648 1.00\np[16] 0.82    0.01 0.18 0.34 0.89  0.99   969 1.00\np[17] 0.67    0.01 0.24 0.14 0.73  0.98  1761 1.00\np[18] 0.93    0.00 0.11 0.57 0.97  1.00   873 1.00\np[19] 0.89    0.00 0.13 0.45 0.93  1.00  1072 1.00\np[20] 0.98    0.00 0.04 0.88 0.99  1.00  1073 1.00\np[21] 0.92    0.00 0.12 0.52 0.96  1.00   896 1.00\np[22] 0.89    0.01 0.14 0.43 0.95  1.00   552 1.01\np[23] 0.98    0.00 0.04 0.90 0.99  1.00   797 1.00\np[24] 0.59    0.01 0.25 0.10 0.63  0.96  1447 1.00\np[25] 0.81    0.01 0.19 0.32 0.88  0.99  1149 1.00\np[26] 0.86    0.01 0.16 0.39 0.92  1.00   754 1.00\np[27] 0.64    0.01 0.25 0.11 0.69  0.98  1152 1.00\np[28] 0.89    0.00 0.14 0.48 0.95  1.00  1261 1.00\np[29] 0.93    0.00 0.09 0.66 0.96  1.00  1214 1.00\np[30] 0.51    0.01 0.26 0.06 0.52  0.94  1563 1.00\np[31] 0.91    0.00 0.12 0.54 0.95  1.00  1090 1.00\np[32] 0.56    0.01 0.26 0.08 0.60  0.96  1198 1.00\np[33] 0.65    0.01 0.23 0.16 0.70  0.97  1775 1.00\np[34] 0.79    0.01 0.20 0.24 0.87  0.99  1477 1.00\np[35] 0.95    0.00 0.07 0.75 0.97  1.00   935 1.00\np[36] 0.47    0.01 0.26 0.05 0.46  0.93  1451 1.00\np[37] 0.53    0.01 0.27 0.06 0.55  0.96  1742 1.00\np[38] 0.30    0.01 0.25 0.01 0.23  0.88  1131 1.00\np[39] 0.18    0.01 0.18 0.01 0.11  0.68  1135 1.00\np[40] 0.26    0.01 0.23 0.01 0.18  0.83  1224 1.00\np[41] 0.14    0.01 0.16 0.00 0.08  0.63   838 1.00\np[42] 0.37    0.01 0.25 0.03 0.33  0.90  1471 1.00\np[43] 0.14    0.01 0.17 0.00 0.07  0.61   808 1.00\np[44] 0.31    0.01 0.24 0.02 0.25  0.82   989 1.00\np[45] 0.10    0.00 0.14 0.00 0.05  0.51   794 1.00\np[46] 0.58    0.01 0.25 0.09 0.60  0.96  1467 1.00\np[47] 0.08    0.00 0.12 0.00 0.04  0.45   760 1.00\np[48] 0.01    0.00 0.02 0.00 0.00  0.06   635 1.00\np[49] 0.03    0.00 0.06 0.00 0.01  0.19   756 1.00\np[50] 0.19    0.01 0.19 0.01 0.12  0.71   986 1.00\np[51] 0.14    0.00 0.15 0.01 0.08  0.57  1129 1.00\np[52] 0.29    0.01 0.22 0.02 0.23  0.80  1281 1.00\np[53] 0.05    0.00 0.07 0.00 0.02  0.25   799 1.00\np[54] 0.56    0.01 0.26 0.08 0.59  0.95  1655 1.00\np[55] 0.29    0.01 0.23 0.01 0.22  0.82  1232 1.00\np[56] 0.67    0.01 0.24 0.15 0.73  0.98  1416 1.00\np[57] 0.13    0.00 0.15 0.00 0.07  0.63   957 1.00\np[58] 0.32    0.01 0.24 0.02 0.26  0.85  1504 1.00\np[59] 0.23    0.01 0.20 0.01 0.17  0.78  1074 1.00\np[60] 0.01    0.00 0.02 0.00 0.00  0.06   816 1.00\np[61] 0.09    0.00 0.12 0.00 0.04  0.43   836 1.00\np[62] 0.44    0.01 0.27 0.03 0.43  0.94  1257 1.00\np[63] 0.23    0.01 0.19 0.01 0.18  0.72  1192 1.00\np[64] 0.01    0.00 0.03 0.00 0.00  0.07   801 1.00\np[65] 0.21    0.01 0.20 0.01 0.14  0.74  1034 1.00\np[66] 0.19    0.01 0.20 0.01 0.11  0.72  1243 1.00\np[67] 0.05    0.00 0.08 0.00 0.02  0.26   833 1.00\np[68] 0.11    0.00 0.14 0.00 0.06  0.52   888 1.00\np[69] 0.17    0.01 0.18 0.01 0.10  0.66   816 1.00\np[70] 0.18    0.01 0.19 0.01 0.10  0.67  1223 1.00\n\nSamples were drawn using NUTS(diag_e) at Fri Apr 29 15:25:00 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWhen \\(Q\\) gets big, we can have this p get wonky, turning into exact zeros or ones. At this point I remembered that multiplying probabilities together can cause an explosion because things get small very quickly, and computers do not like small numbers.\nWe can do the standard trick and take sums in log space, rather than multiplying in the original space to fix this. At the same time, let’s add in some prediction functionality. There are a couple ways to do this at the moment.\nI decided to pass unlabelled data into Stan, and wrote a function to prevent duplicate code. The function goes in its own block. It took me a while to figure out how to get the function signature right, and how variable scoping worked, but I eventually got to:\n\n// survey_4.stan\n\nfunctions {\n\n  /**\n   * class_prob\n   *\n   * Given an array of responses and probabilities for each response\n   *  for class 0 and class 1, calculate the probability that each\n   *  respondent is in class 0 via Bayes rule. In the training set,\n   *  the actual class of respondent `i` is given by `y[i]`.\n   *\n   * We assume that responses to different questions are independent\n   *  and that each class is equally likely. That is, we take\n   *  P(y=1) = P(y=0) = 0.5, and thus these terms cancel.\n   *\n   * @param R A 2-array of integers, where each row corresponds to a\n   *  a respondent, and each column corresponds to a question. Elements\n   *  can be 1, 2, ..., A.\n   *\n   * @param theta_0 A 2-array of response probabilities for class 0.\n   *  That is, `theta_0[q, r]` is the probability of (integer-valued)\n   *  response `r` to question number `q`.\n   *\n   * @param theta_1 A 2-array of response probabilities for class 1.\n   *\n   * @return A vector of probabilities that each user is in class 0.\n   *  This vector has the same number of elements as there are rows\n   *  in R.\n   */\n   \n  // note the type signatures here!\n  vector class_prob(int[,] R, vector[] theta_0, vector[] theta_1) {\n\n    real p_0;\n    real p_1;\n\n    int N = dims(R)[1];\n    int Q = dims(R)[2];\n\n    vector[N] p;\n\n    for (i in 1:N) {\n    \n      vector[Q] pr_0;\n      vector[Q] pr_1;\n\n      for (q in 1:Q) {\n\n        pr_0[q] = theta_0[q, R[i, q]];\n        pr_1[q] = theta_1[q, R[i, q]];\n\n      }\n\n      // take the product of probabilities across all questions\n      // since we assume responses to different questions are\n      // independent. work in log space for numerical stability\n\n      p_0 = exp(sum(log(pr_0)));\n      p_1 = exp(sum(log(pr_1)));\n\n      p[i] = p_0 / (p_0 + p_1);\n    }\n\n    return(p);\n  }\n}\n\ndata {\n  int Q;      // number of questions\n  int A;      // number of possible answers to each question\n\n  int N;      // number of respondents\n  int new_N;  // number of unlabelled respondents\n\n  int<lower=1, upper=5> R[N, Q];          // responses to questions (train)\n  int<lower=1, upper=5> new_R[new_N, Q];  // responses to questions (test)\n\n  int<lower=0, upper=1> y[N];             // binary feature for user\n\n  vector<lower=0>[A] alpha;               // dirichlet prior parameter\n}\n\nparameters {\n  // response probabilities for each question\n  simplex[A] theta_0[Q];  // for group 0\n  simplex[A] theta_1[Q];  // for group 1\n}\n\nmodel {\n\n  for (q in 1:Q) {\n\n    theta_0[q] ~ dirichlet(alpha);\n    theta_1[q] ~ dirichlet(alpha);\n\n    for (i in 1:N) {\n\n      if (y[i] == 0) {\n        R[i, q] ~ categorical(theta_0[q]);\n      }\n\n      if (y[i] == 1) {\n        R[i, q] ~ categorical(theta_1[q]);\n      }\n    }\n  }\n}\n\ngenerated quantities {\n  vector[N] pred = class_prob(R, theta_0, theta_1);\n  vector[new_N] new_pred = class_prob(new_R, theta_0, theta_1);\n}\n\nNow we need to generate a test set to predict on, and do the prediction.\n\nnew_df <- sample_n(theta_0, 35, id = 0) %>%\n  bind_rows(sample_n(theta_1, 35, id = 1))\n\nnew_data <- list(\n  R = as.matrix(select(df, -y)),\n  N = nrow(df),\n  y = df$y,\n  new_R = as.matrix(select(new_df, -y)),\n  new_N = nrow(new_df),\n  Q = Q,\n  A = A,\n  alpha = alpha\n)\n\nfit4 <- sampling(m4, data = new_data, chains = 2, iter = 1000, refresh = 0)\nprint(fit4, pars = \"new_pred\", probs = c(0.025, 0.5, 0.975))\n\nInference for Stan model: 7d8c081efce19d64ef1af0a50d09efed.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n             mean se_mean   sd 2.5%  50% 97.5% n_eff Rhat\nnew_pred[1]  0.53    0.01 0.26 0.08 0.54  0.94  1116 1.00\nnew_pred[2]  0.76    0.01 0.21 0.23 0.83  0.99  1226 1.00\nnew_pred[3]  0.42    0.01 0.27 0.04 0.39  0.91  1149 1.01\nnew_pred[4]  0.54    0.01 0.27 0.06 0.54  0.96  1223 1.00\nnew_pred[5]  0.84    0.01 0.17 0.39 0.91  0.99  1040 1.00\nnew_pred[6]  0.91    0.00 0.10 0.63 0.95  1.00   855 1.00\nnew_pred[7]  0.66    0.01 0.25 0.12 0.72  0.98  1117 1.00\nnew_pred[8]  0.76    0.01 0.22 0.16 0.83  0.99  1050 1.00\nnew_pred[9]  0.36    0.01 0.27 0.02 0.29  0.91  1255 1.00\nnew_pred[10] 0.72    0.01 0.23 0.20 0.79  0.98   964 1.00\nnew_pred[11] 0.79    0.01 0.20 0.26 0.86  0.99   869 1.00\nnew_pred[12] 0.98    0.00 0.04 0.88 0.99  1.00   988 1.00\nnew_pred[13] 0.72    0.01 0.23 0.17 0.78  0.99   950 1.00\nnew_pred[14] 0.66    0.01 0.25 0.14 0.71  0.98  1211 1.00\nnew_pred[15] 0.93    0.00 0.11 0.59 0.97  1.00   739 1.00\nnew_pred[16] 0.19    0.01 0.20 0.01 0.11  0.73   935 1.00\nnew_pred[17] 0.75    0.01 0.22 0.19 0.81  0.99   869 1.00\nnew_pred[18] 0.55    0.01 0.28 0.05 0.55  0.98  1231 1.00\nnew_pred[19] 0.53    0.01 0.28 0.05 0.54  0.96  1089 1.00\nnew_pred[20] 0.79    0.01 0.20 0.25 0.86  0.99  1097 1.00\nnew_pred[21] 0.94    0.00 0.10 0.66 0.98  1.00   511 1.01\nnew_pred[22] 0.49    0.01 0.27 0.06 0.48  0.94   802 1.00\nnew_pred[23] 0.86    0.01 0.16 0.38 0.92  1.00   981 1.00\nnew_pred[24] 0.64    0.01 0.24 0.12 0.68  0.97  1260 1.00\nnew_pred[25] 0.72    0.01 0.23 0.15 0.80  0.98  1015 1.00\nnew_pred[26] 0.31    0.01 0.25 0.02 0.24  0.88  1156 1.00\nnew_pred[27] 0.81    0.01 0.18 0.32 0.88  0.99   777 1.00\nnew_pred[28] 0.89    0.00 0.14 0.47 0.94  1.00   995 1.00\nnew_pred[29] 0.62    0.01 0.26 0.08 0.67  0.97  1182 1.00\nnew_pred[30] 0.92    0.00 0.11 0.59 0.96  1.00   737 1.00\nnew_pred[31] 0.83    0.01 0.18 0.37 0.89  0.99  1028 1.00\nnew_pred[32] 0.90    0.00 0.13 0.52 0.95  1.00  1081 1.00\nnew_pred[33] 0.49    0.01 0.27 0.05 0.50  0.93  1274 1.00\nnew_pred[34] 0.39    0.01 0.26 0.03 0.35  0.91  1203 1.00\nnew_pred[35] 0.66    0.01 0.25 0.13 0.70  0.98  1033 1.00\nnew_pred[36] 0.24    0.01 0.22 0.01 0.17  0.81   861 1.00\nnew_pred[37] 0.64    0.01 0.25 0.12 0.69  0.97  1234 1.00\nnew_pred[38] 0.53    0.01 0.26 0.07 0.55  0.96  1194 1.00\nnew_pred[39] 0.47    0.01 0.27 0.06 0.46  0.94  1237 1.00\nnew_pred[40] 0.07    0.00 0.11 0.00 0.03  0.38   853 1.00\nnew_pred[41] 0.11    0.00 0.15 0.00 0.05  0.56   931 1.00\nnew_pred[42] 0.12    0.00 0.14 0.00 0.06  0.51   912 1.00\nnew_pred[43] 0.53    0.01 0.27 0.06 0.55  0.96  1066 1.00\nnew_pred[44] 0.82    0.01 0.19 0.26 0.90  0.99   801 1.00\nnew_pred[45] 0.39    0.01 0.27 0.02 0.34  0.92  1057 1.00\nnew_pred[46] 0.12    0.00 0.14 0.00 0.06  0.57   998 1.00\nnew_pred[47] 0.76    0.01 0.22 0.22 0.83  0.99  1055 1.00\nnew_pred[48] 0.30    0.01 0.24 0.02 0.23  0.83  1011 1.00\nnew_pred[49] 0.37    0.01 0.25 0.03 0.32  0.87   998 1.00\nnew_pred[50] 0.14    0.01 0.16 0.00 0.08  0.63   872 1.00\nnew_pred[51] 0.14    0.01 0.16 0.00 0.07  0.60   865 1.00\nnew_pred[52] 0.71    0.01 0.24 0.13 0.76  0.99  1223 1.00\nnew_pred[53] 0.26    0.01 0.23 0.01 0.18  0.82  1013 1.00\nnew_pred[54] 0.21    0.01 0.20 0.01 0.14  0.74   915 1.00\nnew_pred[55] 0.08    0.00 0.12 0.00 0.03  0.43   651 1.00\nnew_pred[56] 0.26    0.01 0.22 0.02 0.20  0.80  1249 1.00\nnew_pred[57] 0.05    0.00 0.08 0.00 0.02  0.29   700 1.00\nnew_pred[58] 0.14    0.01 0.16 0.00 0.08  0.60   886 1.00\nnew_pred[59] 0.53    0.01 0.27 0.06 0.55  0.96  1222 1.00\nnew_pred[60] 0.47    0.01 0.28 0.04 0.44  0.95  1196 1.00\nnew_pred[61] 0.05    0.00 0.09 0.00 0.02  0.34   669 1.00\nnew_pred[62] 0.08    0.00 0.11 0.00 0.04  0.40   993 1.00\nnew_pred[63] 0.50    0.01 0.25 0.06 0.49  0.93  1197 1.00\nnew_pred[64] 0.11    0.00 0.14 0.00 0.06  0.55   794 1.01\nnew_pred[65] 0.84    0.01 0.17 0.35 0.90  0.99   833 1.00\nnew_pred[66] 0.65    0.01 0.25 0.15 0.70  0.98  1047 1.00\nnew_pred[67] 0.32    0.01 0.25 0.02 0.25  0.84  1111 1.00\nnew_pred[68] 0.38    0.01 0.27 0.02 0.33  0.92  1257 1.00\nnew_pred[69] 0.20    0.01 0.19 0.01 0.14  0.72  1003 1.00\nnew_pred[70] 0.20    0.01 0.20 0.01 0.13  0.76  1198 1.00\n\nSamples were drawn using NUTS(diag_e) at Fri Apr 29 15:25:21 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nAgain, you can have a go with shinystan::launch_shinystan(fit4) to check the MCMC diagnostics. We can quickly calculate accuracy on the test set using a prediction threshold of 0.5:\n\n#\\ warning: false\nlibrary(tidybayes)\n\n# here `tidybayes::spread_draws()` and `tidybayes::median_qi()`\n# save an immense amount of headache\n\npred <- fit4 %>%\n  spread_draws(new_pred[i]) %>%\n  median_qi(new_pred[i]) %>% \n  mutate(.pred = if_else(`new_pred[i]` > 0.5, 0, 1))\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\nacc <- round(mean(pred$.pred == new_df$y), 2)\ncat(\"The classification accuracy is:\", acc)\n\nThe classification accuracy is: 0.51\n\n\nOriginally I was planning to compare to the LASSO, since this model was inspired by a tweet that used the LASSO, but I’m running out of steam a bit so will leave that be for the moment (although I do think it would be cool to try the ExclusiveLasso to select at most a single response to each question as important).\nFinally, we can also look at how group 0 and group 1 differ by plotting theta_0 and theta_1. We munge a little bit first\n\ntheta_0_draws <- fit4 %>%  \n  spread_draws(theta_0[i, j])\n\ntheta_1_draws <- fit4 %>%  \n  spread_draws(theta_1[i, j])\n\ntheta_draws <- theta_0_draws %>% \n  left_join(theta_1_draws)\n\nJoining, by = c(\"i\", \"j\", \".chain\", \".iteration\", \".draw\")\n\n\nand then can visualize response probabilities by question and class\n\nlibrary(ggplot2)\n\ntheta_draws %>% \n  gather(group, theta, theta_0, theta_1) %>% \n  mutate(\n    group = if_else(group == \"theta_0\", \"Group 0\", \"Group 1\"),\n    question = i,\n    response = j\n  ) %>% \n  ggplot(aes(theta, fill = group, color = group)) +\n  geom_density(alpha = 0.5) +\n  facet_grid(\n    rows = vars(question),\n    cols = vars(response)\n  ) +\n  labs(\n    title = \"Estimated probability of each response by question and group\",\n    subtitle = \"Columns correspond to response, rows to questions\",\n    x = \"Probability of response\"\n  ) +\n  theme_classic() +\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)\n  )\n\n\n\n\nThe data is made up, so this isn’t terribly interesting, but if these were actual responses there might be something here.\nAt this point, we’ve got the Stan part of things working, the rest of our job is just figuring out how to munge the samples back in R."
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-end",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#the-end",
    "title": "some things i’ve learned about stan",
    "section": "The End",
    "text": "The End\nThis is pretty much everything I’ve figured out how to do in Stan. Thanks to all the people who have taken the time to answer my questions. I’m enjoying Stan quite a bit and look forward to learning more. If you have comments on how the code or documentation in the post could be improved, please let me know!"
  },
  {
    "objectID": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#resources",
    "href": "post/2018-12-24_some-things-ive-learned-about-stan/index.html#resources",
    "title": "some things i’ve learned about stan",
    "section": "Resources",
    "text": "Resources\nGetting help\n\nThe Stan Forums: If you have a question, this is probably the best place to ask it. It’s expected that you’ve already read most of the manual.\n\nBlogs and tutorials\nI recommend that you read much more Stan than you write as you’re getting started. Reading lots of Stan will acquaint you with what is possible and general conventions. khakieconomics by Jim Savage is my favorite place to read about both Stan and clever generative modeling. Mikhail Popov has a great notebook on time series innovations.\nIn terms of more didactic material, Michael Clark’s Bayesian Basics is a nice introduction to Bayes. You may also enjoy Rasmus Bååth’s exercises, which are both an introduction to both Stan and Bayesian inference.\nReference\n\nStan Manual: The canonical reference. You answer is probably somewhere in here, if you can find it.\nStan Example Models: A huge number of example implementations. Part of the Stan Manual.\nStan Function Reference: Function documentation for Stan functions."
  },
  {
    "objectID": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html",
    "href": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html",
    "title": "overlapping confidence intervals: correcting bad intuition",
    "section": "",
    "text": "In this post I work through a recent homework exercise that illustrates why you shouldn’t compare means by checking for confidence interval overlap. I calculate the type I error rate of this procedure for a simple case. This reveals where our intuition goes wrong: namely, we can recover the confidence interval heuristic by confusing standard deviations and variances."
  },
  {
    "objectID": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#checking-confidence-intervals-for-overlap",
    "href": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#checking-confidence-intervals-for-overlap",
    "title": "overlapping confidence intervals: correcting bad intuition",
    "section": "Checking confidence intervals for overlap",
    "text": "Checking confidence intervals for overlap\nSometimes you may want to check if two (or more) means are statistically distinguishable. Since proper inferential procedures can be a bit of a pain, you might be tempted to use a shortcut: plot the (\\(\\alpha\\)-level) confidence intervals for the means and check if they overlap.\nThis might seem especially useful when someone sends you plots like:\n\n\n\n\n\nIn this case you might say, oh hey, the confidence intervals for the carb and qsec coefficients overlap, so those coefficients must be different1. Or you might get a plot like the following:\n\n\n\n\n\nand do something similar. While this procedure is intuitive satisfying, you should avoid it because it doesn’t work."
  },
  {
    "objectID": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#type-i-error-rate-for-a-simple-case",
    "href": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#type-i-error-rate-for-a-simple-case",
    "title": "overlapping confidence intervals: correcting bad intuition",
    "section": "Type I error rate for a simple case",
    "text": "Type I error rate for a simple case\nTo demonstrate what can go wrong, we’ll explicitly calculate the type I error rate for a simplified case. In particular, we’ll assume that we have two samples \\(x_1, ..., x_n\\) and \\(y_1, ..., y_n\\) from a \\(\\mathrm{Normal}(\\mu, \\sigma^2)\\) distribution with known \\(\\sigma^2\\). For this example, both samples are of size \\(n\\). Here the true mean for both samples is the same value, \\(\\mu\\), and we’d like to know the probability that we reject the null of equal means using the overlapping confidence interval heuristic. That is, we want\n\\[\nP(\\text{confidence intervals overlap} | \\text{sampling situation from above})\n\\]\nThere are two ways the confidence intervals can overlap. If \\(\\color{blue}{\\bar x} < \\color{blue}{\\bar y}\\), the upper confidence bound of \\(\\color{blue}{\\bar x}\\) can be greater than the lower confidence bound of \\(\\color{blue}{\\bar y}\\). The other situation is symmetric, and since \\(P(\\color{blue}{\\bar x} < \\color{blue}{\\bar y}) = 0.5\\), we can calculate the probability of type I error for the \\(\\color{blue}{\\bar x} < \\color{blue}{\\bar y}\\) case and multiply by two to get the overall probability of type I error.\nLet’s translate the rejection condition into a mathematical statement. In particular, we reject when:\n\\[\\begin{align}\n\\color{blue}{\\bar x}  +\n  \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n}\n  &> \\color{blue}{\\bar y}  -\n  \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n}\n\\end{align}\\]\nwhere \\(\\color{blue}{\\bar x}\\) and \\(\\color{blue}{\\bar y}\\) are the sample means, \\(\\color{purple} z_\\alpha\\) is the \\(\\alpha^{th}\\) quantile of a standard normal, and \\(\\color{green} {\\sigma \\over \\sqrt n}\\) is the standard error of \\(\\color{blue}{\\bar x}\\) (and also \\(\\color{blue}{\\bar y}\\)).\nNow we rearrange and see this is equivalent to\n\\[\\begin{align}\n\\color{blue}{\\bar x}  +\n  \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n}\n  &> \\color{blue}{\\bar y}  -\n  \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n} \\\\\n\\color{purple}{z_{1 - \\alpha / 2}} \\color{green} {\\sigma \\over \\sqrt n}\n   + \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n}\n  &> \\color{blue}{\\bar y}  - \\color{blue}{\\bar x} \\\\\n2 \\cdot \\color{purple}{z_{1 - \\alpha / 2}}\n  \\color{green} {\\sigma \\over \\sqrt n}\n  &> \\color{blue}{\\bar y}  - \\color{blue}{\\bar x} \\\\\n\\sqrt 2 \\cdot \\color{purple}{z_{1 - \\alpha / 2}}\n  &> {\\color{blue}{\\bar y}  - \\color{blue}{\\bar x} \\over \\sqrt 2\n  \\cdot \\color{green} {\\sigma \\over \\sqrt n}}\n\\end{align}\\]\nand at this point you may note that the right hand side looks suspiciously like a pivot that should be standard normal. Let’s work out the distribution of \\(\\color{blue}{\\bar y} - \\color{blue}{\\bar x}\\) to see if this is the case. Recall that both \\(\\color{blue}{\\bar y}\\) and \\(\\color{blue}{\\bar x}\\) are \\(\\text{Normal}(\\mu, \\color{green} \\sigma^2 / n )\\) random variables, and they are independent.\nAdding two normals together gives us a new normal, and all that’s left to do is calculate the mean and variance of the new normal. To do this we need to use the following properties of expectation and variance:\n\\[\\begin{align}\n\\mathbb{E}(a \\cdot X + b \\cdot Y) &= a \\cdot \\mathbb{E}(X) + b \\cdot \\mathbb{E}(Y) \\\\\n\\mathbb{V}(a \\cdot X + b \\cdot Y) &= a^2 \\cdot \\mathbb{V}(X) + b^2 \\cdot \\mathbb{V}(Y)\n\\end{align}\\]\nApplying these rules, we see that\n\\[\\begin{align}\n\\color{blue}{\\bar y}  - \\color{blue}{\\bar x}\n  &\\sim \\text{Normal} \\left(0, 2 \\cdot \\color{green} {\\sigma^2 \\over n}\n   \\right) \\qquad \\text{and also that} \\\\\n{\\color{blue}{\\bar y}  - \\color{blue}{\\bar x} \\over \\sqrt 2 \\cdot \\color{green}\n  {\\sigma \\over \\sqrt n}}  &\\sim \\text{Normal}(0, 1)\n\\end{align}\\]\nNow we can calculate our overall type I error probability\n\\[\\begin{align}\nP(\\text{type I error}) &=\n  2 \\cdot P \\left({\\color{blue}{\\bar x_1}  -\n  \\color{blue}{\\bar x_2} \\over \\sqrt 2 \\color{green} {\\sigma \\over \\sqrt n}} >\n  \\sqrt 2 \\cdot \\color{purple}{z_{1 - \\alpha / 2}} \\right) \\\\\n  &= 2 \\cdot P \\left(Z > \\sqrt 2 \\cdot\n  \\color{purple}{z_{1 - \\alpha / 2}} \\right) \\\\\n  &= 2 \\cdot (1 - \\Phi(\\sqrt 2 \\cdot\n  \\color{purple}{z_{1 - \\alpha / 2}} ))\n\\end{align}\\]\nwhere \\(\\Phi\\) is the CDF of a standard normal. Effectively, we’ve picked up an extra factor of \\(\\sqrt 2\\) here. To see this, consider:\n\\[\\begin{align}\nP(\\text{type I error})\n  &= 2 \\cdot (1 - \\Phi(\\textcolor{purple}{z_{1 - \\alpha / 2}} ))\n  &\\text{throw out the $\\sqrt 2$!} \\\\\n  &= 2 \\cdot (1 - (1 - \\alpha / 2)) \\\\\n  &= 2 - 2 + 2 \\cdot \\alpha / 2 \\\\\n  &= \\alpha\n\\end{align}\\]\nSo if we drop the \\(\\sqrt 2\\) term, we get what we want!\nIn practice, though, the \\(\\sqrt 2\\) is still there, and we have this expression \\(2 \\cdot (1 - \\Phi(\\sqrt 2 \\cdot \\color{purple}{z_{1 - \\alpha / 2}} ))\\), which is a bit hard to parse. To get an idea of what this looks like, I’ve plotted \\(\\alpha\\) (used to construct the confidence intervals for mean) against the actual type I error:\n\n\n\n\n\nThis plot indicates that the actual type I error is always lower than the desired type I error rate for this problem. That is, our heuristic about overlapping confidence intervals is far too conservative and will be systematically underpowered."
  },
  {
    "objectID": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#what-went-wrong",
    "href": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#what-went-wrong",
    "title": "overlapping confidence intervals: correcting bad intuition",
    "section": "What went wrong",
    "text": "What went wrong\nAt first, your intuition may suggest that this confidence interval thing is a reasonable testing procedure, but clearly something is wrong with it. Where is our intuition leading us astray?\nLet’s go back to that missing factor of \\(\\sqrt 2\\).\nRemember that we found the variance \\(\\color{blue}{\\bar y} - \\color{blue}{\\bar x}\\) using the rule \\(\\mathbb{V}(a \\cdot X + b \\cdot Y) = a^2 \\cdot \\mathbb{V}(X) + b^2 \\cdot \\mathbb{V}(Y)\\) and it was \\(2 \\cdot \\color{green} {\\sigma^2 \\over n}\\). To get the standard error, we take the square root and get \\(\\sqrt 2 \\cdot \\color{green} {\\sigma \\over \\sqrt n}\\).\nBut if we had accidentally have worked on the standard deviation scale instead, and mistakenly assumed\n\\[\\begin{align}\n\\text{se}(a \\cdot X + b \\cdot Y) = a^2 \\cdot \\text{se}(X) + b^2 \\cdot \\text{se}(Y) \\qquad \\text{this is wrong!}\n\\end{align}\\]\nNow we would miscalculate and determine that\n\\[\\begin{align}\n\\text{se}(\\color{blue}{\\bar y}  - \\color{blue}{\\bar x})\n  = \\color{green} {\\sigma \\over \\sqrt n}  +\n  \\color{green} {\\sigma \\over \\sqrt n}\n  = 2 \\color{green} {\\sigma \\over \\sqrt n}\n   \\qquad \\text{this is also wrong!}\n\\end{align}\\]\nIf we standardize \\(\\color{blue}{\\bar y} - \\color{blue}{\\bar x}\\) by this incorrect standard error, it’s equivalent to the calculation we did above, the one that resulted in a type I error rate of \\(2 \\cdot (1 - \\Phi(\\sqrt 2 \\cdot \\color{purple}{z_{1 - \\alpha / 2}} ))\\) instead of our desired \\(\\alpha\\)!\nSo our missing factor of \\(\\sqrt 2\\) appears if we forget that confidence intervals work on the standard-deviation-scale, and we accidentally apply our variance-scale intuition to the problem. Minkowski’s inequality tells us that, for this particular setup, our mistake will always result in overestimating the true variance of \\(\\color{blue}{\\bar y} - \\color{blue}{\\bar x}\\), and thus we have a systematically underpowered test."
  },
  {
    "objectID": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#conclusion",
    "href": "post/2019-01-31_overlapping-confidence-intervals-correcting-bad-intuition/index.html#conclusion",
    "title": "overlapping confidence intervals: correcting bad intuition",
    "section": "Conclusion",
    "text": "Conclusion\nThere are a bunch of heuristics for determining if means are different based on confidence interval overlap. You shouldn’t take them too seriously. People have written great papers on this, but I seem to have misplaced my references at the moment.\nFor an interesting comparison of some of the many correct ways to compare means, check out Andrew Heiss’ recent blog post. You may also enjoy David Darmon’s very similar discussion on confidence interval procedures. He ends with the following thought-provoking call to action:\n\nLeft as an exercise for the reader: A coworker asked me, “If the individual confidence intervals don’t tell you whether the difference is (statistically) significant or not, then why do we make all these plots with the two standard errors?” … Develop an answer that (a) isn’t insulting to non-statisticians and (b) maintains hope for the future of the use of statistics by non-statisticians.\n\nIn future posts I plan build off this idea and explore statistics as a primarily sociological problem.\nCorrection: Originally this post used \\(\\color{purple}{t_{1 - \\alpha / 2, \\, n-1}}\\), but \\(\\color{purple}{z_{1 - \\alpha / 2}}\\) is the appropriate quantile for the sampling distribution of \\(\\color{blue}{\\bar y} - \\color{blue}{\\bar x}\\)."
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html",
    "title": "implementing the super learner with tidymodels",
    "section": "",
    "text": "In this post I demonstrate how to implement the Super Learner using tidymodels infrastructure. The Super Learner is an ensembling strategy that relies on cross-validation to determine how to combine predictions from many models. tidymodels provides low-level predictive modeling infrastructure that makes the implementation rather slick. The goal of this post is to show how you can use this infrastructure to build new methods with consistent, tidy behavior. You’ll get the most out of this post if you’ve used rsample, recipes and parsnip before and are comfortable working with list-columns."
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#how-do-i-fit-the-super-learner",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#how-do-i-fit-the-super-learner",
    "title": "implementing the super learner with tidymodels",
    "section": "How do I fit the super learner?",
    "text": "How do I fit the super learner?\nThe Super Learner is an ensembling strategy with nice optimality properties. It’s also not too terrible to implement:\n\nFit a library of predictive models \\(f_1, ..., f_n\\) on the full data set\nGet heldout predictions from \\(f_1, ..., f_n\\) using k-fold cross-validation\nTrain a metalearner on the heldout predictions\n\nThen when you want to predict on new data, you first run the data through \\(f_1, ..., f_n\\), then take these predictions and send them through the metalearner.\nI’ll walk through this step by step in R code, and then we’ll wrap it up into a slightly more reusable function."
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#implementation",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#implementation",
    "title": "implementing the super learner with tidymodels",
    "section": "Implementation",
    "text": "Implementation\nYou’ll want to load the requisite packages with:\n\nlibrary(tidymodels)\nlibrary(tidyr)\nlibrary(tune)\nlibrary(modeldata)\n\nlibrary(furrr)\n\n# use `plan(sequential)` to effectively convert all\n# subsequent `future_map*` calls to `map*`\n# calls. this will result in sequential execution of \n# embarassingly parallel model fitting procedures\n# but may prevent R from getting angry at parallelism\n\nplan(multicore)  \n\nset.seed(27)  # the one true seed\n\nWe’ll build a super learner on the iris dataset, which is build into R. iris looks like:\n\ndata <- as_tibble(iris)\ndata\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\n\nWe want to predict Species based on Sepal.Length, Sepal.Width, Petal.Length and Petal.Width. While this data isn’t terribly exciting, multiclass classification is the most general case to deal with. Our code should just work for binary classification, and will require only minor modifications for regression problems."
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-1-fitting-the-library-of-predictive-models",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-1-fitting-the-library-of-predictive-models",
    "title": "implementing the super learner with tidymodels",
    "section": "Step 1: Fitting the library of predictive models",
    "text": "Step 1: Fitting the library of predictive models\nFirst we need to fit a library of predictive models on the full data set. We’ll use parsnip to specify the models, and dials to specify hyperparameter grids. Both parsnip and dials get loaded when you call library(tidymodels).\nFor now we record the model we want to use. I’m going to fit C5.0 classification trees, where each tree has different hyperparameters:\n\nmodel <- decision_tree(\n  mode = \"classification\",\n  min_n = tune(),\n  tree_depth = tune()\n) %>%\n  set_engine(\"C5.0\")\n\nmodel\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: C5.0 \n\n\nIf you look at ?decision_tree, you’ll see that we need to specify two hyperparameters, min_n and tree_depth, for the C5.0 engine. To do this we’ll create a random hyperparameter grid using dials.\n\nhp_grid <- grid_random(\n  min_n() %>% range_set(c(2, 20)),\n  tree_depth(),\n  size = 10\n)\n\nhp_grid\n\n# A tibble: 10 × 2\n   min_n tree_depth\n   <int>      <int>\n 1     6         15\n 2    19         11\n 3    10         14\n 4     9          8\n 5    20          1\n 6    17         12\n 7     2          1\n 8     2         14\n 9     2         15\n10     4          8\n\n\nNow we create a tibble with a list-column of completed model specifications (C5.0 trees where we’ve specified the hyperparameter values). It’ll be useful to keep track of precisely which tree we’re working with, so we also add a model_id column:\n\nspec_df <- merge(model, hp_grid) %>%  # tune::merge(), formerly dials::merge()\n  dplyr::rename(spec = x) %>% \n  mutate(model_id = row_number())\n\nspec_df\n\n# A tibble: 10 × 2\n   spec      model_id\n   <list>       <int>\n 1 <spec[?]>        1\n 2 <spec[?]>        2\n 3 <spec[?]>        3\n 4 <spec[?]>        4\n 5 <spec[?]>        5\n 6 <spec[?]>        6\n 7 <spec[?]>        7\n 8 <spec[?]>        8\n 9 <spec[?]>        9\n10 <spec[?]>       10\n\n\nNow that we’ve specified our library of models, we’ll describe the data design we’d like to use with a recipe. For giggles, we’ll use the first two principle components:\n\nrecipe <- data %>% \n  recipe(Species ~ .) %>% \n  step_pca(all_predictors(), num_comp = 2)\n\nrecipe\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nPCA extraction with all_predictors()\n\n\nNow we can wrap up the first step and fit each of these trees on the full dataset. Here I use furrr::future_map() to do this in parallel, taking advantage of the embarrassingly parallel nature of model fitting.\n\nprepped <- prep(recipe, training = data)\n\nx <- juice(prepped, all_predictors())\ny <- juice(prepped, all_outcomes())\n\nfull_fits <- spec_df %>% \n  mutate(fit = future_map(spec, fit_xy, x, y))\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\nfull_fits\n\n# A tibble: 10 × 3\n   spec      model_id fit     \n   <list>       <int> <list>  \n 1 <spec[?]>        1 <fit[+]>\n 2 <spec[?]>        2 <fit[+]>\n 3 <spec[?]>        3 <fit[+]>\n 4 <spec[?]>        4 <fit[+]>\n 5 <spec[?]>        5 <fit[+]>\n 6 <spec[?]>        6 <fit[+]>\n 7 <spec[?]>        7 <fit[+]>\n 8 <spec[?]>        8 <fit[+]>\n 9 <spec[?]>        9 <fit[+]>\n10 <spec[?]>       10 <fit[+]>"
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-2-getting-holdout-predictions",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-2-getting-holdout-predictions",
    "title": "implementing the super learner with tidymodels",
    "section": "Step 2: Getting holdout predictions",
    "text": "Step 2: Getting holdout predictions\nWe’ll use rsample to generate the resampled datasets for 10-fold cross-validation, like so:\n\nfolds <- vfold_cv(data, v = 10)\n\nWe will want to fit a model on each fold, which is a mapping operation like before. We define a helper that will fit one of our trees (defined by a parsnip model specification) on a given fold, and pass the data in the form of a trained recipe object, which we call prepped:\n\nfit_on_fold <- function(spec, prepped) {\n  \n  x <- juice(prepped, all_predictors())\n  y <- juice(prepped, all_outcomes())\n  \n  fit_xy(spec, x, y)\n}\n\nNow we create a tibble containing all combinations of the cross-validation resamples and all the tree specifications:\n\n# note that tidyr::crossing() used to work for this operation but no\n# longer does\ncrossed <- expand_grid(folds, spec_df)\ncrossed\n\n# A tibble: 100 × 4\n   splits           id     spec      model_id\n   <list>           <chr>  <list>       <int>\n 1 <split [135/15]> Fold01 <spec[?]>        1\n 2 <split [135/15]> Fold01 <spec[?]>        2\n 3 <split [135/15]> Fold01 <spec[?]>        3\n 4 <split [135/15]> Fold01 <spec[?]>        4\n 5 <split [135/15]> Fold01 <spec[?]>        5\n 6 <split [135/15]> Fold01 <spec[?]>        6\n 7 <split [135/15]> Fold01 <spec[?]>        7\n 8 <split [135/15]> Fold01 <spec[?]>        8\n 9 <split [135/15]> Fold01 <spec[?]>        9\n10 <split [135/15]> Fold01 <spec[?]>       10\n# … with 90 more rows\n\n\nThe fitting procedure is then the longest part of the whole process, and looks like:\n\ncv_fits <- crossed %>%\n  mutate(\n    prepped = future_map(splits, prepper, recipe),\n    fit = future_map2(spec, prepped, fit_on_fold)\n  )\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nWarning: UNRELIABLE VALUE: Future ('<none>') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe random\nnumbers are produced via the L'Ecuyer-CMRG method. To disable this check, use\n'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\nNow that we have our fits, we need to get holdout predictions. Recall that we trained each fit on the analysis() set, but we want to get holdout predictions using the assessment() set. There are a lot of moving pieces here, so we define a prediction helper function that includes the original row number of each prediction:\n\npredict_helper <- function(fit, new_data, recipe) {\n  \n  # new_data can either be an rsample::rsplit object\n  # or a data frame of genuinely new data\n  \n  if (inherits(new_data, \"rsplit\")) {\n    obs <- as.integer(new_data, data = \"assessment\")\n    \n    # never forget to bake when predicting with recipes!\n    new_data <- bake(recipe, assessment(new_data))\n  } else {\n    obs <- 1:nrow(new_data)\n    new_data <- bake(recipe, new_data)\n  }\n  \n  # if you want to generalize this code to a regression\n  # super learner, you'd need to set `type = \"response\"` here\n  \n  predict(fit, new_data, type = \"prob\") %>% \n    tibble::add_column(obs = obs, .before = TRUE)\n}\n\nNow we use our helper to get predictions for each fold, for each hyperparameter combination. First we get the complete set of predictions for each fold and save them in a list-column called raw_preds. Then, since the predictions are perfectly correlated (.pred_setosa + .pred_versicolor + .pred_virginica = 1), we drop the last column of predictions to avoid issues with metalearners sensitive to colinearity. Finally, the preds column will be a list-column, so we unnest() to take a look.\n\ndrop_last_column <- function(df) df[, -ncol(df)]\n\nholdout_preds <- cv_fits %>% \n  mutate(\n    raw_preds = future_pmap(list(fit, splits, prepped), predict_helper),\n    preds = future_map(raw_preds, drop_last_column)\n  )\n\nholdout_preds %>% \n  unnest(preds)\n\n# A tibble: 1,500 × 10\n   splits           id     spec      model_id prepped  fit      raw_preds   obs\n   <list>           <chr>  <list>       <int> <list>   <list>   <list>    <int>\n 1 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     17\n 2 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     29\n 3 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     41\n 4 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     42\n 5 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     56\n 6 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     64\n 7 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     65\n 8 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     96\n 9 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>    111\n10 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>    128\n# … with 1,490 more rows, and 2 more variables: .pred_setosa <dbl>,\n#   .pred_versicolor <dbl>\n\n\nNow we have to shape this into something we can train a metalearner on, which means we need:\n\n1 row per original observation\n1 column per regression tree and outcome category\n\nGetting data into this kind of tidy format is exactly what tidyr excels at. Here we need to go from a long format to a wide format, which will often be the case when working with models in list columns1.\nThe new pivot_wider() function exactly solves this our reshaping problem once we realize that:\n\nThe row number of each observation in the original dataset is in the obs column\nThe .pred_* columns contain the values of interest\nThe model_id column identifies what the names of the new columns should be.\n\nWe’re going to need to use this operation over and over again, so we’ll put it into a function.\n\nspread_nested_predictions <- function(data) {\n  data %>% \n    unnest(preds) %>% \n    pivot_wider(\n      id_cols = obs,\n      names_from = model_id,\n      values_from = contains(\".pred\")\n    )\n}\n\nholdout_preds <- spread_nested_predictions(holdout_preds)\nholdout_preds\n\n# A tibble: 150 × 21\n     obs .pred…¹ .pred…² .pred…³ .pred…⁴ .pred…⁵ .pred…⁶ .pred…⁷ .pred…⁸ .pred…⁹\n   <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1    17 0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986  \n 2    29 0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986  \n 3    41 0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986  \n 4    42 0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986   0.986  \n 5    56 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774\n 6    64 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774\n 7    65 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774\n 8    96 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774 0.00774\n 9   111 0.00725 0.00725 0.00725 0.00725 0.00725 0.00725 0.00947 0.00947 0.00947\n10   128 0.00725 0.00725 0.00725 0.00725 0.00725 0.00725 0.00947 0.00947 0.00947\n# … with 140 more rows, abbreviated variable names ¹​.pred_setosa_1,\n#   ²​.pred_setosa_2, ³​.pred_setosa_3, ⁴​.pred_setosa_4, ⁵​.pred_setosa_5,\n#   ⁶​.pred_setosa_6, ⁷​.pred_setosa_7, ⁸​.pred_setosa_8, ⁹​.pred_setosa_9, and 11\n#   more variables: .pred_setosa_10 <dbl>, .pred_versicolor_1 <dbl>,\n#   .pred_versicolor_2 <dbl>, .pred_versicolor_3 <dbl>,\n#   .pred_versicolor_4 <dbl>, .pred_versicolor_5 <dbl>,\n#   .pred_versicolor_6 <dbl>, .pred_versicolor_7 <dbl>, …\n\n\nWe’re almost ready to fit a the metalearning model on top of these predictions, but first we need to join these predictions back to the original dataset using obs to recover the labels!\n\nmeta_train <- data %>% \n  mutate(obs = row_number()) %>% \n  right_join(holdout_preds, by = \"obs\") %>% \n  select(Species, contains(\".pred\"))\n\nmeta_train\n\n# A tibble: 150 × 21\n   Species .pred…¹ .pr…² .pr…³ .pr…⁴ .pr…⁵ .pr…⁶ .pr…⁷ .pr…⁸ .pr…⁹ .pr…˟ .pred…˟\n   <fct>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00791\n 2 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00708\n 3 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00791\n 4 setosa    0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.00709\n 5 setosa    0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.00709\n 6 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00724\n 7 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00757\n 8 setosa    0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.00692\n 9 setosa    0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.986 0.00725\n10 setosa    0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.00791\n# … with 140 more rows, abbreviated variable names ¹​.pred_setosa_1,\n#   ²​.pred_setosa_2, ³​.pred_setosa_3, ⁴​.pred_setosa_4, ⁵​.pred_setosa_5,\n#   ⁶​.pred_setosa_6, ⁷​.pred_setosa_7, ⁸​.pred_setosa_8, ⁹​.pred_setosa_9,\n#   ˟​.pred_setosa_10, ˟​.pred_versicolor_1, and 9 more variables:\n#   .pred_versicolor_2 <dbl>, .pred_versicolor_3 <dbl>,\n#   .pred_versicolor_4 <dbl>, .pred_versicolor_5 <dbl>,\n#   .pred_versicolor_6 <dbl>, .pred_versicolor_7 <dbl>, …"
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-3-fit-the-metalearner",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#step-3-fit-the-metalearner",
    "title": "implementing the super learner with tidymodels",
    "section": "Step 3: Fit the metalearner",
    "text": "Step 3: Fit the metalearner\nI’m going to use a multinomial regression as the metalearner. You can use any metalearner that does multiclass classification here, but I’m going with something simple because I don’t want to obscure the logic with additional hyperparameter search here.\n\n# these settings correspond to multinomial regression\n# with a small ridge penalty. the ridge penalty makes\n# sure this doesn't explode when the number of columns  \n# of heldout predictions is greater than the number of\n# observations in the original data set\n#\n# in practice, you'll probably want to avoid base learner\n# libraries that large due to difficulties estimating\n# the relative performance of the base learners\n\nmetalearner <- multinom_reg(penalty = 0.01, mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  fit(Species ~ ., meta_train)\n\nThat’s it! We’ve fit the super learner! Just like the training process, prediction itself proceeds involves two separate stages:\n\nnew_data <- head(iris)\n\n# run the new data through the library of base learners first\n\nbase_preds <- full_fits %>% \n  mutate(  \n    raw_preds = future_map(fit, predict_helper, new_data, prepped),\n    preds = future_map(raw_preds, drop_last_column)\n  ) %>% \n  spread_nested_predictions()\n\n# then through the metalearner\n\npredict(metalearner, base_preds, type = \"prob\")\n\n# A tibble: 6 × 3\n  .pred_setosa .pred_versicolor .pred_virginica\n         <dbl>            <dbl>           <dbl>\n1        0.949           0.0384          0.0130\n2        0.949           0.0384          0.0130\n3        0.949           0.0384          0.0130\n4        0.949           0.0384          0.0130\n5        0.949           0.0384          0.0130\n6        0.949           0.0384          0.0130"
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#putting-it-all-together",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#putting-it-all-together",
    "title": "implementing the super learner with tidymodels",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow we can take all the code we’ve written up and encapsulate it into a single function (still relying on the helper functions we defined above).\nNote that this is a reference implementation and in practice I recommend following the tidymodels recommendations when implementing new methods. Luckily, we do end up inherit a fair amount of nice consistency from parsnip itself.\n\n#' Fit the super learner!\n#'\n#' @param library A data frame with a column `spec` containing\n#'   complete `parsnip` model specifications for the base learners \n#'   and a column `model_id`.\n#' @param recipe An untrained `recipe` specifying data design\n#' @param meta_spec A singe `parsnip` model specification\n#'   for the metalearner.\n#' @param data The dataset to fit the super learner on.\n#'\n#' @return A list with class `\"super_learner\"` and three elements:\n#'\n#'   - `full_fits`: A tibble with list-column `fit` of fit\n#'     base learners as parsnip `model_fit` objects\n#'\n#'   - `metalearner`: The metalearner as a single parsnip\n#'     `model_fit` object\n#'\n#'   - `recipe`: A trained version of the original recipe\n#'\nsuper_learner <- function(library, recipe, meta_spec, data) {\n  \n  folds <- vfold_cv(data, v = 5)\n  \n  cv_fits <- expand_grid(folds, library) %>%\n    mutate(\n      prepped = future_map(splits, prepper, recipe),\n      fit = future_pmap(list(spec, prepped), fit_on_fold)\n    )\n  \n  prepped <- prep(recipe, training = data)\n  \n  x <- juice(prepped, all_predictors())\n  y <- juice(prepped, all_outcomes())\n  \n  full_fits <- library %>% \n    mutate(fit = future_map(spec, fit_xy, x, y))\n  \n  holdout_preds <- cv_fits %>% \n    mutate(\n      raw_preds = future_pmap(list(fit, splits, prepped), predict_helper),\n      preds = future_map(raw_preds, drop_last_column)\n    ) %>% \n    spread_nested_predictions() %>% \n    select(-obs)\n  \n  metalearner <- fit_xy(meta_spec, holdout_preds, y)\n  \n  sl <- list(full_fits = full_fits, metalearner = metalearner, recipe = prepped)\n  class(sl) <- \"super_learner\"\n  sl\n}\n\nWe also write an S3 predict method:\n\npredict.super_learner <- function(x, new_data, type = c(\"class\", \"prob\")) {\n  \n  type <- rlang::arg_match(type)\n  \n  new_preds <- x$full_fits %>% \n    mutate(\n      raw_preds = future_map(fit, predict_helper, new_data, x$recipe),\n      preds = future_map(raw_preds, drop_last_column)\n    ) %>% \n    spread_nested_predictions() %>% \n    select(-obs)\n    \n  predict(x$metalearner, new_preds, type = type)\n}\n\nOur helpers do assume that we’re working on a classification problem, but other than this we pretty much only rely on the parsnip API. This means we can mix and match parts to our hearts desire and things should still work2. For example, we can build off the parsnip classification vignette, which starts like so:\n\n# we have to do this because `modeldata` doesn't use lazy data loading\ndata(\"credit_data\")  \n\ndata_split <- credit_data %>% \n  na.omit() %>% \n  initial_split(strata = \"Status\", prop = 0.75)\n\ncredit_train <- training(data_split)\ncredit_test  <- testing(data_split)\n\ncredit_recipe <- recipe(Status ~ ., data = credit_train) %>%\n  step_center(all_numeric()) %>%\n  step_scale(all_numeric())\n\nBut now let’s fit a Super Learner based on a stack of MARS fits instead of a neural net. You could also mix in other arbitrary models3. First we take a moment to set up the specification:\n\n# needed due to a namespace bug at the moment,\n# but not in general\nlibrary(earth)  \n\ncredit_model <- mars(mode = \"classification\", prune_method = \"backward\") %>% \n  set_engine(\"earth\")\n\ncredit_hp_grid <- grid_random(\n  num_terms() %>% range_set(c(1, 30)),\n  prod_degree(),\n  size = 5\n)\n\ncredit_library <- merge(credit_model, credit_hp_grid) %>% \n  dplyr::rename(spec = x) %>% \n  mutate(model_id = row_number())\n\ncredit_meta <- multinom_reg(penalty = 0, mixture = 1) %>% \n  set_engine(\"glmnet\")\n\nNow we do the actual fitting and take a quick coffee break:\n\ncredit_sl <- super_learner(\n  credit_library,\n  credit_recipe,\n  credit_meta,\n  credit_train\n)\n\nSince we inherit the tidymodels predict() conventions, getting a holdout ROC curve is as easy as:\n\npred <- predict(credit_sl, credit_test, type = \"prob\")\n\npred %>% \n  bind_cols(credit_test) %>% \n  roc_curve(Status, .pred_bad) %>%\n  autoplot()"
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#wrap-up",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#wrap-up",
    "title": "implementing the super learner with tidymodels",
    "section": "Wrap up",
    "text": "Wrap up\nThat’s it! We’ve fit a clever ensembling technique in a few lines of code! Hopefully the concepts are clear and you can start to play with ensembling on your own. I should note that this post uses a ton of different tidymodels abstractions, which can be intimidating. The goal here is to demonstrate how to integrate all of various components together into a big picture. If you aren’t familiar with the individual tidymodels packages, my impression is that the best way to gain this familiarity is by gradually working through the various tidymodels vignettes.\nIn practice, it is a bit of relief to be done with this post. I’ve been playing around with implementing the Super Learner since summer 2017, but each time I gave it a shot things got messy much faster than I anticipated and I kicked the task down the line. Only recently have the tools to make the Super Learner implementation so pleasant come to life4. Thanks Max and Davis!\nIf you want to use the Super Learner in practice, I believe the sl3 package is the most actively developed. There’s also Eric Polley’s classic SuperLearner package, which may be more full featured than sl3 at the moment. Also be sure to check out h2o::automl(), which makes stacking about as painless as can be if you just need results!"
  },
  {
    "objectID": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#references",
    "href": "post/2019-04-13_implementing-the-superlearner-with-tidymodels/index.html#references",
    "title": "implementing the super learner with tidymodels",
    "section": "References",
    "text": "References\nIf you’re new to the Super Learner, I recommend starting with LeDell (2015a). Section 2.2 of LeDell (2015b) is similar but goes into more detail. Laan, Polley, and Hubbard (2007) is the original Super Learner paper and contains the proof the oracle property, an optimality result. Polley and Laan (2010) discusses the Super Learner from a more applied point of view, with some simulations demonstrating performance. Laan and Rose (2018) is a comprehensive reference on both the Super Learner and TMLE. The Super Learner papers and book are targeted at a research audience with a high level of mathematical background, and are not easy reading. Wolpert (1992) is another often cited paper on stacking that is more approachable."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html",
    "href": "post/2019-05-21_type-stable-estimation/index.html",
    "title": "type stable estimation",
    "section": "",
    "text": "This post discusses how the mathematical objects we use in formal data modeling are represented in statistical software. First I introduce these objects, then I argue that each object should be represented by a distinct type. Next I present three principles to ensure the type system is statistically meaningful. These principles suggest that existing modeling software has an overly crude type system. I believe a finer type system in statistical packages would result in more intuitive interfaces while increasing extensibility and reducing possibilities for methodological errors.\nAlong the way, I briefly discuss the difference between a type and a class, and some of the tradeoffs we make when using object orientation for statistical modeling. After remaining language agnostic up to this point, I dive into the practical difficulties of modeling with the S3 class system from R, although I also mention Python and Julia briefly. You’ll get the most out of this post if you’ve read the chapters on Object Orientation in Hadley Wickham’s Advanced R."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#what-is-good-modeling-software",
    "href": "post/2019-05-21_type-stable-estimation/index.html#what-is-good-modeling-software",
    "title": "type stable estimation",
    "section": "What is good modeling software?",
    "text": "What is good modeling software?\nUnfortunately, not all modeling code is good code1. But what makes modeling code good? Here is what I find most important:\n\nCorrectness: The code returns computationally correct results, and this has been verified by extensive testing2.\nDocumentation: It is easy to determine what computation the code has actually performed.\nIntuitive interface: Users can easily make the software do what they want it to do.\nExtensibility: Researchers can extend the software to broader classes or models and build on existing work.\nReadability: It is easy to understand the code base, such that the method can be translated into other languages or reimplemented in different manners.\nPerformance: The code runs quickly, uses minimal memory and scales to large data3.\n\nFor the purpose of this post, extensibility turns out to be the most important."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#objects-in-play",
    "href": "post/2019-05-21_type-stable-estimation/index.html#objects-in-play",
    "title": "type stable estimation",
    "section": "Objects in play",
    "text": "Objects in play\nBroadly speaking, there are three stages in formal data modeling:\n\nIdentification\nEstimation\nInference\n\nIn the identification stage, data analysts make assumptions about the data generating processing and determine which models may be appropriate for their data. They then determine which properties of those models they would like to estimate. These targets are called estimands. Both of these steps are largely conceptual work.\nIn the estimation stage, the analyst chooses an estimator to use for each estimand. Different estimators have different statistical properties under different data generating process. Again, selecting an appropriate estimator is conceptual work. Next comes actual estimation, where the analyst uses statistical software to calculate actual estimates for their estimands of interest. This happens on the computer. Analysts typically want to know the value of a whole collection of estimands: we call the corresponding collection of estimates a fit4.\nFinally, in the inference stage, an analyst interprets the resulting fit and comes to substantive conclusions. To do so, they perform follow-up computations that combine multiple estimands together, estimate new fits and compare existing fits.\nTo make the computational parts of this process easy for users and developers, we have to create code objects that correspond to the mathematical objects of models, estimands and estimators. Unfortunately, the distinctions between statistical objects have been widely disregarded in modeling software, resulting in mass conflation of models, estimators, and the algorithms used to compute estimators."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#principles-for-type-stable-estimation",
    "href": "post/2019-05-21_type-stable-estimation/index.html#principles-for-type-stable-estimation",
    "title": "type stable estimation",
    "section": "Principles for type stable estimation",
    "text": "Principles for type stable estimation\nI propose the following three principles to organize statistical software:\n\nProvide types that correspond to fits and fit specifications . Users need to be able to specify collections of estimands, and also to specify estimators they would like to use for each estimand. In existing modeling software, estimand and estimator selection is almost always implicit and inflexible. For example, stats::lm() always calculates a fixed set of estimates with fixed estimators for the linear model via fixed algorithms. We should fundamentally work with fit specifications, declarative requests describing what to calculate, and fits, the collections of resulting estimates.\nEstimate uniquely. For any given estimand, only calculate estimates with a single estimator. That is, you should never have two different estimates for the same estimand living in the same fit object. Also, fits with different sets of estimands should have different types.\nDefine methods by their required estimator inputs. If two statistical procedures operate on distinct estimators, their methods must be distinct. This requirement ensures that we do not define methods if they are not statistically meaningful.\n\nWhen a modeling package meets these requirements, I say that it performs type stable estimation, although this is somewhat an abuse of language. Nonetheless, I believe these principles are the minimal conditions that result in consistent statistical software. Primarily these rules suggest that we need a much finer type system for statistical objects than those currently in use. Note that these principles are not tied to any particular form of object-orientation."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#types-and-classes",
    "href": "post/2019-05-21_type-stable-estimation/index.html#types-and-classes",
    "title": "type stable estimation",
    "section": "Types and classes",
    "text": "Types and classes\nA type is an abstract definition of the kinds of operations you can perform on an object5. A class is an implementation of a type. I’m advocating for a type system with statistically meaningful types, together with type stability. Type stability means that “you can predict the output type based only on the types of its inputs”, to paraphrase the tidyverse principles. The idea is that we want to design our types to represent the modeling problem space, so that logic errors become easily identifiable type mismatches6.\nI’ve phrased my principles in terms of types rather than classes because the particular implementations matter less than the contract that describes how those objects work. Notably, I am not advocating for static typing or type safety."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#two-approaches-to-object-orientation",
    "href": "post/2019-05-21_type-stable-estimation/index.html#two-approaches-to-object-orientation",
    "title": "type stable estimation",
    "section": "Two approaches to object orientation",
    "text": "Two approaches to object orientation\nTo implement types, we need to use some form of object oriented programming. Recall from Advanced R the two main approaches to object-oriented programming:\n\n\nIn encapsulated OOP, methods belong to objects or classes, and method calls typically look like object.method(arg1, arg2). This is called encapsulated because the object encapsulates both data (with fields) and behaviour (with methods), and is the paradigm found in most popular languages.\n\n\n\n\nIn [generic function] OOP, methods belong to generic functions, and method calls look like ordinary function calls: generic(object, arg2, arg3). This is called functional because from the outside it looks like a regular function call, and internally the components are also functions.\n\n\nIn practice, however, we do not have much choice at all. Most languages only provide first class support for a single form of object orientation. Python primarily supports encapsulated OOP. R and Julia primarily support functional OOP. This last bit isn’t quite correct, as R provides support for encapsulated OOP via R6 classes, but there is a long standing tradition of functional OOP for modeling. In Julia you can recreate encapsulated OOP as well, but again, this type of code is not idiomatic to the best of my knowledge7.\nPersonally I’m convinced modeling methods should belong to generic functions, not classes, but again, it’s a bit of a moot point8.\n\nObject orientation enables polymorphism\nWhile modeling software makes use of object orientation, I want to point out that the motivation is often different from traditional reasons. One classic take motivates object orientation by arguing that classes allow developers to make fields private. This forces developers to use the class interface, decoupling interface from implementation and (theoretically) making it easier to reason about complex systems.\nIn the modeling contexts, our motivation is different, and we use object orientation primarily to take advantage of polymorphism9. We do the same statistical computations over and over, on slightly different objects. Think about prediction. We do prediction with literally thousands of different models. Polymorphism solves an interface problem more than anything else, allowing predict() to mean “calculate \\(\\mathbb{E}(Y|X^*)\\)” regardless of the underlying model.\nImagine the modeling world without polymorphism. Now you need a different predict_linear_model(), predict_logistic_reg(), etc, etc, and you immediately end up with a redundant and polluted namespace. From this point of view, classes are only attributes that enable polymorphic dispatch.\nIf we don’t use some sort of object-orientation, we effectively reduce modeling code to functions that return collections of estimates, and functions that operate on those estimates. If you’ve used R long enough, you’ve almost certainly come across professor code that does exactly this: it throws a bunch of matrices and vectors into a list and calls it a day10. This also makes it near impossible to extend the method: how do you distinguish between those estimates and an arbitrary collection of arrays in a list?\n\n\nAside: multiple dispatch\nIn my ideal world, statistical methods would not only correspond to functional OOP methods for generic functions, but the generic functions would allow for multiple dispatch. Multiple dispatch means that generic functions determine which method to call based on all of their arguments, rather than just the first argument, as is the case with R’s S3 class system.\nIn fact, both R and Julia have multiple dispatch. In R, multiple dispatch is part of the S4 class system (R has three total classes systems if you’ve been keeping track: S3, R6 and S4). But pretty much no one enjoys S4 except geneticists with stockholm syndrome. Also, S4 is too complicated to establish as a broad standard11.\nJulia, however, has excellent support for multiple dispatch, and makes heavy use of multiple dispatch idioms throughout the language. See the glorious Distributions.jl package for some examples of how pleasant this can make things. At some point I may give Julia a more serious try, but right now the R community is too welcoming and the Julia statistics ecosystem too nascent for me to do much more than keep a half-hearted eye on the language12."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#just-say-no-to-inheritance",
    "href": "post/2019-05-21_type-stable-estimation/index.html#just-say-no-to-inheritance",
    "title": "type stable estimation",
    "section": "Just say no to inheritance",
    "text": "Just say no to inheritance\nMost programmers who I talk to have similar opinions about encapsulated OOP: (1) it’s magical when you get it right, (2) it’s awful when you get it wrong, and (3) it’s pretty damn hard to get it right. Typically, we think about inheritance as a mechanism for reusing code and following that sweet, sweet mandate that we Don’t Repeated Ourselves13. The primary tool we use to prevent repeated code is inheritance.\nThe issue with inheritance is that it works best in closed contexts, when you know what you’d like to do ahead of time, and potential changes to the fundamental logic of workflow and computation are small.\nThis is decidedly not the case with statistical modeling. Researchers are constantly coming up with new techniques that operate outside the current modeling idioms and extending existing models in unexpected ways.\nAs a result, it’s hard to make encapsulated model objects extensible14. Consider the case of a researcher who comes up with a new diagnostic for linear regression. How should they implement their method in an encapulated OOP modeling world?\nOne option is for the researcher to open pull requests to popular packages the define linear regression classes. This creates a lot of extra friction for the researcher, who doesn’t want to deal with the idiosyncracy of those packages or delays in getting pull requests approved, or the difficulty of developing the software privately before the paper is released, etc. Also, how many packages should they make pull requests to? All of the ones that implement linear regression? Only some? Which ones?\nA second option is for the researcher to define a new class B themselves, and to either subclass or superclass the existing implementation A. Suppose they subclass A. First observe that doing so will violate the principle of unique estimation if they are proposing a new estimator for an existing estimand.\nBut even if the researcher has come up with an entirely new estimand, when someone goes to extend class B, what should they inherit from? The original class? The extended class? Both? As this chain of extension continues, it probably makes less and less sense for these classes to share code, and in any case you quickly find yourself in the hellscape of multiple inheritance.\nA third, and more palatable option, is to inherit from an interface or some notion of an abstract class. I’m curious to explore this idea more, but haven’t spent much time on it just yet."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#the-bizarre-and-occasionally-beautiful-world-of-s3",
    "href": "post/2019-05-21_type-stable-estimation/index.html#the-bizarre-and-occasionally-beautiful-world-of-s3",
    "title": "type stable estimation",
    "section": "The bizarre and occasionally beautiful world of S3",
    "text": "The bizarre and occasionally beautiful world of S3\nIf anyone from the Julia community is reading this, this is the point where I apologize, because I haven’t used Julia at all for modeling, and so my discussion of generic function OOP for modeling is really just some commentary on using R’s S3 class system for modeling. Buckle up, it’s a wild ride.\nThe primary benefit of S3 is that it is extensible. This is also the primary downside of S3. For example, if you come from an encapsulated OOP background, you will probably be horrified to learn that R lets you set the class of any object on the fly, like this:\n\n# get a collection of OLS related estimates\n# from the linear model\nfit <- lm(hp ~ mpg + am, data = mtcars)\nclass(fit)\n\n[1] \"lm\"\n\n# oh god the horror\nclass(fit) <- \"curious_george\"\nclass(fit)\n\n[1] \"curious_george\"\n\n\nThere is also no such thing as a private field in S3, so you can modify any element of an S3 object at will. For the most part I find the generic functions of S3 to be a pleasant fit for statistical modeling, but I’d like to highlight some of the anti-patterns I come across more frequently.\n\nWilly-nilly and diabolical construction of model objects\nSince there are no formal constructors for S3 objects, you have very few guarantees about S3 objects you come across in wild. Package authors often create S3 objects in different ways in different places15. Then it’s a fun game of Russian roulette for downstream users and developers to figure out which version of the object they actually got. Trying to write reliable code to process and extend these objects is fraught with difficulty since you’re trying to build on top of a house of cards.\nIn R lm objects are a canonical example. A bunch of people wanted to replicate the functionality of lm objects without writing their own methods, so they just subclassed lm. This works to varying degrees, but makes lm objects difficult to reason about.\nSimilarly, at some point anova() went from a function for calculating F-tests to a general purpose tool for model comparison. Everybody and their grandma has come up with something vaguely F-test or AIC related and slammed it into the resulting anova object. Try to manage all the possible results is not fun. You can see how broom tries to tidy() anova objects here. Every time I try this and it actually works I am somewhat astonished.\nThe solution to this problem is to use constructors and validators, as described in Advanced R. I highly, highly recommend reading that chapter carefully. Hopefully this advice will start filtering down and bring some sanity to the madness. I suspect there’ll be mixed success, mostly because doing S3 right takes a fair amount of work, and nobody has time for that.\nAlso note that using validators doesn’t actually go very far toward solving the S3 object construction issue in modeling. Once you have estimates in hand, there is often no way to tell which estimator they came from. So you could do something like construct an lm object using HC1 covariance estimates and the resulting object can be exactly the same in terms of fields and types as a true lm object, just with slightly different values for the covariance matrix. There’s no way to detect this (to my knowledge) without knowing how a given object was constructed."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#convenience-functions",
    "href": "post/2019-05-21_type-stable-estimation/index.html#convenience-functions",
    "title": "type stable estimation",
    "section": "Convenience functions",
    "text": "Convenience functions\nOne common pattern in R is to allow users to fit many different models via a single function, such as glm(), lme4::glmer(), brms::brm(), etc. I refer to these functions as convenience functions. Sometimes these functions only allow users to get fits from a single estimator per model (glm() and brms::brm()), sometimes users can get fits corresponding to multiple estimators (lme4::lmer() and stats::arima()) and sometimes both estimators and models are allowed to vary (metafor::rma()).\nThe practical implications of this have come up a lot in broom, where having a single class represent fits from multiple models (or estimators) somewhat frequently leads to a headache.\nI’m curious to think more about this approach. On one hand, this sort of high-level interface is undeniably useful and user-friendly. Putting “inferentially close” models close to each other in user-interface space is a smart idea. On the other hand, the resulting objects are often quite difficult to program with and extend. Using constructors would go a long way towards making things nicer.\nThis is also one circumstance where I think inheritance is a good idea. It’s useful to give objects constructed from the same convenience function a consistent class. Most R packages authors do this. It less common to respect estimand uniqueness and give the resulting objects meaningful subclasses16. For example, poisson and binomial GLMs are both glm objects, which makes it difficult to write methods extending poisson GLMs but not binomial GLMs.\nIf you are a package author exporting convenience functions, I strongly encourage you to: use a class (glm) to prevent code duplication, but also use give objects meaningful subclasses (poisson_fit, binomial_fit) to enable other people to program on top of the results."
  },
  {
    "objectID": "post/2019-05-21_type-stable-estimation/index.html#takeaways",
    "href": "post/2019-05-21_type-stable-estimation/index.html#takeaways",
    "title": "type stable estimation",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe should be thinking in terms of estimators, estimands and unique estimation, but we aren’t.\nThe extensibility of modeling code depends heavily on OOP design decisions.\nCurrent practices in the S3 modeling world aren’t quite enough to build reliable software on top of. That said, S3 is still probably better than S4.\n\nThanks to Amos Elberg, Achim Zeileis, Matthieu Stigler, Keiran Thompson, Adam Kelleher, Max Kuhn and Colin Carroll for an interesting discussion on OOP for modeling, as well as the numerous broom contributors who discussed object orientation in this broom issue. I encourage interested parties to take a read for more concrete examples of some of the principles I’ve discussed here.\nYou can also watch me struggle to articulate related ideas about estimand uniqueness in parsnip issues #19 and #54, although my thinking has changed somewhat since then. I also include more concrete examples of types and methods we should differentiate between in those issues.\nI would also like to pass on Colin Carroll’s recommendation of A Philosophy of Software Design by John Ousterhout, which is excellent."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html",
    "href": "post/2019-06-07_testing-statistical-software/index.html",
    "title": "testing statistical software",
    "section": "",
    "text": "Recently I’ve been implementing and attempting to extend some computationally intense methods. These methods are from papers published in the last several years, and haven’t made their way into mainstream software libraries yet. So I’ve been spending a lot of time reading research code, and I’d like to share what I’ve learned.\nIn this post, I describe how I evaluate the trustworthiness of a modeling package, and in particular what I want from the test suite. If you use statistical software, this post will help you evaluate whether a package is worth using. If you write statistical software, this post will help you confirm the correctness of the code that you write."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#before-we-get-started",
    "href": "post/2019-06-07_testing-statistical-software/index.html#before-we-get-started",
    "title": "testing statistical software",
    "section": "Before we get started",
    "text": "Before we get started\nReading and evaluating tests involves a number of fundamental skills that are rarely taught. First, you need to know how to find, read and navigate the source code of the packages themselves1. Second, you need to know just a tiny bit about how software gets tested. For R packages, this means you need to know about testthat, which you can learn about by perusing the Testing chapter of R Packages."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#how-important-is-correctness",
    "href": "post/2019-06-07_testing-statistical-software/index.html#how-important-is-correctness",
    "title": "testing statistical software",
    "section": "How important is correctness?",
    "text": "How important is correctness?\nCorrect code is part of correct inference. Just as statistically literate data analysts avoid dubious methods, computationally literate data analysts avoid dubious code.\nData analysts use software to decide if drugs worked, to make public policy decisions, and to invest astronomical sums of money. In these high-impact settings, we really want to do correct inference, which means we need to hold code to high standards2.\nIdeally, we’d always work with well validated code. However, confirming the correctness of code takes a huge amount of effort, and it isn’t always realistic to hold the code you work with to the highest standards. When a data analysis is low impact, we don’t need to worry about correctness quite as much.\nCorrectness is also less important in prediction problems than in inference problems. If the code runs and gives you predictions, you can always cross-validate to determine how good they are. If the predictions are off, you still know how well the code performs for its intended purpose, so errors may not matter much3."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#types-of-tests",
    "href": "post/2019-06-07_testing-statistical-software/index.html#types-of-tests",
    "title": "testing statistical software",
    "section": "Types of tests",
    "text": "Types of tests\nThere are a lot of different ways you can test modeling code. Broadly, I think of tests as living in four categories:\n\nCorrectness tests check whether the code calculates the quantity it is supposed to calculated.\nParameter recovery tests check whether the implementation can recover correct parameters in well understood scenarios.\nConvergence tests check whether iterative fitting procedures have actually reached a solution.\nIdentification tests check whether the solution is unique, and stable under small perturbations to the data.\n\nMost research software tests for correctness, but on occasion I come across parameter recovery tests and convergence tests4. I rarely come across identification tests5.\n\nCorrectness tests\nCorrectness tests are by far the most important tests. Correctness tests come in two flavors: unit tests and integration tests. In the modeling context, a unit test checks the correctness of a individual calculations in a fitting procedure. An integration test means shoving data into the fitting procedure and making sure that the resulting estimates are correct.\nThe general strategy for writing unit tests is to compute the same thing in as many different ways as possible, and verify that all the results agree. For example, I recently needed to find the sum of squared eigenvalues of a matrix. To test my calculations, I calculated this sum using two different eigendecomposition algorithms, and also by finding the squared Frobenius norm of the matrix. The results didn’t agree, and it turned out something was wrong in one of the eigendecompositions.\nFor integration tests, you run the entire fitting procedure, and then you can:\n\nCompare results to a reference implementation that you trust\nCompare results to a solution you’ve worked out by hand\nCompare results to simple scenarios with obvious solutions\n\nThe gold standard is to compare to a reference implementation. With research code, which is often the first implementation of method, this might not be possible. Additionally, it’s pretty rare to be able to work out the solution by hand.\nIn practice, most people end up writing a reference implementation and checking that the reference implementation closely matches the pseudocode of their algorithm. Then they declare this implementation correct. How trustworthy this approach is depends on the clarity of the connection between the algorithm pseudocode and the reference implementation.\n\nAn example of an easily verifiable reference implementation\nConsider Algorithm 1 of Allen and Weylandt (2013):\n\nThere used a MATLAB reference implementation available online[^It has seen been replaced with a more full fledged R package MoMa.], which looked like:\n\nThis is a good reference implementation because the pseudocode is close enough to the actual code that we can be confident that the translation is correct almost by eyeballing it. I ended up implementing this algorithm in both R and C++ and testing against this reference implementation. Curious parties read the correctness test here.\nNote that my test is really an integration test, making sure the entire computation is correct. I don’t do any tests on sub-function or calculations, although it would be good to write a unit test for the soft thresholding function.\n\n\nA reference implementation that is more difficult to verify\nIn other cases, the connection between the reference implementation and the algorithm pseudocode is less clear. For example, consider Algorithm 2 of Cho, Kim, and Rohe (2018):\n\nYou can read the corresponding reference implementation here. While the code does perform the computations outlined in the algorithm pseudocode, it is much harder to make the connection between the two.\nThe first deviation is in AdaptImpute(), which truncates the reconstructed SVD to within user specified bounds. The paper discusses this computation, but the discussion is somewhat informal, and the truncation doesn’t appear in the pseudocode. The second deviation is in the subfunction SVD.F(), which uses some linear algebra tricks to calculate the sum of squared singular values. Both of these deviations make it more difficult to declare the reference implementation obviously correct6.\nAside: While it’s nice for the algorithm pseudocode to match up nicely with the reference implementation, there are good reasons why this might not be the case. Often the psuedocode will express abstract concepts that are easier to understand when we omit lower level computational details. In cases like this, it’s essential to have good documentation linking the reference implementation and the pseudocode.\n\n\n\nParameter recovery tests\nAnother way to sanity check results is to see if we can recover known parameters with our implementation. Both SFPCA and AdaptImpute are essentially fancy versions of SVD that estimate low rank matrix components. A reasonable thing to do then is to generate a low rank matrix and see if SFPCA and AdaptImpute produces estimates close to the known low rank structure.\nAt first, we should do this with no noise. Then we should add random noise to the observed matrix. We then want to see the parameter estimates degrade more and more with increasing noise.\nIf you are a package user, you should look for parameter recovery tests that use data similar to the data you have. A method may work well in some data regimes and not in other data regimes. If there are no tests on data that looks similar to your own, you can often write some without too much hassle.\nThe Stan community is particularly good about emphasizing parameter recovery tests as a part of the data analysis workflow. You may enjoy Jim Savage’s post on simulating fake data, as well as the Stan Best Practices wiki.\nIn the package evaluation context, you just want to check that the package you’re interested in does this at least a couple times in its test suite.\n\n\nConvergence tests\nConvergence tests check that iterative processes have actually reached solutions. Ideally you want packages to confirm that they convergence on a variety of sample problems. You also want them to perform runtime convergence tests.\nFor example, I recently came across a modeling function that calculated estimates via gradient descent. It looked like the following:\n\nfunction (x, eps = 1e-07) \n{\n  \n  # prep work\n  \n  for (iteration in 1:1000) {\n    \n    # gradient descent steps\n    \n    if (relative_change_is_small) \n      break\n  }\n  \n  # return\n}\n\nThe descent portion of the algorithm is contained in the for loop. If the algorithm converges early, the loop will break. But, if the algorithm does not converge in 1000 iterations, it will return an incorrect solution silently.\nWe can contrast this behavior with lme4, which explicitly warns users on convergence failures. The package also tests to make sure sure it issues this warning.\nIt can often be difficult to test for convergence, but for simple maximum likelihood or convex problems, I’d love to see more these tests. I suspect there are a lot of convergence failures out there that we don’t know about just because we haven’t checked.\nOther communities have taken different approaches. For example, the Stan community is careful to emphasize the importance of MCMC convergence checks as part of the modeling workflow, a task that needs to repeated by the data analyst for each model they fit. The machine learning community sometimes deals with convergence via early stopping, which is more a sort of statistical convergence than algorithmic convergence.\nWhile packages should definitely include convergence tests, automated procedures to check for convergence can be misleading. I would love to see convergence checks become a larger part of the data analysis workflow, and think of convergence as a responsibility shared by both users and developers.\n\n\nIdentification tests\nFinally we arrive at identification tests7. These tests are the most subtle out of anything we’ve considered because they get at the gap between theory and practice. For the most part, people don’t let you fit models that are actually unidentifiable.\nBut you often can fit nearly unidentifiable models, where small perturbations to your data lead to dramatic changes in your estimates. I’m going to avoid details about conditioning and numerical stability here; for a concrete example you can read more about near unidentifiability in lme4 in this post by Camelia Simoiu and Jim Savage.\nGenerally, the more flexible a modeling tool, the more likely it is that you can fit an unindentifiable or near unidentifiable model. I find an example from Simpson (2018) particularly interesting, where a model with a smooth through time together with CAR(1) structure on the residuals leads to fitting issues due to identification challenges.\nWhen you work with really flexible tools like Stan, you can also write down a model where some parameters just aren’t identified. For example, consider alpha and beta in the following:\n```{stan}\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta, sigma);\n}\n```\nHere alpha + beta is identified, but alpha and beta individually aren’t.\nMy limited impression is that it’s pretty hard to write tests for poorly conditioned problems. I mostly wanted to include this discussion here to encourage package authors to think about how identifiability might come up in their own work.\nI’d also like to mention Gelman’s folk theorem here, which says:\n\nComputational issues during model fitting indicate that you’re trying to fit an inappropriate model to your data."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#things-i-am-especially-careful-about",
    "href": "post/2019-06-07_testing-statistical-software/index.html#things-i-am-especially-careful-about",
    "title": "testing statistical software",
    "section": "Things I am especially careful about",
    "text": "Things I am especially careful about\nSo far I’ve talked about things to test at a fairly high level. Now I’d like to switch gears and talk about about very specific issues that you need to be careful about, especially in R.\nThe first thing to note is that most modeling code will fail silently. When modeling code fails, it normally doesn’t throw an error, or return an object with some nonsensical type. Rather, bad modeling code looks and works just like good modeling code, except it silently performs the wrong calculation.\nSuppose you want to get an unbiased estimate of population variance and you write the following function:\n\nvariance <- function(x) {\n  mean((x - mean(x))^2)\n}\n\nThis looks so, so much like the sample version of \\(\\mathbb{E}((X - \\mathbb{E}(X))^2)\\), but silently divides by \\(n\\) instead of \\(n - 1\\). If you expected an unbiased estimate of sample variance, this is wrong. Most modeling failures are in this vein.\nIn my experience, silent failures happen most often when:\n\nArguments disappear into dots\nConsider the following\n\nfit <- lm(hp ~ mpg, mtcars)\nbroom::tidy(fit, conf.int = TRUE, conf.levl = 0.9)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   324.       27.4      11.8  8.25e-13    268.     380.  \n2 mpg            -8.83      1.31     -6.74 1.79e- 7    -11.5     -6.16\n\n\nHere conf.levl = 0.9 should read conf.level = 0.9, so the code has silently gone ahead and calculate a 95 percent confidence interval instead of a 90 percent confidence interval. This issue appear over and over in modeling code, and you can read more about it in the tidyverse principles book draft.\n\n\nUsing metaprogramming or interpreting formulas in custom ways\nThere’s almost always a couple of versions of a variable called x floating around in different scopes, and once you start metaprogramming, the risk of confusing these goes way up.\n\n\nWrapping lots and lots of different models\nThis is mostly because packages that wrap lots of other packages (i.e. broom, parsnip, mlr, etc, etc) don’t normally do integration tests to make sure that the wrapped estimates are correct, and so things slip through the cracks every once in a while.\n\n\nValidating input\nMore precisely, most modeling packages don’t validate their input8, and on top of this a surprising number of packages have buckass crazy data format specifications. Be especially careful around factors, and always double check whether a modeling function standardizes your data internally or not."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#signals-of-software-quality",
    "href": "post/2019-06-07_testing-statistical-software/index.html#signals-of-software-quality",
    "title": "testing statistical software",
    "section": "Signals of software quality",
    "text": "Signals of software quality\nIn practice, I often do not have time to check for each of these issues, except in critical cases. Instead, I evaluate most modeling packages by counting up red flags and green flags that generally signal software quality. Here’s some things I look for.\nGreen flags\nThe code:\n\nlives in a package rather than a standalone script,\nhas an active Github repo,\nhas extensive tests,\nis on CRAN,\nwas reviewed by ROpenSci,\nwas reviewed by JOSS or JSS,\nfrequently gets used by the developer in their own research,\nand follows a style guide.\n\nRed flags\n\nNo tests, or tests don’t substantive check correctness.\nYou need to use capture.output() to interact with results9.\nModel objects are built using structure() rather than an S3 constructor function.\nLarge sections of commented out code in the source.\nCode doesn’t appear to have a large user base."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#some-code-to-peruse",
    "href": "post/2019-06-07_testing-statistical-software/index.html#some-code-to-peruse",
    "title": "testing statistical software",
    "section": "Some code to peruse",
    "text": "Some code to peruse\nNow that I’ve pontificated for a while, I recommend that you go out and assess some of your favorite packages and see how you feel about their testing. If you need recommendations to get started, have a look at:\n\nExclusiveLasso\nrms\nlme4\nmgcv\ncar\nbrms\nmetafor"
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#deciding-which-packages-to-trust",
    "href": "post/2019-06-07_testing-statistical-software/index.html#deciding-which-packages-to-trust",
    "title": "testing statistical software",
    "section": "Deciding which packages to trust",
    "text": "Deciding which packages to trust\nThe R community already has shared some excellent thoughts on how to choose which packages we should trust. I especially like Thomas Lumley’s blog post, as well as Jeff Leek’s blog post. Hadley Wickham shares some thoughts in this Twitter thread. The common theme amongst all these takes is that you should trust people rather than packages, and trust heavily used software with crucial functionality.\nI agree with all of this. Additionally, I’d pretty strongly endorse packages that have reviewed by ROpenSci. I’m less certain how I feel about packages published in the Journal of Open Source Software (JOSS) and the Journal of Statistical Software (JSS). More peer review is always better than less, but my impression is that peer review is does a lot more to ensure that package authors follow good software engineering practices than it does to ensure actual correctness.\nFor example, if you take a look at one of my JOSS reviews in progress, note that the Reviewer Checklist doesn’t require me to verify the correctness of calculations, just the existence of tests for those calculations. In theory, tests should verify correctness, but for many modeling packages, the code gets written first, and then the tests get filled in with the results from the code, under the assumption that the code is correct.\nIn general, the issue is that most interesting model computations are rather complicated, and I’m unwilling to trust that time constrained reviewers are going to take the time to understand all the details of the implementation. I’m also not certain that this is the best way for reviewers to spend their time. Merely as an anecdote, I recently reimplemented the algorithms in Cho, Kim, and Rohe (2018), a process which took me six weeks10, even though I was able to repeatedly sit down with the author of the original implementation to ask questions."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#takeaways-for-users",
    "href": "post/2019-06-07_testing-statistical-software/index.html#takeaways-for-users",
    "title": "testing statistical software",
    "section": "Takeaways for users",
    "text": "Takeaways for users\nIf you remember one thing from this blog post, it should be that you need to read the tests. Many packages write tests that do not actually ensure correctness. These tests may make sure the code runs without throwing an error, or that the type of returned objects is correct, but may not verify that computations are performed correctly."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#takeaways-for-researchers",
    "href": "post/2019-06-07_testing-statistical-software/index.html#takeaways-for-researchers",
    "title": "testing statistical software",
    "section": "Takeaways for researchers",
    "text": "Takeaways for researchers\nRemember: sane people do not use untested software. You have two jobs. The first job is to write correct code. The second job is to convince users that you have written correct code.\nUsers will read the tests in your package when they decide if they want to use your software. The easier it is for a user to understand your tests, the more likely it is they will use your software. Appropriately testing code takes a lot of time, and I hope you build this time into your schedule in the future.\nYou can read lots more about the mechanics of testing in R Packages, which focuses extensively on how to test code11."
  },
  {
    "objectID": "post/2019-06-07_testing-statistical-software/index.html#references",
    "href": "post/2019-06-07_testing-statistical-software/index.html#references",
    "title": "testing statistical software",
    "section": "References",
    "text": "References\nThere are a lot of important correctness issues in data science beyond implementation correctness in software packages. You might want to sanity check (1) the results of a data analysis, or (2) a machine learning system running in production. These are both very different beasts, each requiring their own distinct set of tools.\nFor thoughts on sanity checking the correctness of a data analysis, you may enjoy Checking Driven Development by Greg Wilson or Hicks and Peng (2019). If you run machine learning in production, some good resources are Breck et al. (2017) and Kirk (2015)."
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "",
    "text": "I’ve been reading a lot of papers on network analysis recently. I thought I’d write down some takeaways and point out papers that I’ve found helpful. This collection of papers is centered around the stochastic blockmodel, and is intended to be introductory rather than comprehensive. I’ve included a few papers with other miscellaneous tidbits of interest."
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#models",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#models",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "Models",
    "text": "Models\nThe most basic random graph model is the Erdos-Renyi graph, which assumes that you have a fixed number of nodes, and edges appear independently with probability \\(p\\). The Erdos-Renyi model has been studied to death and there are tons of interesting papers you can read about it. In particular, the graph structure goes through a number of phrase transitions as \\(p\\) varies. I’m pretty sure there’s a nice paper on this by Fan Chung but I can’t find the reference at the moment.\nHowever, the Erdos-Renyi graph is pretty unrealistic and doesn’t look anything like real world graphs. The stochastic blockmodel (SBM) represents a first pass at a more plausible real world graph. In the SBM, we assume that each node belongs to one of \\(k\\) communities. We then assume connections occur independently with probabilities that depend only on the communities of the nodes.\nOne major issue with SBMs is that they assume a uniform degree distribution, but real world graphs have highly skewed degree distributions. Karrer and Newman (2011) propose the degree-corrected SBM as a remedy to this problem. The paper additionally contains an interesting information-theoretic interpretation of modularity scores.\nIt also presents one standard (but unappealing) approach for fitting SBMs: (1) assign every node a community at random, (2) swap community labels at random, keeping the community assignments that result in the highest likelihood, and (3) do this a bunch of times because it’s random and you don’t want to get stuck in a local maximum. This is heuristic way to maximize the likelihood, which we should note is a hard problem here. Likelihood maximization requires optimizing over an \\(n \\times k\\) binary matrix, where \\(n\\) is the number of nodes and \\(k\\) is the number of communities. This is very non-convex.\nAnother key extension of the SBM comes in the form of the mixed membership SBM of Airoldi et al. (2008). Instead of assigning each node to a single cluster, the mixed membership SBM allows soft community assignments, where each node can belong to different communities, with total memberships summing up to one. The paper makes an argument for this model based on heterogeneous degrees (it came out before the degree corrected SBM paper) that I didn’t find terribly convincing (to be fair, I also only skimmed it), but it’s easy to add degree correction to the mixed membership SBM, and I would consider the degree corrected mixed membership SBM as a good starting point in many circumstances. The paper also discusses selecting the number of communities \\(k\\) via cross-validation and BIC, and introduces a variational Bayes estimator.\nPersonally I am not a huge fan of variational Bayes, so I didn’t read the details about the estimation procedure. In practice, if you use graph-tool, one of the standard Python libraries to fit this sort of thing, I’m fairly sure you get variational Bayes estimates. Finally, the high resolution graphics in the PDF version of this paper caused my printer to shit a brick.\nAnother interesting extension of the SBM (that can also be degree corrected!) is the overlapping SBM of Latouche, Birmelé, and Ambroise (2011). The overlapping SBM respects multiple community memberships better than the mixed SBM, but the finer details are somewhat lost on me. The paper has an interesting identifiability proof that went way over my mind but that I’d refer back to in the future if I ever wanted to do something similar. The paper also proposes a variational estimator, and then it compares the performance of the overlapping SBM with the mixed SBM on a network of French political blogs. It turns out the overlapping SBM does a little better than the mixed SBM, but they seem to be generally have comparable performance. The data analysis does a great job highlighting that sometimes you just get a trash cluster of nodes that don’t belong in any of the other, more meaningful clusters.\nOne slightly weird thing about the SBM is that the model is typically parameterized as having Poisson edges (i.e. elements of the network adjacency matrix \\(A_{ij}\\) are assumed to come from a Poisson(\\(\\lambda_{ij}\\)) distribution). The naive way to generate a sample from a SBM is to loop through every element of the adjacency matrix and sample a Poisson random variable for that element. This is \\(O(n^2)\\), which is ridiculous because most graphs are sparse. Rohe et al. (2017) presents an \\(O(m)\\) sampling scheme for random dot product graphs (a generalization of SBMs), where \\(m\\) is the number of edges in the graph. I’m currently working on cleaning up the original code for this into an R package.\nReading Karl’s paper helped me connect my “edge probabilities depend on communities” understanding of SBMs to the matrix-notation Poisson formulation of SBMs. The gist is that the Poisson distribution is completely determined by \\(\\lambda\\), which is also the mean of a Poisson distribution, so writing down the expected value of an adjacency matrix is enough to completely determine an SBM. This matrix also has some nice structure and can be expressed as \\(\\mathbb{E} (A) = X B X^T\\) in the undirected case, where \\(X\\) is binary, \\(n \\times k\\) and encodes community membership, and \\(B\\) is \\(k \\times k\\) and encodes probabilities of connections between communities.\nI’m fairly sure all of the papers so far mostly deal with undirected graphs (at least in the main exposition), and it’s a bit harder to think about a directed variant of the SBM. Now you have to imagine communities based on incoming edges, and communities based on outgoing edges. Rohe, Qin, and Yu (2016) present a directional version of the SBM and fit it using regularized spectral clustering. They also introduce an asymmetry score for nodes that can detect when incoming edges typically come from a different community than outgoing edges typically head to.\nRegularized spectral clustering is definitely something you want to read about at some point; Qin and Rohe (2013) is a good reference. One thing that I’ve been struggling with is the connection between the eigendecomposition of a graph and graph properties that actually make sense to me (things like betweenness, degree, etc). A key connection here comes in the form of Cheeger’s ineuqality, which connects eigenvalues of the graph Laplacian (more on this in a moment) with graph conductance.\nSome other interesting extensions to the SBM come in the form of the bipartite SBM (Larremore, Clauset, and Jacobs (2014)) and the high dimensional SBM (Rohe, Qin, and Fan (2012)). Both of these models are SBMs with structural parameters set to zero. The bipartite SBM seems like a good idea; Karl has told me the estimator proposed in Rohe, Qin, and Fan (2012) doesn’t really work.\n\nOther network models\nGerlach, Peixoto, and Altmann (2018) make interesting connections between SBMs and Latent Dirichlet Allocation. Another part of the network analysis universe deals with Exponential Random Graph Models (ERGMs). Personally I find ERGM parameters totally uninterpretable and the software difficult to use so I haven’t dug into them much and don’t really plan to."
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#estimators-for-sbms",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#estimators-for-sbms",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "Estimators for SBMs",
    "text": "Estimators for SBMs\nWhile I feel like I’ve finally wrapped by head around the various SBM generative models, I am much less certain how to fit the bloody things. Ghasemian, Hosseinmardi, and Clauset (2018) compares a large number of methods for community detection, the vast majority of which are estimators for SBMs. The takeaway seems to be that degree corrected mixed SBMs are the way to go (if you want to do inference, at least). I’m sure there are a bunch of wacky neural nets that do better on various prediction tasks, but I haven’t found those papers yet. Well, more accurately, I have found a ton of papers but I have no idea which ones are good and I don’t care enough about neural nets to read them closely and find out.\nAnyway, a rough overview of fitting SBMs. There are a lot of ways to do it, and none of them is especially appealing. The naive approach is to get MLE estimates. The papers that I’ve seen do this use a “swap labels randomly and keep the highest likelihood” approach. I find this unsatisfying, and also confusing. I don’t get why people do this instead of throwing the problem into an industrial grade integer program solver like Gurobi1. As a side note, I assume it’s much easier to optimize the likelihood of the mixed SBM, since group memberships live on simplices, which are convex, rather than lattices, which are not.\nThen there’s a wide variety of Bayesian estimation approaches, many of which seem to be variational Bayes. Sure, I guess. I’m happy to use these if someone else implements them, and am happy to pick \\(k\\) based on BIC from these models, but I’m trying to avoid learning anything about variational methods because then people might actually ask me to use them at some point. Also Bayes is slow.\nSpectral methods form another very important category of estimators. Spectral here just means “based on an eigendecomposition or an SVD”. Undirected graphs have symmetric adjacency matrices, so you can do an eigendecomposition. Directed graphs might not be positive definite, so you have to do an SVD instead. Oftentimes spectral estimates are used to initialize variational fitting methods.\nOne important thing to note with spectral methods is that you don’t really want to use the raw adjacency matrix. The eigenvectors of the adjacency matrix localize on high degree nodes (Martin, Zhang, and Newman (2014)). So what people typically do is work with the degree normalized adjacency matrix, which is often called the graph Laplacian2. Using the graph Laplacian (often called \\(\\mathcal L\\)) like this instead of the adjacency matrix \\(A\\) is sort of like moving from a basic SBM to a degree-corrected SBM. It turns out that the graph Laplacian has problems too, though. The eigenvectors of the graph Laplacian localize on low degree nodes (Zhang and Rohe (2018)). The solution to this is to use a regularized graph Laplacian. Fingers crossed I’ll post a short preprint containing some folk wisdom on regularization soonish.\nAnyway, it turns out that running k-means on the eigenvectors of the regularized graph Laplacian is a decent way to estimate the node cluster memberships (Qin and Rohe (2013), Rohe, Chatterjee, and Yu (2011)). For these spectral methods, one way to pick the number of communities is to look at the eigenvalues and use the elbow method. This is very heuristic but it works decently well. Wang and Rohe (2016) point out that getting the number of communities right isn’t terribly important, in a cleverly titled paper Don’t mind the (eigen) gap. You definitely don’t want to wildly overestimate \\(k\\), but if you cluster using \\(k = 5\\) versus \\(k = 10\\), the resulting clusters typically tell a coherent story and don’t contradict each other in terms of substantive conclusions. In spectral land, and eigenvalue of zero corresponds to “this definitely isn’t a cluster”, so as long as you aren’t pushing \\(k\\) up so high that \\(\\lambda_k \\approx 0\\), you’re probably fine. The intuition for this comes from the \\(\\mathbb E (A) = X B X^T\\) parameterization of the blockmodel.\nApparently there’s also some good minimum description length (MDL) approaches to selecting the number of communities, but I haven’t actually done any reading on this.\nSome other spectral papers to know about include Chaudhuri, Chung, and Tsiatas (2012), which explores a couple different forms of regularization, and Chung, Lu, and Vu (2003), which discusses the distribution of eigenvalues of the adjacency matrix and the graph Laplacian.\nA third category of estimators is based on graph thingajigs like betweenness. For example, the Girvan-Newman algorithm (Newman and Girvan (2004)) finds communities in graphs and I vaguely recall that it’s a consistent estimator for community assignments under some variant of the SBM3. I could be wrong. Anyway, you typically run the algorithm for a bunch of different \\(k\\) and then select \\(k\\) based on a modularity score. The algorithm and modularity score are both based on computer science-y graph concepts, but it’s equivalent to MLE estimation sometimes (Newman (2016)). Anyway, Newman’s papers are generally great and you should just go read all of them4."
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#the-triangle-trick",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#the-triangle-trick",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "The triangle trick",
    "text": "The triangle trick\nAnother paper that I find particularly interesting is Rohe and Qin (2013), both as a technical paper and as an insight into Karl’s psyche as he tries to start a religious cult based on network analysis. The gist of the paper is that there are lots of uninformative edges in sparse and transitive networks, and you can throw these edges out, keeping only the edges that participate in triangles.\nYou can do this really fast with some matrix algebra. Let \\(A\\) be an adjacency matrix, and let \\(A \\odot B\\) be the elementwise product of \\(A\\) and \\(B\\). Then \\(T = AA \\odot A\\) is the matrix of triangles. That is, \\(T_{ij}\\) is the number of triangles that both node \\(i\\) and node \\(j\\) participate in. You can then use \\(T\\) just like you would normally use \\(A\\), and doing this cleans clusters up really nicely. You can do this with the (regularized) graph Laplacian as well. Benson, Gleich, and Leskovec (2016) is a followup that generalizes from the shared triangles matrix \\(T\\) to the general case of shared motifs, which means “patterns that both node \\(i\\) and node \\(j\\) participate in.”"
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#briefly-contagion",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#briefly-contagion",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "Briefly, contagion",
    "text": "Briefly, contagion\nMost of the reading I’ve been doing has been about SBMs, but there are some other papers that I highly recommend you read. The first is Shalizi and Thomas (2011), which points out that social contagion is generally confounded in graphs where people are connected to people like themselves. The fancy phrase for this is homophily, and it happens in pretty much all interesting graphs. This is an important observation because there’s a huge literature on contagion models, some of which seems to aware of this result, and some of which doesn’t. One way to avoid homophily-contagion confounding is to run an experiment. Taylor and Eckles (2017) discusses how you might want to do this. In generally I think it’s worth reading some of the more sociological network papers with a healthy dose of skepticism."
  },
  {
    "objectID": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#the-end",
    "href": "post/2019-07-26_an-annotated-bibliography-on-stochastic-blockmodels/index.html#the-end",
    "title": "an annotated bibliography on stochastic blockmodels",
    "section": "The end",
    "text": "The end\nThat’s it, that’s all I’ve got. Most of the papers I’ve linked to have great references, so if you get bored or this isn’t enough, you can always go down that rabbit hole.\nI don’t have a good handle on what kinds of neural nets people are using for graph stuff thesedays, nor do I have a good handle on modern graph embeddings. If you read about and/or use that kind of stuff, please leave paper recommendations in the comments!"
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html",
    "title": "consistency and the linear probability model",
    "section": "",
    "text": "A while back Twitter once again lost its collective mind and decided to rehash the logistic regression versus linear probability model debate for the umpteenth time. The genesis for this new round of chaos was Gomila (2019), a pre-print by Robin Gomila, a grad student in pyschology at Princeton1. You can get a taste of the discussion in the replies to the announcement:\n\n\n{{% tweet \"1149419090693513216\" %}}\n\n\nSo I decided to learn some more about the linear probability model (LPM), which has been on my todo list since Michael Weylandt suggested it as an example for a paper I’m working on. In this post I’ll introduce the LPM, and then spend a bunch of time discussing when OLS is a consistent estimator.\nUpdate: I turned this blog post into somewhat flippant, meme-heavy slides."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-is-the-linear-probability-model",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-is-the-linear-probability-model",
    "title": "consistency and the linear probability model",
    "section": "What is the linear probability model?",
    "text": "What is the linear probability model?\nSuppose we have outcomes \\(Y \\in \\{0, 1\\}\\) and fixed covariate vectors \\(X\\). The linear probability model is a model, that is, a set of probability distributions that might have produced our observed data. In particular, the linear probability assumes that the data generating process looks like:\n\\[\\begin{align}\nP(Y = 1 | X) =\n  \\begin{cases}\n  1 & \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k > 1 \\\\\n  \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k & \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k \\in [0, 1] \\\\\n  0 & \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k < 0\n  \\end{cases}\n\\end{align}\\]\nEssentially we clip \\(P(Y = 1 | X) = X \\beta\\) to \\([0, 1]\\) to make sure we get valid probabilities. We can contrast the LPM with the binomial GLM, where the assumption is that:\n\\[\\begin{align}\nP(Y = 1 | X) = \\frac{1}{1 + \\exp(-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k))}\n\\end{align}\\]\nAt points throughout this post we will also be interested in Gaussian GLMs. In slightly different notation, we can write all of these models as follows:\n\\[\\begin{align}\nY_i | X_i &\\sim \\mathrm{Bernoulli}(\\min(1, \\max(0, X_i \\beta)))\n  & \\text{linear probability model} \\\\\nY_i | X_i &\\sim \\mathrm{Bernoulli}(\\mathrm{logit}^{-1} (X_i \\beta))\n  & \\text{logistic regression / binomial GLM} \\\\\nY_i | X_i &\\sim \\mathrm{Normal}(X_i \\beta, \\sigma^2)\n  & \\text{linear regression / Gaussian GLM}\n\\end{align}\\]"
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#the-ols-estimator",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#the-ols-estimator",
    "title": "consistency and the linear probability model",
    "section": "The OLS estimator",
    "text": "The OLS estimator\nWhen people refer to the linear probability model, they are referring to using the Ordinary Least Squares estimator as an estimator for \\(\\beta\\), or using \\(X \\hat \\beta_\\text{OLS}\\) as an estimator for \\(\\mathbb{E}(Y|X) = P(Y = 1|X)\\). The OLS estimator is:\n\\[\\begin{align}\n\\hat \\beta_\\text{OLS} = (X'X)^{-1} X' Y.\n\\end{align}\\]\nMost people have seen the OLS estimator derived as the MLE of a Gaussian linear model. Here we have binary data, which is definitely non-Gaussian, and this is aesthetically somewhat unappealing. The big question is whether or not using \\(\\hat \\beta_\\text{OLS}\\) on binary data actually works."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#a-bare-minimum-for-inference",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#a-bare-minimum-for-inference",
    "title": "consistency and the linear probability model",
    "section": "A bare minimum for inference",
    "text": "A bare minimum for inference\nWhen we do inference, we pretty much always want two things to happen. Given our model (the assumptions we are willing to make about the data generating process):\n\nour estimand should be identified, and\nour estimator should be consistent.\n\nIdentification means that each possible value of the estimand should correspond to a distinct distribution in our probability model. When we say that we want a consistent estimator, we mean that our estimator should recover the estimand exactly with infinite data. All the estimands in the LPM debate are identified (to my knowledge), so this isn’t the big deal here. But consistency matters a lot!\nGomila (2019) claims that \\(\\hat \\beta_\\text{OLS}\\) is unbiased and consistent for \\(\\beta\\), and attempts to demonstrate this two ways: (1) analytically, and (2) via simulation.\nAnd this is the point that I got pretty confused, because the big question is: consistent under what model? Depending on who you ask, we could conceivably be assuming that the data comes from:\n\na Gaussian GLM (linear regression),\nthe linear probability model, or\na binomial GLM (logistic regression)."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#consistency-of-the-ols-estimator",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#consistency-of-the-ols-estimator",
    "title": "consistency and the linear probability model",
    "section": "Consistency of the OLS estimator",
    "text": "Consistency of the OLS estimator\nThe easiest case is when we assume that a Gaussian GLM (linear regression model) holds. In this case, \\(\\hat \\beta_\\text{OLS}\\) is unbiased and consistent. My preferred reference for this is Rencher and Schaalje (2008).\nWhen the linear probability model holds, \\(\\hat \\beta_\\text{OLS}\\) is in general biased and inconsistent (Horrace and Oaxaca (2003)). However, Gomila (2019) claims that OLS is unbiased and consistent. Over Twitter DM I clarified that this claim is with respect to the linear probability model. Gomila referred me to Wooldridge (2001) for an analytic proof, and to his simulation results for empirical confirmation.\nAt this point the main result of Horrace and Oaxaca (2003) becomes germane. It goes like this: the OLS estimator is consistent and unbiased under the linear probability model if \\(X_i^T \\beta \\in [0, 1]\\) for all \\(i\\), otherwise the OLS estimator is biased and inconsistent. In fact, Wooldridge (2001) makes the same point:\n\nUnless the range of \\(X\\) is severely restricted, the linear probability model cannot be a good description of the population response probability \\(P(Y = 1|X)\\).\n\nHere are some simulations demonstrating this bias and inconsistency when the \\(X_i^T \\beta \\in [0, 1]\\) condition is violated. It’s pretty clear that OLS is in general biased and inconsistent under the linear probability model. I was curious why this wasn’t showing up in Gomila’s simulations, so I took a look at his code and it turned out he wasn’t simulating from diverse enough data generating processes.\nWe discussed this over Twitter DM, and Gomila has since updated the code, but I believe the new simulations still do not seriously violate the \\(X_i^T \\beta \\in [0, 1]\\) condition. I briefly played around with the code but then it was 2 AM and I didn’t understand DeclareDesign particularly well so I gave up. Many kudos to Gomila for posting his code for public review.\nAnyway, the gist is that OLS is consistent under the linear probability model if \\(X_i^T \\beta\\) is between zero and one for all \\(i\\)."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-if-logistic-regression-is-the-true-model",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-if-logistic-regression-is-the-true-model",
    "title": "consistency and the linear probability model",
    "section": "What if logistic regression is the true model?",
    "text": "What if logistic regression is the true model?\nAnother reasonable question is: what happens to \\(\\hat \\beta_\\text{OLS}\\) under a binomial GLM? The story here is pretty much the same: \\(\\hat \\beta_\\text{OLS}\\) is not consistent for \\(\\beta\\), but it often does a decent job of estimating \\(\\mathbb{E}(Y|X)\\) anyway (Battey, Cox, and Jackson 2019; Cox 1958).\nThe intuition behind this comes from M-estimation2, which we digress into momentarily. The idea is to observe that \\(\\hat \\beta_\\text{OLS}\\) is an M-estimator. To see this, recall that OLS is based on minimizing\n\\[\\begin{align*}\n  \\mathcal{L}(X, Y, \\beta) = \\frac 12 \\Vert Y - X \\beta \\Vert_2^2\n\\end{align*}\\]\nwhich has a global minimizer \\(\\beta_0\\). The gradient,\n\\[\\begin{align*}\n  \\nabla \\mathcal{L}(X, Y, \\beta) = \\left( Y - X \\beta \\right)' X,\n\\end{align*}\\]\nshould be zero at \\(\\beta_0\\) under any model with linear expectation (provided that \\(X_i \\neq 0\\) for all \\(i\\)). So we take \\(\\psi = \\nabla \\mathcal{L}(X, Y, \\beta_0)\\) as the function in our estimating equation, since \\(\\mathbb{E} (\\psi) = 0\\). This lets us leverage standard results from M-estimation theory.\nIn particular, M-estimation theory tells us that \\(\\hat \\beta_\\text{OLS}\\) is consistent under any regular distribution \\(F\\) such that \\(\\beta_0\\) is the unique solution to\n\\[\\begin{align*}\n  \\mathbb{E}_F \\left[ \\left( Y_i - X_i^T \\beta_0 \\right)' X_i \\right] = 0.\n\\end{align*}\\]\nWe can use this to derive a sufficient condition for consistency; namely OLS is consistent for \\(\\beta_0\\) if\n\\[\\begin{align*}\n  0\n  &= \\mathbb{E}_F \\left[ \\left( Y - X_i^T \\beta_0 \\right)' X_i \\right] \\\\\n  &= \\mathbb{E}_F \\left[ \\mathbb{E}_F \\left[ \\left( Y - X_i^T \\beta_0 \\right)' X_i \\Big \\vert X_i \\right] \\right] \\\\\n  &= \\mathbb{E}_F \\left[ \\mathbb{E}_F \\left[ Y - X_i^T \\beta_0 \\Big \\vert X_i \\right]' X_i \\right].\n\\end{align*}\\]\nSo a sufficient condition for the consistency of OLS is that\n\\[\\begin{align*}\n  \\mathbb{E}_F(Y | X_i) = X_i^T \\beta_0.\n\\end{align*}\\]\nThat is, if the expectation is linear in some model parameter \\(\\beta_0\\), OLS is consistent for that parameter. This sufficient condition is also a necessary condition if we make the very reasonable assumption that \\(X_i \\neq 0\\) for all \\(i\\).\nReturning from our digression into M-estimation, we note the condition that \\(\\mathbb{E}_F(Y | X_i) = X_i^T \\beta_0\\) is shockingly weaker than the assumption we used to derive \\(\\hat \\beta_\\text{OLS}\\), where we used the fact that \\(Y\\) followed a Gaussian distribution. For consistency, we only need an assumption on the first moment of the distribution of \\(Y\\), rather than the full parametric form of the model, which specifies every moment \\(\\mathbb{E}(Y^2), \\mathbb{E}(Y^3), \\mathbb{E}(Y^4), ...\\) of \\(Y\\).\nAnyway, the expectation of a binomial GLM is \\(\\mathrm{logit}^{-1} (X_i \\beta)\\), and the expectation of the LPM is \\(\\min(1, \\max(0, X_i \\beta))\\). For certain \\(X_i\\) and \\(\\beta\\), these expectations are very close to \\(X_i \\beta\\). In these cases, estimates for \\(\\mathbb{E}(Y|X)\\) based on \\(\\hat \\beta_\\text{OLS}\\) will do well3.\nLooking at the expectations of the models, we can see they aren’t wildly different:\n\n\n\n\n\nNote that, under the LPM, \\(\\hat \\beta_\\text{OLS}\\) can give us good estimates for \\(\\beta\\), but under logistic regression, we only get good estimates of \\(\\mathbb{E}(Y|X)\\)4.\nSo maybe don’t toss the LPM estimator out with the bath water. Sure, the thing is generally inconsistent and aesthetically offensive, but whatever, it works on occasion, and sometimes there will be other practical considerations that make this tradeoff reasonable."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#where-the-estimator-comes-from-doesnt-matter",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#where-the-estimator-comes-from-doesnt-matter",
    "title": "consistency and the linear probability model",
    "section": "Where the estimator comes from doesn’t matter",
    "text": "Where the estimator comes from doesn’t matter\nBayes Twitter in particular had a number of fairly vocal LPM opponents, on the basis that \\(\\hat \\beta_\\text{OLS}\\) was derived under a model that can’t produce the observed data.\nThis might seem like a dealbreaker, but it doesn’t bother me. Where the estimator comes from doesn’t actually matter. If it has nice properties given the assumptions you are willing to make, you should use it! Estimators derived under unrealistic models5 often turn out to be good!\nIn a bit more detail, here’s how I think of model checking:\n\nThere’s an estimand I want to know\nI make some assumptions about my data generating process\nI pick an estimator that has nice properties given this data generating process\n\nThe issue here is that my assumptions about the data generating process can be wrong. And if my modeling assumptions are wrong, then my estimator might not have the nice properties I want it to have, and this is bad.\nThe concern is that using \\(\\hat \\beta_\\text{OLS}\\) corresponds to making a bad modeling assumption. In particular, using a Gaussian model for binary data isn’t defensible.\nThat’s not really what’s going on though. Instead, we start by deriving an estimator under the linear regression model. Then, we show this estimator has nice properties under a new, different model. To do model checking, we need to test whether the new, different model holds. Whether or not the data comes from a linear regression is immaterial.\nNobody who likes using \\(\\hat \\beta_\\text{OLS}\\) is arguing that it’s reasonable to assume a Gaussian generative model for binary data. LPM proponents argue that the Gaussian modeling assumption is violated, but it isn’t violated in an important way. Look, they say, the key assumptions of a Gaussian model that we used to derive consistency results can still hold, even if some other assumptions get violated. This is exactly what the M-estimation approach formalizes."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-about-uncertainty-in-our-estimator",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#what-about-uncertainty-in-our-estimator",
    "title": "consistency and the linear probability model",
    "section": "What about uncertainty in our estimator?",
    "text": "What about uncertainty in our estimator?\nSo far we have established that \\(\\hat \\beta_\\text{OLS}\\) is at least occasionally a consistent estimator for \\(P(Y=1|X)\\) under the LPM and logistic regression.\nIn practice, we also care about the uncertainty in \\(\\hat \\beta\\). We might want a confidence interval, or, God forbid, a p-value. So we should think about consistent estimators for \\(\\mathbf{Cov} (\\hat \\beta)\\). In the grand LPM debate, most people suggest using robust standard errors. For a while, it was unclear to me how these robust standard errors were derived and under what conditions they are consistent. The answer again comes from Boos and Stefanski (2013), and all we need for the consistency of robust standard errors for \\(\\mathbf{Cov} (\\hat \\beta)\\) is some moment conditions (the existence of (7.5) and (7.6) in the text), which linear regression, logistic regression, and the LPM all satisfy.\nIt only took several weeks of misreading Boos and Stefanski (2013) and asking dumb questions on Twitter to figure this out. Thanks to Achim Zeileis, James E. Pustejovsky, Cyrus Samii for answering those questions. Note that White (1980) is another canonical paper on robust standard errors, but it doesn’t click for me like the M-estimation framework does."
  },
  {
    "objectID": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#takeaways",
    "href": "post/2019-08-31_consistency-and-the-linear-probability-model/index.html#takeaways",
    "title": "consistency and the linear probability model",
    "section": "Takeaways",
    "text": "Takeaways\nThis post exists because I wasn’t sure when \\(\\hat \\beta_\\text{OLS}\\) was a consistent estimator, and the rest of the LPM debate seemed like a lot of noise until I could figure that out. I have three takeaways.\n\nProperties of estimators are always with respect to models, and it’s hard to discuss which estimator is best if you don’t clarify modeling assumptions.\nIf there are multiple reasonable estimators, fit them all. If they result in substantively different conclusions: congratulations! Your data is trash and you can move on to a new project!\nIf you really care about getting efficient, consistent estimates under weak assumptions, you should be doing TMLE or Double ML or burning your CPU to a crisp with BART6.\n\nAnyway, this concludes the “I taught myself about the linear probability model and hope to never mention it again” period of my life. I look forward to my mentions being a trash fire."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html",
    "title": "overfitting: a guided tour",
    "section": "",
    "text": "This post introduces overfitting, describes how overfitting influences both prediction and inference problems, provides supervised and unsupervised examples of overfitting, and presents a fundamental relationship between train and test error. The goal is to provide some additional intuition beyond material covered in introductory machine learning resources."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#some-intuition-via-a-guessing-game",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#some-intuition-via-a-guessing-game",
    "title": "overfitting: a guided tour",
    "section": "Some intuition via a guessing game",
    "text": "Some intuition via a guessing game\nBefore we begin, I want to play a guessing game. Here’s how it works: I show you two sequences of coin flips. You have to guess which sequence is random and which one I made up.\n\n\n\nOkay, here are the first two sequences. Which one do you think is random?\nA. THHTTHTTHTHHTHTHHHTT \nB. HTTHTHTHHHHHHHHHTTHT\nLet’s play again. Now the sequences are:\nA. HHTHTTHHTTTHHHTTHTTT\nB. HTHTTHHHTHHHHTTHHHTT\nOne last time.\nA. HTTHTTHHHTHTHHTTTTHH\nB. HHTHTTHHTTTHHTTTHHTH\nThe answers are in a footnote1. In any case, there’s a simple rule you can use to tell the random sequences from the human generated sequences: the random sequences are the sequences with the longest substring of all heads or all tails.\nThe gist is that human intuition is bad at solving this problem. Long sequences of all heads or all tails don’t look random–they appear overly structured. But long sequences of heads and tails are in fact quite probable under independent coin flipping! This example illustrates a fundamental fact that human brains often struggle with:\n\n\n\n\n\n\nImportant\n\n\n\nRandom processes produce highly structured data.\n\n\nUntil we understand this, it’s hard to build any intuition for overfitting."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#sources-of-structure",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#sources-of-structure",
    "title": "overfitting: a guided tour",
    "section": "Sources of structure",
    "text": "Sources of structure\nA major challenge in probability modeling is that there are two sources of structure in data:\n\napparent, happenstance structure that originates in the randomness of the data generating process, and\nsystematic structure inherent in the data generating process.\n\nWhen we observe structure in data, we don’t know where it came from. If the structure comes from randomness in the data generating process, we would like to ignore it. If the structure is the result of some fundamental latent characteristic of the phenomena we are studying, we want to study it or leverage it to improve our estimates. When we mistakenly confused random, apparent structure for true, systematic structure, we call this mistake overfitting.\n\nExample: polynomial regression\nLet’s consider a common statistical problem: prediction. Suppose we have twenty pairs of observations \\((X_i, Y_i)\\), and we believe that a Gaussian linear model is appropriate. We would like to estimate \\(\\mathbb{E}(Y|X)\\), the conditional expectation of \\(Y\\) given \\(X\\).\nIf we use an overly flexible estimator that assumes the data has a more complicated data generating process than it truly does, we can quickly run into overfitting. Consider 20 i.i.d. observations from the simple model:\n\\[\n\\begin{align*}  \n  Y_i \\sim 2 - 0.3 \\cdot X_i + \\varepsilon_i \\\\\n  \\varepsilon_i \\sim \\mathrm{Normal}(0, 0.3^2)\n\\end{align*}\n\\]\nThe simulated data looks like:\n\n\n\n\n\nIf we consider a \\(7^{th}\\) degree polynomial fit, our estimate for the conditional expectation is shown in maroon below and looks like:\n\n\n\n\n\nOur predictions have conformed to random variation in the data rather than systematic variation in the data, and using the polynomial fit for inference or prediction is a bad idea. By contrast, a correctly specified linear model does much better.\n\n\n\n\n\n\n\nExample: model selection amongst polynomial regressions\nOverfitting is a major concern especially when we are choosing between several models. This is because overfit models will look good with respect to some measures of goodness of fit. For example, suppose we want to use the same data from above, and we want to pick a model from several polynomial regression models with differing degrees. We might propose the following estimation procedure:\n\nFor \\(k \\in \\{1, 2, ..., 20\\}\\)\n\nFit a degree \\(k\\) polynomial regression under the assumption that \\(y = \\beta_0 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + ... + \\beta_k \\cdot x^k + \\varepsilon\\)\nRecord the \\(R^2\\) of the model\n\nPick the model that maximizes the \\(R^2\\) of the linear regression model\n\nWe plot \\(R^2\\) versus degree below, and see that we would select a very high degree, even though the true data generating process has degree one.\n\n\n\n\n\nWe can visualize some of the models, and see that high order polynomials fit the data better and better while doing a worse and worse job at estimating the systematic structure in the data.\n\n\n\n\n\nSo overfitting is a concern when we consider a single model, and also when we want to compare many different models. Note that we don’t have to perfectly interpolate the data to overfit! Any of the models with degree larger than one results in a bad estimate of \\(\\mathbb{E}(Y|X)\\). Typically, models with more parameters are more flexible and more prone to overfitting, but this is not always the case2.\n\n\nExample: estimating cluster memberships\nBefore we move on, I want to reiterate my point that overfitting occurs in all contexts, not just prediction. Suppose we want to find clusters in bivariate data, and we have reason to believe that a Gaussian mixture model is appropriate. We don’t know how many components are in the mixture, so we start off by trying a model with five components.\nWhen the data has two components, that is, it comes from a simpler data generating process, our estimator will overfit and make several errors at once:\n\nIt will get the centers of the clusters wrong\nIt will get the covariances of the clusters wrong\nIt will assign data points that belong to the same cluster to different clusters\n\nFor a concrete example we simulate some bivariate data with two clusters.\n\n\n\n\n\nBut if we assume that the data comes from a more flexible model, our estimated cluster memberships are wonky and off:"
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#how-to-handle-overfitting",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#how-to-handle-overfitting",
    "title": "overfitting: a guided tour",
    "section": "How to handle overfitting",
    "text": "How to handle overfitting\nVery roughly, there are three tricks we can use to mitigate overfitting.\n\nUse domain knowledge to consider appropriate classes of models\nPenalize model complexity\nSample splitting\n\nFor the rest of this post, I want to present some basic results on overfitting in a supervised learning context and to give you some intuition on why sample splitting helps.\nIf you’re interested in penalization approaches, key search phrases are “regularization”, “degrees of freedom” and “bias-variance tradeoff”. Many machine learning textbooks describe estimation procedures and model selection procedures that use some form of penalization. See Belkin et al. (2018) for an overview of some interesting recent developments concerning model complexity. For a more theoretical treatment of penalization in a model selection context you may enjoy the first couple chapters of Massart (2003)."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#overfitting-in-prediction",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#overfitting-in-prediction",
    "title": "overfitting: a guided tour",
    "section": "Overfitting in prediction",
    "text": "Overfitting in prediction\nIf you come from the machine learning community, you may think overfitting is the difference between predictive performance on training data and test data. To use more statistical language, think of a flexible machine learning estimator for \\(\\mathbb{E}(Y|X)\\) like a random forest. We fit the random forest on training data, which is sampled from some data generating process. We hope the random forest only finds systematic structure in noisy, observed conditionals mean of \\(Y\\) given \\(X\\), but it will also conform to random variations in the conditional mean. These random variations will not be present in the test set, which will have only the underlying systematic structure plus new random variation. Since random variation in the training set results in a poor estimate of the systematic structure in the data generating process, our overfit estimate will make mistakes when looking for systematic structure in the test set. Thus overfitting will reduce predictive performance3. This is sometimes described as the random forest “memorizing the training set”.\nIt turns out that we can study this phenomena more formally. The math is easiest in a restricted setting, but the intuition generalizes well. First, we assume that our data is independent and identically distributed, where\n\\[\nY_i = f(X_i) + \\varepsilon_i.\n\\]\nHere \\(f(X_i)\\) describes how \\(Y_i\\) varies systematically with \\(X_i\\), which we assume is fixed (this is the restrictive assumption). \\(\\varepsilon_i\\) represents random error, which we take to be mean zero with variance \\(\\sigma^2\\).\nWe consider a training set \\(X_1, ..., X_n\\), and obtain predicted values \\(\\widehat Y_1, ..., \\widehat Y_n\\). Then we consider a test set, observed at the same values \\(X_i\\), but with new random errors \\(\\varepsilon_i^*\\). So our test set is the set of observations\n\\[\nY_i^* = f(X_i) + \\varepsilon_i^*.\n\\]\nLet our predicted values at \\(X_i\\), obtained using only the training set, be \\(\\widehat Y_i = \\hat f (X_i)\\), where \\(\\hat f\\) represents our estimator for the conditional mean (i.e. predictive model). Note that these are not just the predicted values for the training set, but also the predicted values for the test set, since \\(X_i\\) is fixed.\nUnder \\(\\ell_2\\) loss, the training error is\n\\[\n\\frac 1n \\sum_{i=1}^n (Y_i - \\widehat Y_i)^2\n\\]\nand the test error is\n\\[\n\\frac 1n \\sum_{i=1}^n (Y_i^* - \\widehat Y_i)^2.\n\\]\nWe know that the training error should be less than the test error, and we can in fact formalize the relationship between these two measures. In particular, we have:\n\\[\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\frac 1n \\sum_{i=1}^n \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error}\n  =\n  \\underbrace{\n    \\mathbb{E} \\left[ \\frac 1n \\sum_{i=1}^n \\left( Y_i - \\hat Y_i \\right)^2 \\right]\n  }_\\text{training error} +\n  \\underbrace{\n    \\frac 2n \\cdot \\sum_{i=1}^n \\mathrm{Cov} \\left( \\widehat Y_i, Y_i \\right)\n  }_\\text{optimism}\n\\end{align}\n\\] This relationship holds for most important loss functions. It means tells us that test error, or generalization error, is almost always higher than in-sample error evaluated on the training set. We call the amount by which the training error underestimates the test error the optimism; the more optimism, the greater the discrepancy between in-sample and out-of-sample error.\nWe’ll discuss the implications the train-test error relationship more in a moment, but first let’s prove it. Feel free to skip the proof, although it relies only on basic properties of the expectation and variance.\nProof. We’ll follow the proof in these course notes by Larry Wasserman. Consider the \\(i^{th}\\) observation. Then\n\\[\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i - \\hat Y_i \\right)^2 \\right]\n  }_\\text{training error at $X_i$}\n&= \\mathrm{Var} \\left[ Y_i - \\widehat Y_i \\right]\n  + \\left( \\mathbb{E} \\left[ Y_i - \\widehat Y_i \\right] \\right)^2 \\\\\n&= \\mathrm{Var} \\left[ Y_i \\right]\n  + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n  - 2 \\, \\mathrm{Cov} \\left[ Y_i, \\widehat Y_i \\right]\n  + \\left(\n    \\mathbb{E} \\left[ Y_i \\right]\n    - \\mathbb{E} \\left[\\widehat Y_i \\right]\n  \\right)^2\n\\end{align}\n\\]\nand also\n\\[\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error at $X_i$}\n&= \\mathrm{Var} \\left[ Y_i^* - \\widehat Y_i \\right]\n  + \\left( \\mathbb{E} \\left[ Y_i^* - \\widehat Y_i \\right] \\right)^2 \\\\\n&= \\mathrm{Var} \\left[ Y_i^* \\right]\n  + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n  - 2 \\, \\mathrm{Cov} \\left[ Y_i^*, \\widehat Y_i \\right]\n  + \\left(\n    \\mathbb{E} \\left[ Y_i^* \\right]\n    - \\mathbb{E} \\left[\\widehat Y_i \\right]\n  \\right)^2.\n\\end{align}\n\\]\nNow we consider several implications of the fact that \\(Y_i\\) and \\(Y_i^*\\) are independent and identically distributed. In particular, we have \\(\\mathbb{E}(Y_i) = \\mathbb{E}(Y_i^*)\\), \\(\\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_i^*)\\), and most importantly \\(\\mathrm{Cov} \\left[ Y_i^*, \\widehat Y_i \\right] = \\mathrm{Cov} \\left[ f(X_i) + \\varepsilon_i^*, \\hat f(X_i) \\right] = \\mathrm{Cov} \\left[ \\varepsilon_i^*, \\hat f(X_i) \\right] = 0\\). Thus we see\n\\[\n\\begin{align}\n  \\underbrace{\n    \\mathbb{E} \\left[ \\left( Y_i^* - \\widehat Y_i \\right)^2 \\right]\n  }_\\text{test error at $X_i$}\n  &= \\mathrm{Var} \\left[ Y_i \\right]\n    + \\mathrm{Var} \\left[ \\widehat Y_i \\right]\n    + \\left(\n      \\mathbb{E} \\left[ Y_i^* \\right]   \n      - \\mathbb{E} \\left[\\widehat Y_i \\right]\n    \\right)^2 \\\\\n  &=\n    \\underbrace{\n      \\mathbb{E} \\left[ \\left( Y_i - \\widehat Y_i \\right)^2 \\right]\n    }_\\text{training error at $X_i$}\n    + 2 \\, \\underbrace{\n      \\mathrm{Cov} \\left[ Y_i, \\widehat Y_i \\right]\n    }_\\text{how much $\\hat f$ memorized $Y_i$}\n\\end{align}\n\\]\nwhere in the last equality we substitute based on our previous decomposition of the training error. Summing over all \\(i\\) and dividing by \\(n\\) finishes the proof.\nLet’s consider the implications of the train-test error relationship in two extreme cases. First, suppose that the \\(\\widehat Y_i = Y_i\\). This means that our estimator \\(\\hat f\\) has perfectly memorized the training set. In this case, there is zero training error, but the optimism is \\(2 \\sigma^2\\), which is pretty much the worst possible case amongst reasonable estimators (in the fixed \\(X_i\\) setting).\nIn the flip case, the estimator doesn’t memorize the training set at all, so there is no dependence (and thereby no covariance) between the predictions \\(\\widehat Y_i\\) and the training labels \\(Y_i\\) (really, the errors \\(\\varepsilon_i\\), since that’s the only random component of \\(Y_i\\)). This means the estimator \\(\\hat f\\) has ignored the random errors \\(\\varepsilon_i\\) and has learned only generalizable knowledge!\nIn fact, it’s often useful to treat\n\\[\n\\frac{1}{\\sigma^2} \\sum_{i=1}^n \\mathrm{Cov}\n\\left[ \\widehat Y_i, Y_i \\right]\n\\]\nas a generalized notion of “effective number of parameters” or “degrees of freedom” that measures the complexity of a predictive estimator and its capacity to memorize the training set4.\nFinally, this theorem suggests why sample splitting can give us good estimates of test error. If we have an independent dataset, and we assess the performance of the estimator \\(\\hat f\\) on the independent dataset, the predictions on this dataset will be independent from the training data, the covariance will be zero, and the optimism term will disappear. Thus we get an unbiased estimate of the loss on new data. This is sometimes called “unbiased risk estimation”5.\nCross validation now is a natural generalization of our hold out estimator. We have an unbiased estimate, so all the error in our estimate of the generalization error comes from variance. If we can generate \\(k\\) independent-ish estimates of generalization error and average them, then we will reduce the variance in the risk estimate. It’s natural to partition the data into \\(k\\) non-overlapping sets, fit a model \\(\\hat f_j\\) on all but the \\(j^{th}\\) partition, and estimate the validation error on the unseen \\(j^{th}\\) portion of the data. In terms of intuition, we then average these “unbiased” and “independent” estimates get a new “unbiased estimate with smaller variance”. In practice, the situation is more complicated, but cross-validation nonetheless turns out to be a good idea6."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#pulling-it-all-together",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#pulling-it-all-together",
    "title": "overfitting: a guided tour",
    "section": "Pulling it all together",
    "text": "Pulling it all together\nData has structure. Some of this structure is systematic, and some is random. We care about the systematic structure, but we don’t want to confuse the random structure for systematic structure.\nOnce we start estimating things, we need to be careful about how flexible we allow our models to be. This is true for both inferential and predictive modeling. If we allow our models to be more flexible than the true data generating process, we will mistake random structure for systematic structure. On the flip side, if we use models that don’t contain the true data generating process, we won’t capture all the systematic structure in the data (and we can also confuse the systematic and random structure).\nIn the prediction context, we saw that test error is higher than training error due to covariance between predictions and random errors in the training set. Hopefully this example demonstrates how easy it is to confuse systematic error and random error, and provides some intuition that you can use when analyzing data down the line."
  },
  {
    "objectID": "post/2020-01-06_overfitting-a-guide-tour/index.html#further-reading",
    "href": "post/2020-01-06_overfitting-a-guide-tour/index.html#further-reading",
    "title": "overfitting: a guided tour",
    "section": "Further reading",
    "text": "Further reading\nWhen \\(X_i\\) is random, the intuition about the train-test performance relationship is pretty much the same as for the fixed \\(X_i\\) case, but the randomness in \\(X_i\\) contributes some additional terms to the test set error. Rosset and Tibshirani (2017) discusses these additional terms.\nNote that getting good estimates of risk is a key element of model selection, but that cross validation is not a silver bullet. For example Shao (1993) proves the cross validation isn’t consistent for selecting amongst linear models, and Benavoli et al. (2017) and Lei (2017) discuss the need for additional modeling of cross-validated risk estimates to find the most predictive model. Section 2 of Lei (2017) is an especially eloquent introduction to cross-validation7. Roberts et al. (2017) is a nice introduction to specialized types of cross validation that respect independence structures found in spatial, temporal and network data. Vehtari, Gelman, and Gabry (2017) presents modern Bayesian approaches to sample splitting for model selection, and this blog post by Dan Simpson discusses the same work for non-i.i.d. data.\nChapter 7 of Hastie, Tibshirani, and Friedman (2008) presents an overview of cross-validation in the machine learning context. Arlot and Celisse (2010) is a comprehensive review of results in the cross-validation literature, in particular discussing how cross-validation procedures different when the goal is risk estimation (i.e. determining predictive performance) versus model selection (i.e. choosing hyperparameters). Dudoit and Laan (2005) proves an oracle efficiency result about model selection with cross-validation. Finally, cross-validation was introduced in the now classic Stone (1974).\n\nAcknowledgements\nI’d like to thank Julia Silge for providing feedback on a draft of this post!"
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html",
    "title": "to transform or not to transform",
    "section": "",
    "text": "You may have heard that it is impossible to compare models when the outcome has been transformed in one model but not the other. This is not the case. Models fit to transformed data implicitly model the original data as well as the transformed data, and it is relatively straightforward to calculate the corresponding likelihoods. In this post, I’ll show you how to calculate these induced likelihoods. This will allow you to compare models fit to transformed data with models fit to the original, untransformed data."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#setting",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#setting",
    "title": "to transform or not to transform",
    "section": "Setting",
    "text": "Setting\nLet’s assume have some continuous data \\(x_1, ..., x_n\\) that is independent and identically distributed. We transform this data via a nice, one-to-one function \\(g : \\mathbb{R} \\to \\mathbb{R}\\) that sends \\(x \\mapsto g(x)\\) to create a new data set \\(y_1, ..., y_n\\), such that \\(y_i = g(x_i)\\). We fit a model \\(\\mathcal M_x\\) on the original data \\(x_1, ..., x_n\\) and a model \\(\\mathcal M_y\\) on the transformed data \\(y_1, ..., y_n\\). Call the density of \\(\\mathcal M_x\\) and \\(\\mathcal M_y\\) respectively \\(f_x\\) and \\(f_y\\).\nSince \\(\\mathcal M_x\\) and \\(\\mathcal M_y\\) have been fit on different datasets, standard tools to compare \\(\\mathcal M_x\\) and \\(\\mathcal M_y\\) aren’t immediately applicable, and we have to use some tricks to compare them. Note that the internet (and literature, for example Burnham and Anderson (2002) p. 81-82) is littered with people saying that you can’t compare \\(\\mathcal M_x\\) and \\(\\mathcal M_y\\), and the few references (for example Akaike (1978)) that do discuss how to compare models with transformed responses are light on details."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#that-density-transformation-stuff-that-everyone-is-always-forgetting",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#that-density-transformation-stuff-that-everyone-is-always-forgetting",
    "title": "to transform or not to transform",
    "section": "That density transformation stuff that everyone is always forgetting",
    "text": "That density transformation stuff that everyone is always forgetting\nThe key observation is that putting a model on the \\(y_i\\) is equivalent to putting a model on the \\(x_i\\) since \\(g\\) is a one-to-one mapping and \\(y_i = g(x_i)\\). In particular, \\(\\mathcal M_y\\) has a density in terms of \\(x\\), in addition to have a density in terms of \\(y\\). Let’s call this density \\(f_{x'}\\).\nAt this point, we have to remember how transformations affect densities, which I always have to look up because I forget (Casella and Berger (2002) section 4.6 is one reference). In particular, for an individual \\(y_i\\), we have\n\\[\\begin{align}\n  f_y(y_i) = f_{x'}(g^{-1} (y_i)) \\cdot \\left \\vert \\frac{\\partial g^{-1}}{\\partial y} (y_i) \\right \\vert,\n\\end{align}\\]\nwhere the second term comes the Jacobian correction when doing a transformation of variables during integration. Working with individual individual data points is acceptable because we assume that the data points are indepedent and \\(y_i\\) is only a function of \\(x_i\\). Anyway, we leverage the transformed density to compute the log-likelihood of the \\(y_i\\) under \\(\\mathcal M_y\\) and find\n\\[\\begin{align}\n  \\log \\mathcal L(y_1, ..., y_n | \\mathcal M_{y})\n  &= \\sum_{i=1}^n \\log f_y(y_i) \\\\\n  &= \\sum_{i=1}^n \\log f_{x'}(g^{-1} (y_i)) + \\log \\left \\vert \\frac{\\partial g^{-1}}{\\partial y} (y_i) \\right \\vert \\\\\n  &= \\sum_{i=1}^n \\log f_{x'}(x_i) + \\log \\left \\vert \\frac{\\partial g^{-1}}{\\partial y} (y_i) \\right \\vert.\n\\end{align}\\]\nThis is promising, because the \\(\\sum_{i=1}^n \\log f_{x'}(x_i)\\) term is the induced log-likelihood of \\(\\mathcal M_y\\) on the original data \\(x_i\\), which is exactly what we want. Rearranging, we find\n\\[\\begin{align}\n  \\log \\mathcal L(x_1, ..., x_n | \\mathcal M_y)\n  &= \\sum_{i=1}^n \\log f_{x'}(x_i) \\\\\n  &= \\log \\mathcal L(y_1, ..., y_n | \\mathcal M_y) - \\sum_{i=1}^n \\log \\left \\vert \\frac{\\partial g^{-1}}{\\partial y} (y_i) \\right \\vert.\n\\end{align}\\]\nSo we can use a standard log-likelihood calculation for \\(y_i\\) together with an adjustment to compensate for transformation, and end up with the induced likelihood on the original data. This induced likelihood will be comparable to the likelihood induced by \\(\\mathcal M_x\\), and in general other models fit to the original, untransformed data."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#a-simple-example",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#a-simple-example",
    "title": "to transform or not to transform",
    "section": "A simple example",
    "text": "A simple example\nLet’s consider the mpg variable of the mtcars dataset, which comes with every R installation. We’ll consider two models:\n\na normal model for the original data, and\na normal model on log transformed data.\n\nThe data is boring, whatever, here’s a histogram:\n\n\n\n\n\nFirst let’s do some basic prep work and load our data, and define the transformation and the inverse transformation. We’ll also create the transformed data set.\n\nx <- mtcars$mpg\n\ntrans <- function(x) log(x)\ninv_trans <- function(y) exp(y)\n\ny <- trans(x)\n\nIn our case, we’re using a log() transformation, which is nice because we need both the derive and the inverse of this function, and both of these are easy to figure out.\nAside: for some traditional tranformations, such as Box-Cox transformations, or arcsine-square root transformation, finding the inverse by hand can be a pain. As a general rule, you can get the derivative either via automatic differentiation (not well supported in R) or numerical differentiation (moderately well supported in R). You can also use uniroot() to find inverses, but this approach is computationally expensive and I don’t in general recommend it.\nI strongly encourage an enterprising R developer to create an R package that provides objects encoding one-to-one transformations and their inverses, thus enabling the rest of us to be lazy. At the moment, the best hack I’ve come across is to the leverage link functions and their inverses from family objects.\nAnyway, at some point, you define the transformation, and the inverse transformation. It’s a good idea to sanity check that the functions you write are actually inverses of each other:\n\nx == inv_trans(trans(x))\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE\n[13]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n\nx == trans(inv_trans(x))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE\n\n\nIn my case, I’m working with log()s, and so there’s some numerical error and my inverses are not quite exact. In practice, it turns out that I can ignore this for this post, but this may not always be case1.\nOkay, now let’s define a function that computes the induced log-likelihood of \\(\\mathcal M_y\\) on the original, untransformated data. For convenience, I’m going to assume that our model is represented as a distribution object from my distributions3 package.\n\nlibrary(distributions3)\nlibrary(numDeriv)\n\nset.seed(27)\n\n#' @param dist The model fit to the transformed data `y`, as\n#'   a `distributions3::distribution` object.\n#' @param y The tranformed data.\n#' @param inv_trans The inverse transformation function.\ninduced_log_likelihood <- function(dist, y, inv_trans) {\n  \n  # note that grad() is a function from the numDeriv package\n  # for numerical differentiation. automatic differentiation\n  # would be slightly more computationally efficient but is\n  # not as supported in R\n  \n  log_likelihood(dist, y) - sum(log(abs(grad(inv_trans, y))))\n}\n\nNow we fit normal models to the original and the transformed data using MLE estimators.\n\nx_fit <- fit_mle(Normal(), x)\ny_fit <- fit_mle(Normal(), y)\n\nAt first glance, you might want to compare the likelihood of these models, but you should not, since x_fit and y_fit are not fit to the same data!\n\n# misleading! don't do this when y = g(x)!\nlog_likelihood(x_fit, x)\nlog_likelihood(y_fit, y)\n\nInstead we want to compare the likelihood from the model fit to x and the induced likelihood on the \\(x\\) from the fit to y.\n\nlog_likelihood(x_fit, x)\n\n[1] -102.3857\n\ninduced_log_likelihood(y_fit, y, inv_trans)\n\n[1] -100.781\n\n\nIn our case, it looks like the transformed fit is slightly better (recall that more positive log-likelihoods are better). It’s a bit hard to interpret log-likelihoods until you start playing with them more, but this is a relatively small practical difference."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#a-linear-regression-example",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#a-linear-regression-example",
    "title": "to transform or not to transform",
    "section": "A linear regression example",
    "text": "A linear regression example\nNow we’ll leave the pleasant world of distributions3 and do a more practical linear regression example. Now we’ll use mpg as an outcome of a simple linear regression, and use the wt variable from the mtcars dataset as a predictor. We start by fitting out models, again using the log() transformation we previously defined.\n\noriginal_fit <- lm(mpg ~ wt, mtcars)\ntransformed_fit <- lm(trans(mpg) ~ wt, mtcars)\n\nNow we need a way to compute the log-likelihood of these models. We leverage the existing logLik() function for this.\n\n# only works for lm-like objects, but hopefully easy to generalize\n# returns a logLik object\ninduced_logLik <- function(fit, inv_trans) {\n  y <- model.response(model.frame(fit))\n  logLik(fit) - sum(log(abs(grad(inv_trans, y))))\n}\n\nThis let’s us compare the original model and the transformed model:\n\nlogLik(original_fit)\n\n'log Lik.' -80.01471 (df=3)\n\ninduced_logLik(transformed_fit, inv_trans)\n\n'log Lik.' -75.21614 (df=3)\n\n\nAgain, it looks like the transformed model is slightly better (recall that high bigger densities are more likely, so high log-likelihoods are more probable). However, these models are not nested, and so we cannot apply standard likelihood ratio tests (or Wald or Rao tests).\nYou may have been taught that this makes these models incomparable, but this is not the case. The log-likelihood is a proper scoring rule, and so all we really want is to minimize the log-likelihood, as measured on the original, untransformed data. The real issue is that there isn’t a nice analytical form for a likelihood ratio test.\nHowever, we can use the bootstrap to estimate a sampling distribution for the log-likelihoods of these models, just like we might with another loss function like mean square error2.\nAnyway, here’s some code for a simple bootstrap:\n\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\n\nboots <- bootstraps(mtcars, times = 200)\n\nfit_on_original_data <- function(split)\n  lm(mpg ~ wt, analysis(split))\n\nfit_on_transformed_data <- function(split)\n  lm(trans(mpg) ~ wt, analysis(split))\n\nfits <- boots %>% \n  mutate(\n    original_fit = map(splits, fit_on_original_data),\n    transformed_fit = map(splits, fit_on_transformed_data),\n    original_llh = map_dbl(original_fit, logLik),\n    induced_llh = map_dbl(transformed_fit, induced_logLik, inv_trans),\n  ) %>% \n  mutate_if(~inherits(.x, \"logLik\"), as.numeric)\n\nAt this point you could do a paired T-test on the differences, or you could just plot the paired differences in log-likelihoods between the models, resulting in the following:\n\n\n\n\n\nAgain, it seems like the transformed model is better, although only marginally so. You could also compare the AIC and BIC of the two models.\n\ninduced_AIC <- function(fit, inv_trans) {\n  lls <- induced_logLik(fit, inv_trans)\n  -2 * as.numeric(lls) + 2 * attr(lls, \"df\")\n}\n\ninduced_BIC <- function(fit, inv_trans) {\n  lls <- induced_logLik(fit, inv_trans)\n  nos <- attr(lls, \"nobs\")\n  -2 * as.numeric(lls) + log(nos) * attr(lls, \"df\")\n}\n\nAIC(original_fit)\n\n[1] 166.0294\n\ninduced_AIC(transformed_fit, inv_trans)\n\n[1] 156.4323\n\n\nSo tranformation is better in terms of AIC (smaller is better).\n\nBIC(original_fit)\n\n[1] 170.4266\n\ninduced_BIC(transformed_fit, inv_trans)\n\n[1] 160.8295\n\n\nIt is also better in terms of BIC (smaller is better)3.\n\n# these should be equal, but are not\ndeviance(original_fit)\n\n[1] 278.3219\n\nas.numeric(-2 * logLik(original_fit))\n\n[1] 160.0294\n\n\nI’m guessing that these differ by a scaling factor only, since it’s pretty common to omit constant terms from likelihood calculations for efficiency, but also it does result in mysteries like this. It looks like someone else came across a similar thing for GLMs, so heads up I guess."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#for-skeptical-machine-learners",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#for-skeptical-machine-learners",
    "title": "to transform or not to transform",
    "section": "For skeptical machine learners",
    "text": "For skeptical machine learners\nIf you come from a machine learning background, you may find this whole exercise pointless, for two reasons.\nThe first is that you’re probably using an estimator that doesn’t give you densities for your predictions, so comparing likelihoods of transformed and untransformed models isn’t possible for you. It probably feels natural to get predictions using your model fit to the transformed data, and then backtransform to get predictions on the original scale.\nThis is what sklearn does with the TransformedTargetRegressor, for example. You may be surprised to learn that this approach does not in general result in consistent predictions for conditional means! See Duan (1983) for a short, pleasant read on why this is not the case, as well as an introduction to Duan’s smearing estimator, which is consistent for prediction under some relatively sane conditions on the transformation \\(g\\). As a general rule, backtransformed predictions can be surprisingly nasty to reason about.\nYou might also have a second complaint, which is that the likelihood is dumb loss function, and that you should use mean squared error instead. If this is the case, invite your friendly neighborhood Bayesian to yell at you.\n\nOne final trick: Box-Cox transformations\nIt turns out that this likelihood adjustment trick is also how to people estimate the optimal \\(\\lambda\\) for Box-Cox transformations. The Box-Cox transformation is a slight variation on a power tranformation such that \\(\\lambda = 0\\) corresponds to taking a logarithm, and looks like:\n\\[\\begin{align}\n  y &=\n  \\begin{cases}\n    \\displaystyle \\frac{x^\\lambda - 1}{\\lambda} & \\text{ if } \\lambda \\neq 0 \\\\\n    \\log x & \\text{ if } \\lambda = 0\n  \\end{cases}\n\\end{align}\\]\nIf you want to do a quick transformation of \\(y\\), you can fit a normal distribution to \\(y\\). Using distributions3, we can compute the log-likelihood on the original data via the transformation adjustment.\n\nboxcox <- function(x, lambda) {\n  if (lambda == 0) log(x) else (x^lambda - 1) / lambda\n}\n\n# i calculated this inverse by hand which was fine\n# but wow yeah it would be really nice to have a package\n# that automatically finds inverses for common bijections\ninv_boxcox <- function(y, lambda) {\n  if (lambda == 0) exp(y) else (lambda * y + 1)^(1 / lambda)\n}\n\n# not vectorized in lambda!\nsimple_boxcox_log_likelihood <- function(x, lambda) {\n  y <- boxcox(x, lambda)\n  y_fit <- fit_mle(Normal(), y)\n  log_likelihood(y_fit, y) - \n    sum(log(abs(grad(inv_boxcox, y, lambda = lambda))))\n}\n\nIf we plot this log-likelihood as a function of \\(\\lambda\\) we see that a log transformation is about optimal.\n\n\n\n\n\nWe could also fit more complicated models to \\(y\\). For example, a simple linear regression.\n\n# not vectorized in lambda!\nprofile_boxcox_log_likelihood <- function(x, z, lambda) {\n  \n  # x is the original data\n  # y the transformed data\n  # z is a predictor in a linear regression of z on y\n  y <- boxcox(x, lambda)\n  y_fit <- lm(y ~ z)\n  logLik(y_fit) - sum(log(abs(grad(inv_boxcox, y, lambda = lambda))))\n}\n\nUsing the wt variable from the mtcars dataset as a predictor for y in a linear regression, we now find that something more like an inverse quartic root transformation is optimal."
  },
  {
    "objectID": "post/2020-03-22_to-transform-or-not-to-transform/index.html#tl-dr",
    "href": "post/2020-03-22_to-transform-or-not-to-transform/index.html#tl-dr",
    "title": "to transform or not to transform",
    "section": "TL; DR",
    "text": "TL; DR\n\nComparing models with transformed and non-transformed outcomes is hard. You should probably use held-out induced likelihoods for these comparisons, or maybe the optimism bootstrap.\nPrediction is hard with transformations. Use Duan’s Smearing estimator. Also, please, please, please report error on the original scale. Nobody knows how to interpret the root mean square error of log home price.\nTransforming your outcome, with either a direct transformation or a link function, dramatically changes the interpretation of parameters in your model.\nThere isn’t much computational infrastructure to support inference after transformation, so be aware of how this can cause frustration down the line.\nJust use a GLM with a link function instead. You can thank me later.\n\nThanks to Aki Vehtari for writing an FAQ on cross-validation with some comments on transformed outcomes. I didn’t understand some of his comments, and this post is largely a result of me sorting those comments out for myself. Also thanks to commenters who point out helped me when I got stuck in this Stan discourse thread.\nIf you are interested in trying out many transformations all at once, with a Gaussian model on the transformed outcome, you may be interested in the trafo package (Medina et al. (2019)). The associated paper in the R journal derives likelihood adjustments for an enormous number of common transformations. Note that trafo takes a rather different approach to model selection that I do in this blog post – i.e. it helps you find a transformation of your outcome that has normally distributed errors with constant variance, whereas I instead suggest choosing the model that minimizes out-of-sample log-likelihood."
  },
  {
    "objectID": "post/2020-05-01_elon-musk-send-tweet/index.html",
    "href": "post/2020-05-01_elon-musk-send-tweet/index.html",
    "title": "synthetic control: elon’s tweet tanked tesla’s stock",
    "section": "",
    "text": "and Tesla stock started tanking. I find this absolutely hilarious, especially since he did this a while back and got fined like several million dollars for tinkering with market or something illegal like that.\nAnyway, I asked myself: can we causally attribute Tesla tanking stock price to this tweet?\nThe answer is yes, yes, we absolutely can. In the following I use a synthetic control approach to estimate the causal impact of Musk’s tweet on the Tesla stock price (God I hate that I just wrote that sentence). You can read more about this approach in Brodersen et al. (2015).\nI used the S&P500 as my synthetic control and pull 1-minute resolution ticker data using Tiingo with the riingo package, then blindly shoved the data into CausalImpact. We get the following:\n\n\n\n\n\nIn the first panel, the blue is Tesla stock price as predicted by the S&P 500 based on a Bayesian state space model, and the black is the actual Tesla stock price. Elon tweets at the dashed vertical line. You can see that before the tweet, Tesla stock prices are well-predicted by the S&P 500, and afterwards, the Tesla stock drops to prices lower than predicted based on the S&P 500. The current best estimate is that the tweet is responsible for about a $40 drop in the price of Tesla, although there is clearly a fair amount of associated uncertainty in this.\nOf course, you probably shouldn’t believe these estimates since I didn’t generate them with neural net.\nAnyway, I’m gonna go finish a topology takehome now, and try to tank my grade by less than Elon tanked his stock. In the meantime, you should all read Brodersen et al. (2015), it’s great. Everyone else, continue shitposting. Well, unless you’re Elon, in which case, maybe don’t.\n\n\n\n\nReferences\n\nBrodersen, Kay H., Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L. Scott. 2015. “Inferring Causal Impact Using Bayesian Structural Time-Series Models.” The Annals of Applied Statistics 9 (1): 247–74. https://doi.org/10.1214/14-AOAS788.\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html",
    "href": "post/2020-05-04_using-the-data-twice/index.html",
    "title": "using the data twice",
    "section": "",
    "text": "Berna Devezer, Danielle Navarro, Joachim Vandekerckhove, and Erkan Ozge Buzbas recently posted a pre-print, Devezer et al. (2020), responding to various claims within the open science community1. In particular, they explore the following claims:\nI find the arguments presented against these claims compelling, and anticipate directing many people to Devezer et al. (2020) in the future. This post is largely a re-expression of some ideas in their paper in language that feels slightly more natural to me."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#the-setting-the-key-idea",
    "href": "post/2020-05-04_using-the-data-twice/index.html#the-setting-the-key-idea",
    "title": "using the data twice",
    "section": "The setting & the key idea",
    "text": "The setting & the key idea\nIn a data analysis, we observe a sample \\(X_1, ..., X_n\\) and compute some estimate from the sample. The properties of the estimate depend on every step we take to report our final result; for example, if we fit a linear regression, look at residuals, add a spline term, and report predictions from the model with the spline term, these decisions influence the sampling distribution of the final predictions.\nThe fact that our entire procedure matters leads to a lot of confusion."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#what-does-using-the-data-twice-mean",
    "href": "post/2020-05-04_using-the-data-twice/index.html#what-does-using-the-data-twice-mean",
    "title": "using the data twice",
    "section": "What does “using the data twice” mean?",
    "text": "What does “using the data twice” mean?\nWhile reading Devezer et al. (2020) I found it useful to think about three interpretations of the phrase “using the data twice”.\n\nSequential decision making. A data analyst computes an estimate, and uses it to inform the next step of their data analysis. This is in my mind the most traditional meaning of “using the data twice” (although for bayesians “using the data twice” might more commonly mean using the data to pick your priors).\nDeriving new estimators by conditioning an estimating estimator on another statistic. There is a classic result in mathematical statistics that says if you calculate \\(\\mathbb E(U|T)\\) where \\(U\\) is unbiased and \\(T\\) is sufficient, good things happen. This can be thought of as using the data twice.\nReusing data during computations. For example, as you might with in-sample risk estimation or empirical bayes. Devezer et al. (2020) isn’t about computational data reuse, but I kept reading “using the data twice” and thinking computationally (especially with the T-statistic example). I was better able to follow the paper after blocking out “computational data reuse” as a separate thing.\n\nI am going to focus on sequential decision making."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#sequential-decision-making",
    "href": "post/2020-05-04_using-the-data-twice/index.html#sequential-decision-making",
    "title": "using the data twice",
    "section": "Sequential decision making",
    "text": "Sequential decision making\nLet’s first consider an example that most people would consider an obvious and unacceptable “double use” of data. Suppose we do a one sample T-test, but we only report the test statistic (call it \\(T_1\\)) if the p-value is less than 0.05. If the p-value is greater than 0.05, we will instead report some other more significant test statistic \\(T_2\\).\nUnder a two-sided alternative, the p-value of \\(T_1\\) is \\(p = 1 - \\phi_{n-1}(|T_1|) + \\phi_{n-1}(-|T_1|)\\) where \\(\\phi_n\\) is cdf of a T-distribution with \\(n\\) degrees of freedom. Now our overall, actual test statistic is\n\\[\n   T = T_1 \\, \\mathbf 1(p < 0.05) + T_2 \\, \\mathbf 1(p \\ge 0.05),\n\\]\nwhere \\(\\mathbf 1\\) denotes an indicator function. If you decide between reporting \\(T_1\\) or \\(T_2\\) based on \\(p\\) with a threshold of 0.05, you are using this test statistic. If you compute \\(T\\) but report that you did \\(T_2\\), not realizing that entire procedure to arrive at \\(T_2\\) is important, someone else will go to calculate your p-value as\n\\[\n  P(|T_2(X_1, ..., X_n)| > T_2(x_1, ..., x_n))\n\\]\nwhere \\(T_2(x_1, ..., x_n)\\) describes the observed test statistic and \\(T_2(X_1, ..., X_n)\\) is the random variable corresponding to \\(T_2\\). But really, they need to calculate the p-value as\n\\[\n  P(|T(X_1, ..., X_n)| > T(x_1, ..., x_n))\n\\]\nand so they will be using the wrong p-value, and they will think they are doing \\(T_2\\), a test with some nice set of properties, when in fact they are doing the test \\(T\\), which may not have any nice properties at all, and in fact is likely a very bad test from an inferential perspective, at least based on typical choices of \\(T_2\\) amongst social scientists doing null hypothesis testing."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#slight-generalization-a-two-stage-estimation-problem",
    "href": "post/2020-05-04_using-the-data-twice/index.html#slight-generalization-a-two-stage-estimation-problem",
    "title": "using the data twice",
    "section": "Slight generalization: a two-stage estimation problem",
    "text": "Slight generalization: a two-stage estimation problem\nSuppose we want to estimate some parameter \\(\\theta\\) from a sample \\(X_1, ..., X_n\\). First we will compute some point estimate \\(T^{(1)}\\). We will look at \\(T^{(1)}\\) and make some decision based on \\(T^{(1)}\\). In particular, we will choose one of \\(k\\) estimators \\(T^{(2)}_1, ..., T^{(2)}_k\\) and report the estimate from this estimator. We have some decision function, \\(f\\), that, given \\(T^{(1)}\\), we use to choose which of the second stage estimators we will report. In particular, if \\(T^{(1)} \\in A_j\\), where \\(A_1, ..., A_k\\) partition the space of possible estimates from \\(T^{(1)}\\), we will report results from the second stage estimator \\(T^{(2)}_j\\). So our overall estimator is given by\n\\[\n  T = T^{(2)}_{f(T^{(1)})} = T^{(2)}_1 \\, \\mathbf 1( T^{(1)} \\in A_1) +  \n    ... + T^{(2)}_k \\, \\mathbf 1( T^{(1)} \\in A_k).\n\\]\nThe common admonition against using the data twice is directed at researchers who use the unconditional sampling distribution of whichever \\(T^{(2)}_j\\) they selected. Gelman made this mistake famous on his blog and in Gelman and Loken (2014), where he defined the “garden of forking paths” in data analysis. Mayo (2018) refers to this “biasing selection effects.”\nSo how do we do correct inference when our estimation procedure involves sequential decision making? There are two possibilities: (1) we can use sampling distribution of \\(T\\) (i.e. the sampling distribution of the entire procedure), or (2) we can use the sampling distribution of \\(T | \\{T^{(1)} = t_1\\}\\), which equals the sampling distribution of \\(T^{(2)}_j | \\{T^{(1)} = t_1\\}\\) (i.e. the sampling distribution of the final estimator we selected, conditional on selecting that estimator).\nI had a major confusion on my first reading of the paper when I assumed that we would only ever want to do inference unconditionally on the whole procedure (i.e. using \\(T\\)). In particular, I misread Devezer et al. (2020) as claiming that the unconditional distribution of \\(T\\) has nice properties. This is neither true, nor the claim they make! Devezer et al. (2020) instead makes the point that if you condition \\(T\\) on the decisions you made (i.e. use \\(T^{(2)}_j | \\{T^{(1)} = t_1\\}\\) for inference) you can still do inference, and if you made decisions based on nice statistics the conditioning might even improve the properties of your estimator2.\nIt’ll probably help to be a bit more concrete about the difference between correct unconditional inference and correct conditional inference. Consider backwards stepwise regression. In backwards stepwise regression, we fit a linear model on the full original data with OLS, and then iteratively remove terms with the largest p-value until all the remaining p-values are less than \\(\\alpha = 0.05\\). Suppose our data has three predictors \\(X_1, X_2\\) and \\(X_3\\), and we run backwards stepwise regression. In the full model \\(X_3\\) has the largest p-value, and we remove it. Then we refit on \\(X_1\\) and \\(X_2\\) and both terms have p-values less than 0.05, so we stop and report this final model.\nCorrect conditional inference in this case will produce probabilities statements like:\n\nUnder a Gaussian linear model with three predictors \\(X_1, X_2\\) and \\(X_3\\), if we perform backwards stepwise regression and remove \\(X_3\\) from the model, and then stop, the \\(\\alpha\\)-level confidence set for the model coefficients returned from the final OLS model will have coverage level \\(\\gamma_1\\).\n\nCorrect unconditional inference in this case will produce probabilities statements like:\n\nUnder a Gaussian linear model with three predictors \\(X_1, X_2\\) and \\(X_3\\), the \\(\\alpha\\)-level confidence set for the model coefficients returned from the final OLS model will have coverage level \\(\\gamma_2\\).\n\nPersonally I find the unconditional inference much more useful than the conditional inference. For this particular example, note that both \\(\\gamma_1\\) and \\(\\gamma_2\\) will be atrocious, but hopefully this illustrates the general point about interpretation3.\nAs further examples, the Benjamini-Hochberg procedure results in unconditional false discovery control, whereas selective inference4 can only be interpreted conditional on the selected penalty parameter \\(\\lambda\\)."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#just-use-the-sampling-distribution-of-the-procedure-they-said.-it-will-be-easy-they-said.",
    "href": "post/2020-05-04_using-the-data-twice/index.html#just-use-the-sampling-distribution-of-the-procedure-they-said.-it-will-be-easy-they-said.",
    "title": "using the data twice",
    "section": "Just use the sampling distribution of the procedure, they said. It will be easy, they said.",
    "text": "Just use the sampling distribution of the procedure, they said. It will be easy, they said.\nThe real problem is that data analysts might go through many, many decisions before arriving at some final estimate, but the mental process of selecting amongst several competing estimators is not well defined. If a data analyst looks at a residual plot and judges the residuals to be fine based on some gut intuition, this “decision” cannot be encoded formally. If you ask the analyst “what procedure did you follow to get this estimate?” so that you can evaluate the properties of that procedure, most analysts can’t tell you. Humans behaviors do not follow well-specified algorithms, or at least not algorithms that we can elicit. This means that frequentist properties of most procedures carried out by humans aren’t well-defined.\nEven though data analytic procedures may not be well-defined, they can definitely be good! For example, consider updating a model after investigating model diagnostics, a poorly defined procedure that most statisticians nonetheless find essential.\nPre-registration essentially tries to fix ill-definedness of procedures. Pre-registration asks people to describe the procedure they use to arrive at an estimate so that other people can evaluate the properties of that procedure. This transparency is admirable, and in general, I am fully on board with pushing the scientific community to specify more explicitly the procedures they are using to analyze data. But pre-registration only applies to situations when scientists can follow fixed procedures to learn new things.\nCritiques of pre-registration often point out that if you require data analytic procedures to be strictly algorithmic, you give up on a large swathe of (heretofore productive!) science. There is also the fact that knowing the sampling distribution and properties of an estimator does not mean that those properties are good, or that inference based on that estimator is warranted."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#dealing-with-procedures-in-real-life",
    "href": "post/2020-05-04_using-the-data-twice/index.html#dealing-with-procedures-in-real-life",
    "title": "using the data twice",
    "section": "Dealing with procedures in real life",
    "text": "Dealing with procedures in real life\nStatisticians have known that the sampling distribution of the entire procedure matters for a long time. Current practice is a mixture of recognizing this fact when there’s some solution with provably nice properties, and ignoring it, appealing to intuition, or hoping it doesn’t matter when working the sampling distribution of the entire procedure is intractable.\nHere are some ways statisticians handle the mismatch between sampling distributions of procedures and unconditional sampling distributions of individual estimators:\n\nBeing aware of the (potentially large) discrepancy between the sampling distributions, or interpreting results as conditional on decisions made by the data analyst.\nHeuristically trying to limit the dependence between decisions during data analysis and the measures of interest.\nAssessing the stability of a inference by comparing results from a variety of procedures (i.e. multiverse analysis).\nSimulating or bootstrapping entire procedures if possible to assess unconditional properties of the properties.\nTransparently describing the entire data analytic procedure, as well as releasing code and data.\n\nFrank Harrell’s Regression Modeling Strategies deserves special note for providing practical advice on performing inference using the sampling distribution of the entire estimation procedure, and I believe is famous for being one of the first textbooks to emphasize the importance of procedures (Harrell (2015)). Another great textbook that carefully considers procedures is Shalizi (2018)."
  },
  {
    "objectID": "post/2020-05-04_using-the-data-twice/index.html#the-end",
    "href": "post/2020-05-04_using-the-data-twice/index.html#the-end",
    "title": "using the data twice",
    "section": "The End",
    "text": "The End\nDevezer et al. (2020) point out that to understand the properties of your estimator, you need to consider the sampling distribution of your estimation procedure, either conditionally or unconditionally. Understanding the form and properties of the sampling distribution is a key piece to much of the ongoing discussion about pre-registration.\nPersonally, I am curious about justification for frequentist inference using iterative model development procedures. What kind of guarantees can we make when we formally consider, say, model selection. One interesting prospect seems to be decision-analytic inference that doesn’t rely on the sampling distribution of the entire procedure. For example, Van Calster et al. (2016) proves that any prognostic model that satisfies a criterion they call “moderate calibration” is not harmful, as measured by a clinical utility score called Net Benefit5. Notably, calibration can be assessed for any predictive model, regardless of how analysts arrived at the model itself.\n\nAcknowledgements\nI would like to thank Dr. Devezer and Dr. Buzbas for an extensive discussion and feedback on this post!"
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html",
    "title": "many models workflows in python i",
    "section": "",
    "text": "This summer I worked on my first substantial research project in Python. I’ve used Python for a number of small projects before, but this was the first time that it was important for me to have an efficient workflow for working with many models at once. In R, I spend almost all of my time using a ‘many models’ workflow that leverages list-columns in tibbles and a hefty amount of tidyverse manipulation. It’s taken me a while to find a similar workflow in Python that I like, and so I’m documenting it here for myself and other parties who might benefit.\nThis post demonstrates how to organize models into dataframes for exploratory data analysis, and discusses why you might want to do this. In a future blog post I will show how to extend the basic workflow I present here to handle sample splitting, custom estimators, and parallel processing."
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#interlude-for-pythonistas-many-models-workflows",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#interlude-for-pythonistas-many-models-workflows",
    "title": "many models workflows in python i",
    "section": "Interlude for Pythonistas: many models workflows",
    "text": "Interlude for Pythonistas: many models workflows\nThe many models workflow is an extension of the ‘split-apply-combine’ workflow, a largely functional approach to data manipulation implemented in dplyr and pandas. The essential ideas of ‘split-apply-combine’ are articulated nicely in Hadley Wickham’s short and easy to read paper, which I strongly encourage you to read. I assume you are comfortable with this general idea, which largely facilitates natural ways to compute descriptive statistics from data, and have some experience applying these concepts in either dplyr, pandas, or SQL.\nSeveral years ago, this idea evolved: what if, instead of computing descriptive statistics, we wanted to compute more complicated estimands, while leveraging the power of grouped operations? Hadley’s solution, which has proven very fruitful and served as the underlying idea for tidymodels, tidyverts, and several other modeling frameworks, is to put model objects themselves into dataframes. Hadley presented on this idea, and also wrote about it in the many models chapter of R for Data Science and it has turned out to be quite fruitful."
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#why-is-putting-model-objects-in-dataframes-a-good-idea",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#why-is-putting-model-objects-in-dataframes-a-good-idea",
    "title": "many models workflows in python i",
    "section": "Why is putting model objects in dataframes a good idea?",
    "text": "Why is putting model objects in dataframes a good idea?\nThe simple answer is that it keeps information about your models from drifting apart, as it tends to do otherwise.\nMy exploratory modeling often ends up looking something like this:\n\nI want to compare models across a range of hyperparameter values. Often there are several distinct hyperparameters to consider at once1.\nI want to look at many different properties of the model. As a contrived example, I might want to look at AIC, BIC, \\(R^2\\) and RMSE for all my models2.\nI want to quick create plots to compare these models, and am probably using a plotting libraries expects data in data frames\n\nAnyway it turns out that dataframes handle this use case super well, provided we have some helpers. The overall workflow will look like this:\n\nSpecifying models to fit: We organize a dataframe so that each row of the dataframe corresponds to one model we want to fit. For example, a single row of the dataframe might correspond to a single set of hyperparameter values, or a subset of the data.\nIterative model fitting: We create a new column in the dataframes that holds fit models.\nIterative estimate extraction: We extract information we want from the fits into new data frame columns, and then manipulate this data with our favorite dataframe or plotting libraries.\n\nNote that steps (2) and (3) require iteration over many models. In functional languages, map-style operations are a natural (and easily parallelizable!) way to do this; in Python we can use list-comprehensions.\nAnother innovation in this workflow came from standardizing step (3), where we extract information from the models into a new column of the dataframe. A big issue that we can run into here is that when we extract information from the model object, it can have an inconvenient type that is hard to put in a dataframe column. This may seem esoteric but it turns out to matter a lot more than you’d expect.\nDavid Robinson in his broom package proposed a solution that is increasingly the standard within the R community3. The idea is to create special getter methods for model objects that always return information in consistently formatted data frames. For each model object, we get a data frame with information. Since there are many model objects, we end up with a column of dataframes, which we then flatten.\nThere has been a lot of pushback around the idea of a column of dataframes with the Python community, largely on the basis that it is not a performant data structure4. This misses the point. The compute time in these workflows comes from model fitting, and afterwards we just want to keep track of things5.\nAnyway, there’s a lot more to say about the workflow conceptually, but for now, it is time to show some code and see how things work in practice. For this post, I am going to recreate the analysis that Hadley does in his linked presentation above, which makes use of the gapminder data. The gapminder data consists of life expectancies for 142 counties, reported every 5 years from 1952 to 2007."
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#setting-the-scene",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#setting-the-scene",
    "title": "many models workflows in python i",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\n# you can download the gapminder csv file from my google drive\n# https://drive.google.com/file/d/1pAgIKdsZPwYteQ2rfq14WrUtS23Tg4Lu/\n\ngapminder = pd.read_csv('gapminder.csv')\ngapminder\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      Africa\n      1987\n      62.351\n      9216418\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      Africa\n      1992\n      60.377\n      10704340\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      Africa\n      1997\n      46.809\n      11404948\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      Africa\n      2002\n      39.989\n      11926563\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      Africa\n      2007\n      43.487\n      12311143\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n\n\nTo get a feel for the data, lets plot the life expectancy of each country over time, facetting by continent6.\n\np = sns.FacetGrid(\n    data=gapminder,\n    hue='continent',\n    col='continent',\n    col_wrap=2,\n    height=3,\n    aspect=16/9\n).map(plt.plot, 'year', 'lifeExp')\n\nplt.show()"
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-1-specifying-models-to-fit",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-1-specifying-models-to-fit",
    "title": "many models workflows in python i",
    "section": "Step 1: specifying models to fit",
    "text": "Step 1: specifying models to fit\nFollowing Hadley’s presentation, suppose we would like to summarize the trend for each country by fitting a linear regression to the data from each country. So we have a correspondence 1 model ~ data from 1 country, and want to set up our data frame so that each row corresponds to data from a single country.\ngroupby() plus a list-comprehension handles this nicely, leveraging the fact that gapminder.groupby('country') is an iterable. In R, you could also use group_by() for this step, or additionally nest() or rowwise(), two tidyverse specification abstractions.\n\nmodels = pd.DataFrame({\n    # this works because grouped dataframes in pandas are iterable\n    # and because you can pretty much treat series objects like\n    # they are lists\n    'data': [data for _, data in gapminder.groupby('country')],\n})\n\nmodels.index = [country for country, _ in gapminder.groupby('country')]\n\n# the downside of putting weird stuff into pandas dataframes is that\n# the dataframes print poorly\nmodels\n\n\n\n\n  \n    \n      \n      data\n    \n  \n  \n    \n      Afghanistan\n      country continent  year  lifeExp      ...\n    \n    \n      Albania\n      country continent  year  lifeExp      pop ...\n    \n    \n      Algeria\n      country continent  year  lifeExp       pop...\n    \n    \n      Angola\n      country continent  year  lifeExp       pop ...\n    \n    \n      Argentina\n      country continent  year  lifeExp       p...\n    \n    \n      ...\n      ...\n    \n    \n      Vietnam\n      country continent  year  lifeExp       p...\n    \n    \n      West Bank and Gaza\n      country continent  year  life...\n    \n    \n      Yemen, Rep.\n      country continent  year  lifeExp    ...\n    \n    \n      Zambia\n      country continent  year  lifeExp       po...\n    \n    \n      Zimbabwe\n      country continent  year  lifeExp       ...\n    \n  \n\n142 rows × 1 columns"
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-2-iterative-model-fitting",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-2-iterative-model-fitting",
    "title": "many models workflows in python i",
    "section": "Step 2: iterative model fitting",
    "text": "Step 2: iterative model fitting\nNow we need to do the actual model fitting. My preferred approach is to use list-comprehensions.\n\ndef country_model(df):\n    return smf.ols('lifeExp ~ year', data=df).fit()\n\nmodels['fit'] = [\n    country_model(data)\n    for _, data in gapminder.groupby('country')\n]\n\nOne compelling advantage of this (effectively) functional approach to iteration over list-columns of models is that most of these computations are embarrassingly parallel, and map()-like operations are often very easy to parallelize.\nAn alternative approach here is to use DataFrame.apply(). However, I have found the Series.apply() and DataFrame.apply() methods to be hard to reason about when used together with list-columns, and so I recommend avoiding them.\n\n# the pandas apply() approach\n\n# models = (\n#     gapminder\n#     .groupby('country')\n#     .apply(country_model)\n#     .to_frame(name='fit')\n# )"
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-3-iterative-information-extraction",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#step-3-iterative-information-extraction",
    "title": "many models workflows in python i",
    "section": "Step 3: iterative information extraction",
    "text": "Step 3: iterative information extraction\nNow that we’ve fit all of our models, we can extract information from them. Here I’ll define some helper functions very much in the spirit of broom. When you don’t own the model classes you’re using, you pretty much have to write extractor functions to do this; see Michael Chow’s excellent blog post showing how to handle this in an elegant way.\nEven if you do own the model objects you’re using, I recommend extractor functions over class methods. This is because, during EDA, you typically fit some expensive models once, and then repeatedly investigate them–you can modify an extractor function and use it right away, but if you modify a method for a model class, you’ll have to refit all the model objects. This leads to slow iteration.\n\n# ripped directly from michael chow's blog post!!\n# go read his stuff it's very cool!\ndef tidy(fit):\n    from statsmodels.iolib.summary import summary_params_frame\n    tidied = summary_params_frame(fit).reset_index()\n    rename_cols = {\n        'index': 'term', 'coef': 'estimate', 'std err': 'std_err',\n        't': 'statistic', 'P>|t|': 'p_value',\n        'Conf. Int. Low': 'conf_int_low', 'Conf. Int. Upp.': 'conf_int_high'\n    }\n    \n    return tidied.rename(columns = rename_cols)\n\ndef glance(fit):\n    return pd.DataFrame({\n        'aic': fit.aic,\n        'bic': fit.bic,\n        'ess': fit.ess, # explained sum of squares\n        'centered_tss': fit.centered_tss,\n        'fvalue': fit.fvalue,\n        'f_pvalue': fit.f_pvalue,\n        'nobs': fit.nobs,\n        'rsquared': fit.rsquared,\n        'rsquared_adj': fit.rsquared_adj\n    }, index=[0])\n\n# note that augment() takes 2 inputs, whereas tidy() and glance() take 1\ndef augment(fit, data):\n    df = data.copy()\n    \n    if len(df) != fit.nobs:\n        raise ValueError(\"`data` does not have same number of observations as in training data.\")\n    \n    df['fitted'] = fit.fittedvalues\n    df['resid'] = fit.resid\n    return df\n\nWe sanity check the helper functions by seeing if they work on a single model object before working with the entire list-column of models.\n\ntidy(models.fit[0])\n\n\n\n\n  \n    \n      \n      term\n      estimate\n      std_err\n      statistic\n      p_value\n      conf_int_low\n      conf_int_high\n    \n  \n  \n    \n      0\n      Intercept\n      -507.534272\n      40.484162\n      -12.536613\n      1.934055e-07\n      -597.738606\n      -417.329937\n    \n    \n      1\n      year\n      0.275329\n      0.020451\n      13.462890\n      9.835213e-08\n      0.229761\n      0.320896\n    \n  \n\n\n\n\n\nglance(models.fit[0])\n\n\n\n\n  \n    \n      \n      aic\n      bic\n      ess\n      centered_tss\n      fvalue\n      f_pvalue\n      nobs\n      rsquared\n      rsquared_adj\n    \n  \n  \n    \n      0\n      40.69387\n      41.663683\n      271.006011\n      285.958116\n      181.24941\n      9.835213e-08\n      12.0\n      0.947712\n      0.942483\n    \n  \n\n\n\n\naugment() actually takes two inputs, where one input is the model object, and the other is the training data used to fit that model object.\n\naugment(models.fit[0], models.data[0])\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n      fitted\n      resid\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n      29.907295\n      -1.106295\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n      31.283938\n      -0.951938\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n      32.660582\n      -0.663582\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n      34.037225\n      -0.017225\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n      35.413868\n      0.674132\n    \n    \n      5\n      Afghanistan\n      Asia\n      1977\n      38.438\n      14880372\n      786.113360\n      36.790512\n      1.647488\n    \n    \n      6\n      Afghanistan\n      Asia\n      1982\n      39.854\n      12881816\n      978.011439\n      38.167155\n      1.686845\n    \n    \n      7\n      Afghanistan\n      Asia\n      1987\n      40.822\n      13867957\n      852.395945\n      39.543798\n      1.278202\n    \n    \n      8\n      Afghanistan\n      Asia\n      1992\n      41.674\n      16317921\n      649.341395\n      40.920442\n      0.753558\n    \n    \n      9\n      Afghanistan\n      Asia\n      1997\n      41.763\n      22227415\n      635.341351\n      42.297085\n      -0.534085\n    \n    \n      10\n      Afghanistan\n      Asia\n      2002\n      42.129\n      25268405\n      726.734055\n      43.673728\n      -1.544728\n    \n    \n      11\n      Afghanistan\n      Asia\n      2007\n      43.828\n      31889923\n      974.580338\n      45.050372\n      -1.222372\n    \n  \n\n\n\n\nNow we are ready to iterate over the list-column of models. Again, we leverage list-comprehensions. For tidy() and glance() these comprehension are straightforward, but for augment(), which will consume elements from two columns at once, we will need to do something a little fancier. In R we could use purrr::map2() or purrr::pmap(), but the Pythonic idiom here is to use zip() together with tuple unpacking.\n\nmodels['tidied'] = [tidy(fit) for fit in models.fit]\nmodels['glanced'] = [glance(fit) for fit in models.fit]\n\nmodels['augmented'] = [\n    augment(fit, data)\n    for fit, data in zip(models.fit, models.data)\n]\n\nNote for R users: In R the calls to tidy(), etc, would typically live inside a mutate() call. The pandas equivalent is assign, but pandas doesn’t leverage non-standard evaluation and I typically don’t use assign() unless I really want to leverage method chaining for some reason. Normally I save method chaining for data manipulation once I have a flat dataframe, and otherwise I operate entirely via list comprehensions.\nAnyway, the print method garbles the hell out of our results but whatever.\n\nmodels\n\n\n\n\n  \n    \n      \n      data\n      fit\n      tidied\n      glanced\n      augmented\n    \n  \n  \n    \n      Afghanistan\n      country continent  year  lifeExp      ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      aic        bic         ess  centered_t...\n      country continent  year  lifeExp      ...\n    \n    \n      Albania\n      country continent  year  lifeExp      pop ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      aic        bic         ess  centered_...\n      country continent  year  lifeExp      pop ...\n    \n    \n      Algeria\n      country continent  year  lifeExp       pop...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      aic       bic          ess  centered_...\n      country continent  year  lifeExp       pop...\n    \n    \n      Angola\n      country continent  year  lifeExp       pop ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      aic       bic         ess  centered_t...\n      country continent  year  lifeExp       pop ...\n    \n    \n      Argentina\n      country continent  year  lifeExp       p...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate   std_err  statistic ...\n      aic       bic         ess  centered_ts...\n      country continent  year  lifeExp       p...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Vietnam\n      country continent  year  lifeExp       p...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      aic        bic          ess  centered...\n      country continent  year  lifeExp       p...\n    \n    \n      West Bank and Gaza\n      country continent  year  life...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      aic       bic          ess  centered_...\n      country continent  year  life...\n    \n    \n      Yemen, Rep.\n      country continent  year  lifeExp    ...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      aic        bic          ess  centered...\n      country continent  year  lifeExp    ...\n    \n    \n      Zambia\n      country continent  year  lifeExp       po...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate     std_err  statisti...\n      aic        bic        ess  centered_t...\n      country continent  year  lifeExp       po...\n    \n    \n      Zimbabwe\n      country continent  year  lifeExp       ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate     std_err  statisti...\n      aic       bic        ess  centered_ts...\n      country continent  year  lifeExp       ...\n    \n  \n\n142 rows × 5 columns"
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#flattening-the-dataframe",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#flattening-the-dataframe",
    "title": "many models workflows in python i",
    "section": "Flattening the dataframe",
    "text": "Flattening the dataframe\nOur final step before we can recreate Hadley’s plots is to flatten or “unnest” the dataframe. pandas has no unnest method, but the following has served me well so far to unnest a single column. This will not play well with dataframes with MultiIndexes, which I recommend avoiding.\n\ndef unnest(df, value_col):\n    lst = df[value_col].tolist()\n    unnested = pd.concat(lst, keys=df.index)\n    unnested.index = unnested.index.droplevel(-1)\n    return df.join(unnested).drop(columns=value_col)\n\nIf we unnest just the glance() results we get:\n\nglance_results = unnest(models, 'glanced')\n\n# equivalently\nglance_results = (\n    models\n    .pipe(unnest, 'glanced')\n    \n    # little bit of extra cleanup\n    .reset_index()\n    .rename(columns={'index': 'country'})\n)\n\nglance_results\n\n\n\n\n  \n    \n      \n      country\n      data\n      fit\n      tidied\n      augmented\n      aic\n      bic\n      ess\n      centered_tss\n      fvalue\n      f_pvalue\n      nobs\n      rsquared\n      rsquared_adj\n    \n  \n  \n    \n      0\n      Afghanistan\n      country continent  year  lifeExp      ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      country continent  year  lifeExp      ...\n      40.693870\n      41.663683\n      271.006011\n      285.958116\n      181.249410\n      9.835213e-08\n      12.0\n      0.947712\n      0.942483\n    \n    \n      1\n      Albania\n      country continent  year  lifeExp      pop ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      country continent  year  lifeExp      pop ...\n      52.298071\n      53.267884\n      400.445959\n      439.771289\n      101.829014\n      1.462763e-06\n      12.0\n      0.910578\n      0.901636\n    \n    \n      2\n      Algeria\n      country continent  year  lifeExp       pop...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      country continent  year  lifeExp       pop...\n      42.584427\n      43.554240\n      1158.583855\n      1176.087314\n      661.917086\n      1.808143e-10\n      12.0\n      0.985117\n      0.983629\n    \n    \n      3\n      Angola\n      country continent  year  lifeExp       pop ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate    std_err  statistic...\n      country continent  year  lifeExp       pop ...\n      44.061857\n      45.031670\n      156.667858\n      176.464605\n      79.138182\n      4.593498e-06\n      12.0\n      0.887815\n      0.876596\n    \n    \n      4\n      Argentina\n      country continent  year  lifeExp       p...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate   std_err  statistic ...\n      country continent  year  lifeExp       p...\n      6.347866\n      7.317679\n      191.937384\n      192.791819\n      2246.366349\n      4.215567e-13\n      12.0\n      0.995568\n      0.995125\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      137\n      Vietnam\n      country continent  year  lifeExp       p...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      country continent  year  lifeExp       p...\n      42.414079\n      43.383892\n      1612.565329\n      1629.822069\n      934.455357\n      3.289412e-11\n      12.0\n      0.989412\n      0.988353\n    \n    \n      138\n      West Bank and Gaza\n      country continent  year  life...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      country continent  year  life...\n      52.287427\n      53.257240\n      1291.726331\n      1331.016795\n      328.763323\n      5.585089e-09\n      12.0\n      0.970481\n      0.967529\n    \n    \n      139\n      Yemen, Rep.\n      country continent  year  lifeExp    ...\n      <statsmodels.regression.linear_model.Regressio...\n      term     estimate    std_err  statisti...\n      country continent  year  lifeExp    ...\n      46.932773\n      47.902586\n      1310.527555\n      1335.675109\n      521.135193\n      5.868274e-10\n      12.0\n      0.981172\n      0.979290\n    \n    \n      140\n      Zambia\n      country continent  year  lifeExp       po...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate     std_err  statisti...\n      country continent  year  lifeExp       po...\n      72.117172\n      73.086985\n      13.053046\n      218.145441\n      0.636447\n      4.435318e-01\n      12.0\n      0.059836\n      -0.034180\n    \n    \n      141\n      Zimbabwe\n      country continent  year  lifeExp       ...\n      <statsmodels.regression.linear_model.Regressio...\n      term    estimate     std_err  statisti...\n      country continent  year  lifeExp       ...\n      83.262706\n      84.232520\n      30.934127\n      550.116442\n      0.595824\n      4.580290e-01\n      12.0\n      0.056232\n      -0.038145\n    \n  \n\n142 rows × 14 columns\n\n\n\nNow we could ask a question like “what countries seem to have the most linear trends in life expectancy?” and use R-squared as a measure of this.\n\np = (\n    sns.FacetGrid(\n        data=glance_results.sort_values('rsquared'),\n        height=5,\n        aspect=16/9\n    )\n    .map(plt.scatter, 'rsquared', 'country')\n)\nplt.show()\n\n\n\n\nOkay so this plot is awful but I don’t have the patience at the moment to make it better. We could also look at residuals for individual fits to inspect them for any patterns that might indicate systematic error.\n\naugment_results = unnest(models, 'augmented')\n\np = (\n    sns.FacetGrid(\n        data=augment_results,\n        col='continent', \n        col_wrap=2,\n        hue='continent',\n        height=4, aspect=16/13)\n    .map(plt.plot, 'year', 'resid')\n)\nplt.show()\n\n\n\n\nIt would be nice to add smooths by continent as Hadley does but again I don’t have the patience or masochistic urge to figure out how to do that. In any case, there are some clear trends in the residuals, especially for Africa, which suggest that some further modeling is a good idea."
  },
  {
    "objectID": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#the-end",
    "href": "post/2020-08-25_many-models-workflows-in-python-part-i/index.html#the-end",
    "title": "many models workflows in python i",
    "section": "The end",
    "text": "The end\nSo that’s the basic idea behind the many models workflow. Note that we’ve been working at a fairly low-level of abstraction. This means you have a lot of control over what happens (good for research and EDA), but have to write a lot of code. If you’re just fitting prediction models and the only thing you want to do is compare risk estimates, you can save time and effort by using sklearn’s GridSearchCV object.\nOne final note: in Hadley’s gapminder example we iterate over disjoint data sets. In practice I do this extremely rarely. Much more often I find myself iterating over (train, test) pairs, or hyperparameters, or both at once. This hyperparameter optimization over many CV-folds workflow is more complex than the simple example here, but still fits nicely into the many models workflow I’ve described here. I’ll demonstrate how to do that in a followup post."
  },
  {
    "objectID": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html",
    "href": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html",
    "title": "many models workflows in python ii",
    "section": "",
    "text": "In this followup to my earlier post on modeling workflows in Python, I demonstrate how to integrate sample splitting, parallel processing, exception handling and caching into many-models workflows. I also discuss some differences between exploration/inference-centric workflows and tuning-centric workflows."
  },
  {
    "objectID": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html#motivating-example",
    "href": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html#motivating-example",
    "title": "many models workflows in python ii",
    "section": "Motivating example",
    "text": "Motivating example\nWe will work with the Palmer Penguin dataset, which contains various biological measures on three species of penguins. Our goal will be to cluster the penguins into groups that correspond to their species using their bill length, bill depth, flipper length and body mass. We’ll use a Gaussian Mixture Model for clustering. Caveat: my goal here is to demonstrate a workflow, not good science.\nWe’ll start by pulling the data into Python and taking a look.\n\nimport pandas as pd\n\npenguins = pd.read_csv('https://tinyurl.com/palmerpenguincsv').dropna()\n\nclustering_features = [\n    'bill_length_mm',\n    'bill_depth_mm',\n    'flipper_length_mm',\n    'body_mass_g'\n]\n\npenguins\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n    \n      5\n      Adelie\n      Torgersen\n      39.3\n      20.6\n      190.0\n      3650.0\n      male\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      339\n      Chinstrap\n      Dream\n      55.8\n      19.8\n      207.0\n      4000.0\n      male\n      2009\n    \n    \n      340\n      Chinstrap\n      Dream\n      43.5\n      18.1\n      202.0\n      3400.0\n      female\n      2009\n    \n    \n      341\n      Chinstrap\n      Dream\n      49.6\n      18.2\n      193.0\n      3775.0\n      male\n      2009\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n    \n  \n\n333 rows × 8 columns\n\n\n\nOur next step is to define a helper function to fit GMM estimators for us. I’m going to use sklearn’s GMM implementation, which uses the EM algorithm for estimation. The estimator has two key hyperparameters that we’ll want to explore:\n\nn_components: the number of clusters to find\ncovariance_type: the assumed structure of the covariances of each cluster\n\nNothing in the code turns on these hyperparameters dramatically, but it probably worth reading the documentation linked above if haven’t seen GMMs before.\n\nfrom typing import List\nfrom sklearn.mixture import GaussianMixture\n\n# some comments:\n# \n# 1. I highly recommend using type hints, since it'll help you keep\n#   keep track of what's happening in each column of the eventual\n#   data frame of models\n#\n# 2. Passing hyperparameters as keyword arguments is a generally usefu\n#   pattern that make it easy to swap out estimators without having\n#   to modify a lot of code that relies on estimators having\n#   particular hyperparameters.\n#\n# 3. Here I specify the columns of the data to use for modeling by\n#   passing a list of column names. This is extremely limited and\n#   will result in technical debt if you need to do any sort of \n#   interesting work to build a design matrix. Alternatives here\n#   might be a list of sklearn Transformers or a Patsy formula.\n#   In R you would use a formula object or perhaps a recipe\n#   from the recipes packages.\n#\n\ndef fit_gmm(\n    train : pd.DataFrame,\n    features : List[chr],\n    **hyperparameters\n) -> GaussianMixture:\n    \n    # the hyperparameters are: n_components: int, covariance_type: str\n    gmm = GaussianMixture(\n        **hyperparameters,\n        n_init=5,\n        random_state=27\n    )\n    \n    gmm.fit(train[features])\n    return gmm\n\n# sanity check\nfit_gmm(\n    penguins,\n    features=clustering_features,\n    n_components=3,\n    covariance_type='full'\n)\n\nGaussianMixture(n_components=3, n_init=5, random_state=27)\n\n\nNow let’s build some infrastructure so that we can use sample splitting to evaluate how well our models perform out of sample. My approach below is heavily inspired by rsample. At the end of this post I have some longer commentary about this choice and why I haven’t used sklearn built-in resampling infrastructure.\n\nimport numpy as np\n\nclass VFoldCV:\n    \n    def __init__(self, data : pd.DataFrame, num_folds : int = 10):\n        \n        self.data = data\n        self.num_folds = num_folds\n        permuted_indices = np.random.permutation(len(data))\n        self.indices = np.array_split(permuted_indices, num_folds)\n        \n    def __iter__(self):\n        for test_indices in self.indices:\n            test = self.data.iloc[test_indices]\n            train = self.data[~self.data.index.isin(test_indices)]\n            yield train, test\n\nresamples = VFoldCV(penguins, num_folds=5)\n\nfor fold_index, (train, test) in enumerate(resamples):\n    print(f\"\"\"\n    In fold {fold_index} there are {len(train)}\n    training observations and {len(test)} test observations\n    \"\"\")\n\n\n    In fold 0 there are 267\n    training observations and 67 test observations\n    \n\n    In fold 1 there are 269\n    training observations and 67 test observations\n    \n\n    In fold 2 there are 267\n    training observations and 67 test observations\n    \n\n    In fold 3 there are 271\n    training observations and 66 test observations\n    \n\n    In fold 4 there are 269\n    training observations and 66 test observations\n    \n\n\nNow that we have cross validation folds, we want to create a grid of parameters to explore over. Here we leverage sklearn’s ParameterGrid object, which basically takes a Cartesian product that we can iterate over, as well as coerce to a data frame.\n\nfrom sklearn.model_selection import ParameterGrid\n\nparam_grid = ParameterGrid({\n    'n_components': range(2, 11),\n    'covariance_type': ['full', 'tied', 'diag', 'spherical']\n})\n\nmodel_grid = pd.DataFrame(param_grid)\nmodel_grid.head()\n\n\n\n\n  \n    \n      \n      covariance_type\n      n_components\n    \n  \n  \n    \n      0\n      full\n      2\n    \n    \n      1\n      full\n      3\n    \n    \n      2\n      full\n      4\n    \n    \n      3\n      full\n      5\n    \n    \n      4\n      full\n      6\n    \n  \n\n\n\n\nNow we are going to do something a little tricky. We are going to create a new column of model_grid that contains the results from fitting a GMM to each split from the CV object. In particular, each element in this column will itself be a list with num_folds elements. That is, we will create a list-column cv_fits were each element of model_grid.cv_fits is itself a list. To work with these structures it is easiest to define a helper function that fits a GMM to each CV split for a single combination of hyperparameters.\n\ndef fit_gmm_vfold(\n    folds: VFoldCV, \n    **hyperparameters\n) -> List[GaussianMixture]:\n    \n    fits = [\n        fit_gmm(train, **hyperparameters)\n        for train, test in folds \n    ]\n    return fits\n\nmodel_grid['cv_fits'] = [\n    fit_gmm_vfold(\n        resamples,\n        features=clustering_features,\n        **hyperparameters\n    )\n    for hyperparameters in param_grid\n]\n\nmodel_grid.head()\n\n\n\n\n  \n    \n      \n      covariance_type\n      n_components\n      cv_fits\n    \n  \n  \n    \n      0\n      full\n      2\n      [GaussianMixture(n_components=2, n_init=5, ran...\n    \n    \n      1\n      full\n      3\n      [GaussianMixture(n_components=3, n_init=5, ran...\n    \n    \n      2\n      full\n      4\n      [GaussianMixture(n_components=4, n_init=5, ran...\n    \n    \n      3\n      full\n      5\n      [GaussianMixture(n_components=5, n_init=5, ran...\n    \n    \n      4\n      full\n      6\n      [GaussianMixture(n_components=6, n_init=5, ran...\n    \n  \n\n\n\n\nNote: To get our cross validated fits, we iterated over param_grid and stored results in model_grid. It’s essential that the row indices match up between these objects. Here they match by construction, but be careful if you start to play with this structure. For example, one alternative (and frequently convenient approach) here is to create a model_grid object based on itertools.product(param_grid, resamples) rather than just param_grid. This avoids nesting lists in list-columns, at the cost of inefficiency in terms of storage. This route is more fiddly than it looks.\nAnyway, now that we have training fits, we want to compute out of sample performance estimates. In our case, we’ll use a measure of clustering quality known at the Adjusted Rand Score. Again we use nested list comprehensions to get out of sample estimates for all CV splits.\n\nfrom sklearn.metrics import adjusted_rand_score\n\ndef oos_vfold_ars(\n    folds: VFoldCV, \n    fits: List[GaussianMixture],\n    features : List[chr]\n) -> List[float]:\n    \n    ars = [\n        adjusted_rand_score(\n            test.species.astype('category').values.codes,\n            fit.predict(test[features])\n        ) for (_, test), fit in zip(folds, fits)\n    ]\n    \n    return ars\n\nmodel_grid['cv_ars'] = [\n    oos_vfold_ars(\n        resamples, \n        fits, \n        features=clustering_features\n    )\n    for fits in model_grid.cv_fits\n]\n\nmodel_grid.head()\n\n\n\n\n  \n    \n      \n      covariance_type\n      n_components\n      cv_fits\n      cv_ars\n    \n  \n  \n    \n      0\n      full\n      2\n      [GaussianMixture(n_components=2, n_init=5, ran...\n      [0.7494757446724656, 0.6601979361110133, 0.629...\n    \n    \n      1\n      full\n      3\n      [GaussianMixture(n_components=3, n_init=5, ran...\n      [1.0, 0.9183802977833249, 0.9571986362701186, ...\n    \n    \n      2\n      full\n      4\n      [GaussianMixture(n_components=4, n_init=5, ran...\n      [0.835783421282886, 0.6891338866090895, 0.8788...\n    \n    \n      3\n      full\n      5\n      [GaussianMixture(n_components=5, n_init=5, ran...\n      [0.8662259741689476, 0.6533338920699733, 0.521...\n    \n    \n      4\n      full\n      6\n      [GaussianMixture(n_components=6, n_init=5, ran...\n      [0.5695333527048168, 0.8213228325631625, 0.549...\n    \n  \n\n\n\n\nNow we can compare models by expanding the cv_ars column and comparing out of sample performance measures. Here I’m just going to visualize the results, but you could also fit a mixed model of the form adjusted_rand_index ~ hyperparameter_combination + (1|cv_fold) or something along those lines if you want to be fancier.\nIt’s worth pausing a moment to comment on the cv_ars column. In my previous post, I introduced a helper function unnest() that you can use to expand a list-column that contains data frames. That unnest() function does not work with list-columns of lists, and instead we use the pandas.Series.explode() method, which is like an extremely limited version of tidyr::unnest(). Importantly, pandas.Series.explode() is not very clever about types, so you may need to coerce types after expanding list columns, as I do below.\n\ncross_validated_ars = (\n    model_grid\n    .explode('cv_ars')\n)\n\nprint(cross_validated_ars.dtypes, '\\n')\n\ncross_validated_ars[\"cv_ars\"] = pd.to_numeric(cross_validated_ars[\"cv_ars\"])\n\nprint(cross_validated_ars.dtypes)\n\ncovariance_type    object\nn_components        int64\ncv_fits            object\ncv_ars             object\ndtype: object \n\ncovariance_type     object\nn_components         int64\ncv_fits             object\ncv_ars             float64\ndtype: object\n\n\nNow that we have numeric data we can plot it.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplot = sns.lineplot(\n    data=cross_validated_ars.reset_index(),\n    x='n_components', y='cv_ars',\n    hue='covariance_type', ci=None\n)\n\nplt.show()\n\n\n\n\nHigher Adjusted Rand Scores are better, so this initial pass of modeling suggests that we should use three clusters. This is reassuring, since we know there are three species of penguin in the dataset!\nNote that I’m taking a very informal approach to model selection here and a real analysis should be more careful (in slightly more detail: the difference between CV for model selection and CV for risk estimation is germane here).\nThere are other approaches to model selection that we could take. For example, we could compare across hyperparameters with in-sample BIC, which is the approach taken in the R package mclust. We’ll do this briefly for illustrative purposes, and start incorporating some fancier tools along the way:\n\nParallelization (via joblib): Modeling fitting across hyperparameter is embarassingly parallel, so this will make things faster.\nException handling (via a function decorator): In practice, lots of estimators will run into numerical or data issues that you can safely ignore. In particular, when model fitting fails, it’s okay fine to just return a np.nan (in R, an NA-like object) and use the results from whatever estimators did succeed, propagating the np.nan forward.\nCaching (via joblib): In interactive workflows it’s easy to kill your Jupyter kernel or crash your computer or do something dumb that forces you to restart your Python session. It is really frustrating to have to refit your models everytime this happens. Caching models to disk (also known as memoization) prevents you from having to re-run all your models everytime you break Jupyter, etc, etc. If you use caching, you will have to be careful about cache invalidation, which is one of the most notoriously difficult problems in computing.\n\nFull disclosure: I did not get parallelization and caching to work together for this post (bug report), but it has worked for me in the past. I’m going to include some commented out caching code in the following for you to play with as you see fit. See the joblib documentation for more examples of how to combine parallel mapping with caching.\n\nfrom joblib import Parallel, delayed  # parallelism\nfrom functools import wraps           # nicer exception handling\nfrom joblib import Memory             # caching\n\n# setup caching\ncache_dir = \"./model_cache\"\nmemory = Memory(cache_dir, verbose=0)\n\n# NOTE: here `n_components` will exceed the number\n# of observations for some hyperparameter combinations\n# which will cause errors. here i'm artificially introducing\n# errors; in real life you'll have to supply your own\n\nfancy_param_grid = ParameterGrid({\n    'n_components': range(2, 400),\n    'covariance_type': ['full', 'tied', 'diag', 'spherical']\n})\n\nfancy_model_grid = pd.DataFrame(fancy_param_grid)\nfancy_model_grid.tail()\n\n\n\n\n  \n    \n      \n      covariance_type\n      n_components\n    \n  \n  \n    \n      1587\n      spherical\n      395\n    \n    \n      1588\n      spherical\n      396\n    \n    \n      1589\n      spherical\n      397\n    \n    \n      1590\n      spherical\n      398\n    \n    \n      1591\n      spherical\n      399\n    \n  \n\n\n\n\nNow we define our function decorator for handling exceptions in list comprehensions. I’m basically copying purrr::safely() from R here. Exception handling is essential because it’s painful when you fit 9 out of 10 models using a list-comprehension, and then the final model fails and you have to re-run the first 9 models (caching also helps with this when you can get it to work).\n\ndef safely(func, otherwise=np.nan):\n    # traditionally you'd use @wraps here,\n    # but this seems to interact poorly with parallelism\n    # for the time being\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except:\n            return otherwise\n    return wrapper\n\n# if bad models become np.nan, downstream calls that use models\n# need to handle np.nan input. any easy way to do this is use\n# @safely again to just continue propagate np.nan\n\n@safely\ndef get_bic(fit):\n    return fit.bic(penguins[clustering_features])\n    \nfit_safely = safely(fit_gmm)\n\n# to combine parallelism and caching, use something like:\n# fit_cached = memory.cache(fit_safely)\n\n# create persistent pool of workers and use them for fitting\nwith Parallel(n_jobs=20) as parallel:\n    \n    fancy_model_grid['fit'] = parallel(\n        delayed(fit_safely)(penguins, clustering_features, **hyperparameters)\n        for hyperparameters in fancy_param_grid\n    )\n    \n    fancy_model_grid['bic'] = parallel(\n        delayed(get_bic)(fit) for fit in fancy_model_grid.fit\n    )\n\nNow we can compare BIC across our absurdly large number of models. seaborn will automatically drop np.nan() values, so our @safely approach plays very nice here.\n\nplot = sns.lineplot(\n    data=fancy_model_grid,\n    x='n_components', y='bic', hue='covariance_type'\n)\n\nplt.show()\n\n\n\n\nIn this plot lower BIC is better. We see than this in-sample approach to model selection prefers models with as many clusters as observations. This would clearly correspond to overfitting in our case."
  },
  {
    "objectID": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html#returning-to-the-big-picture",
    "href": "post/2021-03-28_many-models-workflows-in-python-part-ii/index.html#returning-to-the-big-picture",
    "title": "many models workflows in python ii",
    "section": "Returning to the big picture",
    "text": "Returning to the big picture\nAt this point, you might be starting to wonderful why you would want to use a many-models workflow at all. All I’ve done in this blog post is some grid searching over hyperparameters, and we could easily recreate everything in this blog post with GridSearchCV and some custom scoring functions.\nThe problem is that GridSeachCV (and other related implementations) are not that flexible. Last summer, I had (1) custom data splitting, (2) custom estimators, and (3) wanted to compute high dimensional summaries for each model I fit. And I want to save my fits so that I could investigate them individually, rather than throwing them out as soon as I know about their predictive performance. GridSearchCV just can’t handle this, and, by and large, the data science ecosystem in Python doesn’t either.\nWe can imagine that there are two contrasting modeling workflows. First, there’s a many-models workflow, which is especially appropriate for research, inference and sensitivity analysis. It’s interactive, and not particularly focused on compute efficiency. Then, there’s a hyperparameter tuning workflow, which has a simple goal: predict well. Tools for tuning workflows are typically developed by machine learners who want to train models as computationally efficiently as possible. Because these practitioners emphasize prediction accuracy over all else, it can be hard to re-purpose tools for tuning workflows to learn about models beyond their predictive accuracy1.\nHopefully this post highlights some design patterns you can use when existing infrastructure isn’t a good fit for your Python modeling needs. I’m very curious to hear about other approaches people take this problem."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "a tidymodels workflow in python using list columns in pandas dataframes, now with hyperparameter sweep and parallelism\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\n\na tidymodels workflow in python using list columns in pandas dataframes\n\n\n\n\n\n\n\n\n\nAug 25, 2020\n\n\n13 min\n\n\n\n\n\n\n\n\n\nmy best attempt at explaining why frequentism is about sampling procedures\n\n\n\n\n\n\n\n\n\nMay 4, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\n\na terse analysis of tesla stock prices and how one of elon’s tweets moves them\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\na likelihood ratio test to check if transforming your data leads to better model fit\n\n\n\n\n\n\n\n\n\nMar 22, 2020\n\n\n15 min\n\n\n\n\n\n\n\n\n\nfleshing out intuition about structure in random processes beyond the standard bias-variance decomposition\n\n\n\n\n\n\n\n\n\nJan 6, 2020\n\n\n19 min\n\n\n\n\n\n\n\n\n\nan explainer about ordinary least squares regression and when it is an acceptable estimator\n\n\n\n\n\n\n\n\n\nAug 31, 2019\n\n\n17 min\n\n\n\n\n\n\n\n\n\nsome pointers to papers that were helpful when i got started in spectral network analysis, a woefully incomplete list\n\n\n\n\n\n\n\n\n\nJul 26, 2019\n\n\n12 min\n\n\n\n\n\n\n\n\n\nan exploration of what it would take to meaningfully probe the correctness of computations in modeling software\n\n\n\n\n\n\n\n\n\nJun 7, 2019\n\n\n16 min\n\n\n\n\n\n\n\n\n\nmy thesis about why modeling software is an enormous mess\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\n21 min\n\n\n\n\n\n\n\n\n\na demonstration of low levels tidymodels infrastructure to build sophisticated tools in a hurry\n\n\n\n\n\n\n\n\n\nApr 13, 2019\n\n\n13 min\n\n\n\n\n\n\n\n\n\nsome math to show that confidence interals are significantly different parameters can overlap\n\n\n\n\n\n\n\n\n\nJan 31, 2019\n\n\n8 min\n\n\n\n\n\n\n\n\n\nwhat i wish my mother had told me about sampling from posteriors\n\n\n\n\n\n\n\n\n\nDec 24, 2018\n\n\n23 min\n\n\n\n\n\n\n\n\n\nsome intuition for multinomial regression’s initially intimidating functional form\n\n\n\n\n\n\n\n\n\nOct 23, 2018\n\n\n6 min\n\n\n\n\n\n\n\n\n\nreflections on a great internship\n\n\n\n\n\n\n\n\n\nAug 10, 2018\n\n\n9 min\n\n\n\n\n\n\n\n\n\na demonstration of how to profile r code on a toy problem\n\n\n\n\n\n\n\n\n\nJun 15, 2018\n\n\n18 min\n\n\n\n\n\n\n\n\n\na quick analysis of my running fitness using splines\n\n\n\n\n\n\n\n\n\nMay 16, 2018\n\n\n10 min\n\n\n\n\n\n\n\n\n\nresampling based approaches to estimating the risk of a predictive model\n\n\n\n\n\n\n\n\n\nMay 3, 2018\n\n\n12 min\n\n\n\n\n\n\n\n\n\nhow to use a computer to check your derivative calculations\n\n\n\n\n\n\n\n\n\nOct 18, 2017\n\n\n3 min\n\n\n\n\n\n\n\n\n\ncopy-pasteable example code for programming with the tidyverse.\n\n\n\n\n\n\n\n\n\nAug 7, 2017\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "talks",
    "section": "",
    "text": "None"
  },
  {
    "objectID": "talks/index.html#past-talks",
    "href": "talks/index.html#past-talks",
    "title": "talks",
    "section": "past talks",
    "text": "past talks\nThe Low Hanging Fruit of the Twitter Following Graph\n2021-08-11, Joint Statistical Meetings\nIn recent applied work on the Twitter media ecosystem, we have found that Twitter metadata (such as follows, likes, quotes, retweets, mentions, etc) is often more informative than the actual content of tweets themselves. The metadata, in some sense, is the right data to use for many inference tasks. In particular, we find that embedding the Twitter following graph is highly informative. However, collecting the following graph is rather challenging due to API rate limits, and storing graphs can also be challenging. We present some computational infrastructure to make access and storage of this high signal data more straightforward, and suggest that research progress would be well served by an increased focus on instrumentation. slides\nSolving the model representation problem with broom\n2019-01-25, rstudio::conf(2019)\nThe R objects used to represent model fits are notoriously inconsistent, making data analysis inconvenient and frustrating. The broom package resolves this issue by defining a consistent way to represent model fits. By summarizing essential information about fits in tidy tibbles, broom makes it easy to programmatically work with model objects. Combining broom with list-columns results in an especially powerful way to work with many model fits at once. This talk will feature several case studies demonstrating how broom resolves common problems in data analysis. video, slides\nSolving the model representation problem with broom\n2018-09-19, Statistics Graduate Student Seminar\nslides\nConvenient data analysis with broom\n2018-11-30, RStudio Webinar Series\nBroom is a package that converts statistical objects into tibbles. This consistent structure makes it easier to accomplish many standard modelling tasks. In this webinar I’ll demonstrate how to use to broom to work with many models at once. We’ll see how broom makes it easier to visualize models, work with bootstrapped fits and assess model diagnostics. video, slides\nSolving the model representation problem with broom\n2018-09-19, Madison R User Group\nslides"
  },
  {
    "objectID": "talks/index.html#slides-from-various-informal-presentations",
    "href": "talks/index.html#slides-from-various-informal-presentations",
    "title": "talks",
    "section": "slides from various informal presentations",
    "text": "slides from various informal presentations\nIdentifiability of homophily and contagion in social networks\n2022-02-23, Madison Networks Reading Group\nslides\nTriangles & networks\n2021-02-17, STAT 992 Seminar on Tensors\nslides\nA new way to think about citations\n2020-11-17, Rohe Lab Group Meeting\nslides\nThe linear probability model\n2019-11-19, STAT 992 Seminar Course presentation\nslides\nrstudio internship progress update\n2018-07-23, RStudio tidyverse team\nslides\nLocally Interpretable Model-Agnostic Explanations\n2018-03-29, Rice DataSci club\nslides\nYour First R Package\n2018-02-22, Rice DataSci club\nslides\nCOMP 540 Prediction Project Advice\n2018-03-09, Rice University\nslides\nSuper Learner\n2018-01-23, Allen Lab Group Journal Club\nslides\nLast updated on 2022-04-27."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "teaching",
    "section": "",
    "text": "None."
  },
  {
    "objectID": "teaching/index.html#past-courses",
    "href": "teaching/index.html#past-courses",
    "title": "teaching",
    "section": "past courses",
    "text": "past courses\nBased on my teaching in the 2018-2019 academic year I received a Statistics Department Outstanding TA award.\nGraduate Teaching Assistant\nSpring 2019, STAT 324 Intro to Statistics for Engineers, UW-Madison\nstudent eval comments, student eval ratings\nApplied Machine Learning Workshop\nJanuary 15-16, 2019, rstudio::conf(2019)\n(co-taught with Max Kuhn and Davis Vaughn)\nmaterials\nGraduate Teaching Assistant\nFall 2018, STAT 324 Intro to Statistics for Engineers, UW-Madison\nstudent eval comments, student eval ratings\nUndergraduate Teaching Assistant\nSpring 2018, COMP 540 Statistical Machine Learning, Rice University\nUndergraduate Teaching Assistant\nFall 2017, COMP 330 Data Science: Tools & Models, Rice University\nI enjoy teaching a fair amount but have not taught since my first year of graduate school due to funding that has allowed me to focus on research.\nOutside of academic settings, I used to devote a fair amount of time to teaching R workshops for the Rice DataSci club, and to helping people make their first open source contributions, primarily to the broom package."
  },
  {
    "objectID": "teaching/index.html#teaching-material",
    "href": "teaching/index.html#teaching-material",
    "title": "teaching",
    "section": "teaching material",
    "text": "teaching material\nIf you are teaching an introductory statistics class in the classic “barrage of hypothesis tests” tradition, you may find my cheatsheet on common hypothesis tests useful. I also wrote distributions3 to be an approachable set of tools for manipulating probability distributions for intro stat students. The package vignettes are designed to walk students intro stat courses though many classic hypothesis tests as well."
  },
  {
    "objectID": "teaching/index.html#mentoring",
    "href": "teaching/index.html#mentoring",
    "title": "teaching",
    "section": "mentoring",
    "text": "mentoring\n\nNathan Kolbow worked with me as a undergraduate RA to develop the neocache data collection infrastructure. He is now a PhD student in Biostatistics at UW-Madison.\n\nLast updated 2022-04-28."
  }
]