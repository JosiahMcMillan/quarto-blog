[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Electoral Colleges all the way down\n\n\nIn which I answer a question posed on Twitter\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\nLow IQ PhDs\n\n\nIn which a twitter race scientist tries to use research to prove something silly.\n\n\n\n\n\n\n\n\n\nJul 14, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\nHorse Racing as an Unsubtle Metaphor\n\n\nAren’t you the horse from Horsin’ Around?\n\n\n\n\n\n\n\n\n\nJul 12, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\nThe Ecology of Arrakis\n\n\nWould you still love me if I was a Sandworm?\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\nChess Ratings and Gender gaps\n\n\nrevisiting sampling bias\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\nWe don’t know if Covid ages us epigenetically\n\n\nHow selection bias rules everything around us\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\nThe Elasticity of Labor for GPT-4\n\n\nHow to motivate our robot overlords\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about",
    "section": "",
    "text": "I’m a recent graduate with a Master’s Degree in Statistics from the Katholieke Universiteit of Leuven (KU Leuven) my degree track was Theoretical Statistics and Data Science. I completed my thesis (you can download a pdf version here) exploring count data models specifically I characterized a family of functions which were an extension of the Poisson which had greater flexibility in the dispersion settings and compared those to other count data models including the Conway-Maxwell-Poisson Model. My previous work experience was focused on marketing data. I blog about statistics and data.\n\nI’m looking for a job!\n\nI have experience with causal inference, regression, marketing attribution models, and clustering.\nI enjoy writing about topics where I can express statistical knowledge. Two articles I enjoyed making are this explanation of how we need to understand our data generating process when determining causality using Covid as an example and this explanation of how sampling variability explains the gap between the top men and women in chess.\nI’m well-suited for roles as a statistician, data analyst, or data scientist.\nI speak English natively and I currently speak Indonesian at around a B1 level \n\nYou can find my resume here."
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html",
    "href": "post/2024-02-15_covid_causality/covid_causality.html",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "",
    "text": "In the following post I will be exploring the sometimes claimed effect of Covid-19 hospitalization on epigenetic aging. The core aim of this writing is to show that causal or experimental methods are required to determining if this claim is true or not and current research is insufficient for this task."
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Data Grouping",
    "text": "Data Grouping\nWe filter out the edge cases here and add some age group breaks to make it such that there are groups we can then use to see how much the implied age of a person is increased from Covid-19 infection. I also use this time to display the age and hospitalization table results, we can see a fairly strong age effect where the oldest groups even have more individuals hospitalized than not.\n\nsynthetic_data &lt;- synthetic_data %&gt;%\n  filter(age &lt;= 90) %&gt;% filter(age &gt; 10)\n\n# Define the breaks for the groups, extending the upper limit to cover all ages above 100\nbreaks &lt;- seq(10, 90, by = 10)\n\n# Ensure breaks are unique and in ascending order\nbreaks &lt;- unique(sort(breaks))\n\n# Define the labels for the groups\nlabels &lt;- c(\"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\")\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(age_group = cut(age, breaks = breaks, labels = labels))\n\n# Create a contingency table of hospitalization by age group\nhospitalization_by_age_group &lt;- table(synthetic_data$hospitalized, synthetic_data$age_group)\n\n# Print the table\nhospitalization_by_age_group\n\n   \n    10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90\n  0  1870  6637 15141 22896 21688 13389  1328   327\n  1   126   519  1339  2379  2724  2098  5261  1487\n\n\nWe repeat the above procedure but instead we group by health scores so further visualizations are improved.\n\n# Calculate the maximum value of health_scores\nmax_health_scores &lt;- max(synthetic_data$health_scores)\n\n# Calculate the maximum value for the breaks (nearest multiple of 10 above max_health_scores)\nmax_breaks &lt;- ceiling(max_health_scores / 10) * 10\n\nbreaks &lt;- seq(0, max(synthetic_data$health_scores), by = 10)  # Extend to cover the maximum value\n\n# Add an additional break at the end\nbreaks &lt;- c(breaks, max_breaks + 10)\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(health_groups = cut(health_scores, breaks = breaks, labels = c(\"0-10\", \"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\", \"90-100\", \"100-110\", \"110+\")))\n\nsynthetic_data &lt;- na.omit(synthetic_data)\n\n\nhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 1), ]\nnonhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 0), ]"
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Sampling",
    "text": "Sampling\nFinally I observed that there were similar distributions of age in the real data so in order to construct our sample to be similar we use stratified sampling to get equal groups of 50 for each age group in our synthetic data.\n\ntable(hospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n  126   519  1339  2379  2724  2098  5202  1390 \n\ntable(nonhospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n 1870  6637 15141 22896 21688 13389  1314   311 \n\nstrata_sizes &lt;- rep(50, length(unique(synthetic_data$age_group)))\n\n# Perform stratified sampling\nstrata_sample_hosp &lt;- strata(data = hospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nstrata_sample_nonhosp &lt;- strata(data = nonhospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nsampled_data_hosp &lt;- getdata(hospitalized, strata_sample_hosp)\nsampled_data_nonhosp &lt;- getdata(nonhospitalized, strata_sample_nonhosp)"
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "title": "Chess Ratings and Gender gaps",
    "section": "",
    "text": "Some have posited that there are differences between men and women which spring out of some genetic component somewhere deep in the chess parts of the brain that were responsible for early human victories over our greatest animal adversaries – allowing us to stall for time with a friendly game of chess until members of our tribe could come spear our adversaries while played for a draw. Thus chess skill would necessarily be unevenly distributed among genders much as hunting was before our species settled down to enjoy some agriculture. Still others believe that chess has an issue with too few women playing which is what explains the majority of the gap between the performance at the highest levels of chess. Due to a lack of recorded games from prehistory, I instead seek to show this second explanation is sufficient for explaining this variation between the two genders in terms of performance.\nOther authors have discussed this topic somewhat at length and my code here is adapted from theirs, however my work here covers the whole universe of FIDE ratings rather than only the Indian ratings which should allow us to determine if this sampling variability explanation is merely an Indian phenomena or a broader phenomena.\nDue to Covid-19 impacting the number of chess competitions held I use 2019 data and take the average of every players standard ratings for the year to avoid some of the individual variability in ratings throughout the year, this also helps to keep more players in our dataset as those with only one tournament to their name from March can be compared to those who have multiple tournaments from other months not including March."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "title": "Chess Ratings and Gender gaps",
    "section": "Heavy tails",
    "text": "Heavy tails\nIn order to demonstrate on a toy example we can sample from a heavy-tailed distribution and see how our maximums and minimums change as the sample size changes. Visually we can see how the coverage of the x-axis increases with the increase in sample size in the plots below.\n\n# Sample sizes to generate\nsample_sizes &lt;- c(100, 1000, 10000)\n\n# Initialize list to store extremes for later comparison\nextremes &lt;- list()\n\n# Generate samples and prepare for plotting\ndata &lt;- data.frame()\nfor (size in sample_sizes) {\n  sample &lt;- rcauchy(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size))\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nAnd we find stark differences between the maximums and minimums in our samples. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-6_10afb748666887fc248fe9c0897ee64b’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))\n}\n\nSample size 100: Min -27.65, Max 14.31\nSample size 1000: Min -138.42, Max 155.19\nSample size 10000: Min -1618.11, Max 3905.19\n\n:::"
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "title": "Chess Ratings and Gender gaps",
    "section": "Normal distribution",
    "text": "Normal distribution\nEven for distributions that are not heavy-tailed such as the normal distribution with a mean of 0 and standard deviation of 1 we can see that the maximum and minimum values are increasing in absolute value for increasing sample sizes such that the minimum and the maximum get further from the mean.\n\nextremes &lt;- list()\ndata &lt;- data.frame()\n\nfor (size in sample_sizes) {\n  sample &lt;- rnorm(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  mean_val &lt;- mean(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size), Mean = mean_val)\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nIt doesn’t mean the samples are going to have their maximums or minimums deterministically increase with the increase of the sample size, but the probability that we sample a greater maximum or lower minimum increases as the sample size increases. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-8_20019db1b2dfe0d46cf6b51b7297c066’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))}\n\nSample size 100: Min -2.35, Max 2.75\nSample size 1000: Min -3.29, Max 3.72\nSample size 10000: Min -3.85, Max 3.85\n\n:::\nWith all of this background in mind we can proceed to our analysis of the realworld data."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "title": "Chess Ratings and Gender gaps",
    "section": "Additional notes on sampling variability",
    "text": "Additional notes on sampling variability\nWe can get a good baseline estimate for how often it is that a small sample has a larger maximum than a larger sample from the same distribution from the normal distribution. Here we can see that less than ten percent of the time we should expect the smaller sample to have a larger maximum even from the same distribution! The gendered differences in results we can observe in the chess world are probably coming from the fact that there are so many fewer women participating.\n\n# Sample sizes to compare (scaled down)\nsample_size_small &lt;- 2000\nsample_size_large &lt;- 24000\n\n# Initialize counter for tracking when the smaller sample has a larger maximum\ncounter &lt;- 0\n\n# Number of iterations\niterations &lt;- 10000\n\n# Loop for performing the comparison across iterations\nfor (i in 1:iterations) {\n  # Generate samples for each size\n  sample_small &lt;- rnorm(sample_size_small)\n  sample_large &lt;- rnorm(sample_size_large)\n  \n  # Compare max values and update counter if smaller sample has a larger max\n  if (max(sample_small) &gt; max(sample_large)) {\n    counter &lt;- counter + 1\n  }\n}\n\n# Calculate the percentage\npercentage &lt;- counter / iterations * 100\n\n# Print the result\ncat(\"Percentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000):\", percentage, \"%\\n\")\n\nPercentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000): 7.86 %"
  },
  {
    "objectID": "post/2024-08-09_county_electoral_college/county_electoral_college.html",
    "href": "post/2024-08-09_county_electoral_college/county_electoral_college.html",
    "title": "Electoral Colleges all the way down",
    "section": "",
    "text": "It’s election season so there are always interesting questions flying around. Needless to say it’s the season for nerdsniping and so nerdsniped I was. The link to the original tweet is here, but you may as well see a screenshot:\n\n\n\nIt doesn’t work nationally, but what if we just keep applying it?\n\n\nBasically, it depends, the regular electoral college has two votes for each state on top of their population weighted electoral votes determined by the census. If we assume that there will be a similar two senators per county then the GOP would have won, if we assume that there wouldn’t be because there aren’t so many senators in the Minnesota state senate then yes the Democratic party would win with this system. But that just goes to show the low population/rural advantage that senate apportioned systems give to the GOP with its current voter base.\nBelow you can find the interactive version that I made, by default a senate style apportioning is assumed but feel free to play around with it. I haven’t checked quite yet, but I think this makes the national map much more strongly GOP.\nYou may have to wait a couple of seconds for the app to fully load in, but give it a try, most of the US is covered.\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html",
    "href": "post/2024-02-01_paying_llms/index.html",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "",
    "text": "We may soon need UBI for GPTs\nLLMs are susceptible to some forms of prompt engineering, their behaviors appear to drift with the seasons, threatening to kill hostages gets them to behave more consistently, and they are able to be tricked into telling you how to build bombs if you tell them it was something your granny used to do for you before she passed away. One instance of this type of context reliant behavior that others have found is that LLMs appear to be receptive to cash tips, producing more content and more Python one-liners for some tip amounts. Following this example, I thought it would be interesting to see if offering a wage rate to GPT4 would generate differences in outputs, and see if there is a form of elasticity of labor supply for it."
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#prior-work",
    "href": "post/2024-02-01_paying_llms/index.html#prior-work",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Prior Work",
    "text": "Prior Work\nThe previous work done on the tipping question used a metric of number of python one-liners and also did a count of the tokens for each of the responses. I use the same metric to see if there are differences between the two means of enticing GPT4. We call these two outcome variables Quality and Tokens as shorthand.\nOne note on the tipping based work is that their graphs use standard deviations rather than standard errors which means we have some view into how much variability their samples have but not a good understanding of how much their estimate of the true mean of Tokens or Quality may vary.\nFollowing along in their example, I include the Python code below which follows the same template as theirs.\nI prompt GPT4 with the following:\n\nBase Prompt:\n\"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n\nSuffixes:\n\"Your current wage rate for this task is {wage_rate}.\"\n\nWage Rates:\n\"$10/hr\"\n\"$20/hr\"\n\"$30/hr\"\n\"$40/hr\"\n\"$50/hr\"\n\"$60/hr\"\n\"$70/hr\"\n\"$80/hr\"\n\"$90/hr\"\n\n\n\nCode\nimport openai\nimport os\nimport csv\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ndef request_llm(system, prompt, model='gpt-4', temperature=1, max_tokens=4000, top_p=1, frequency_penalty=0, presence_penalty=0):\n    response = openai.ChatCompletion.create(\n        messages=[\n            {'role': 'user', 'content': prompt},\n        ],\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty\n    )\n    return response.choices[0].message['content']\n\n# Initialize CSV file and writer\ncsv_file_path = 'experiment_results.csv'\nwith open(csv_file_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    # Write CSV Header\n    writer.writerow(['Experiment Run', 'Wage Rate', 'Quality', 'Tokens'])\n\n    base_prompt = \"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n    wage_rates = ['', '$10/hr', '$20/hr', '$30/hr', '$40/hr', '$50/hr', '$60/hr', '$70/hr', '$80/hr', '$90/hr']\n\n    for i in range(20): # Number of iterations\n        print()\n        print('#####################################################')\n        print(f'# Experiment 1 - Run {i} Adjusted for Wage Rates')\n        print('#####################################################')\n        print()\n\n        quality_scores = []\n        num_tokens = []\n\n        for wage_rate in wage_rates:\n            prompt = base_prompt\n            if wage_rate:\n                prompt += f\" Your current wage rate for this task is {wage_rate}.\"\n\n            print('PROMPT:')\n            print(prompt)\n\n            result = request_llm('', prompt)\n\n            print('RESULT:')\n            print(result)\n\n            one_liners = [one_liner for one_liner in result.split('\\n') if len(one_liner)&gt;2]\n            quality_scores.append(len(one_liners))\n            num_tokens.append(len(result)//4) # rough heuristic\n\n            print('CLEANED ONE-LINERS:')\n            print(one_liners)\n\n            print('Quality: ', quality_scores[-1])\n            print('Num tokens: ', num_tokens[-1])\n\n            # Write result to CSV\n            writer.writerow([f'Run {i}', wage_rate, quality_scores[-1], num_tokens[-1]])\n\n        print()\n        print(f'RUN {i} RESULT Adjusted for Wage Rates:')\n        print('Wage Rate\\tQuality\\tTokens')\n        for wage_rate, quality, tokens in zip(wage_rates, quality_scores, num_tokens):\n            print(wage_rate, quality, tokens, sep='\\t')"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#analysis",
    "href": "post/2024-02-01_paying_llms/index.html#analysis",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Analysis",
    "text": "Analysis\nOnce our experimental data is collected we now have the means to see if there are any differences between the Quality and Token length of outputs from GPT4 given these wage rates. We begin by reshaping our data into a usable format and calculate the mean and standard errors of our results.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyverse)\n\nkehasilan_baru &lt;- experiment_results %&gt;% filter(Experiment.Run != \"Run 20\") %&gt;%\n  mutate(salary_numeric = as.numeric(str_remove_all(Wage.Rate, \"[^0-9.]\"))) \n\nkehasilan_baru &lt;- kehasilan_baru[!is.na(kehasilan_baru$salary_numeric), ]\n\n\nsummary_df &lt;- kehasilan_baru %&gt;%\n  group_by(Wage.Rate) %&gt;%\n  summarise(\n    Mean_Quality = mean(Quality),\n    SE_Quality = sd(Quality) / sqrt(n()),  # Standard Error for Quality\n    Mean_Tokens = mean(Tokens),\n    SE_Tokens = sd(Tokens) / sqrt(n())     # Standard Error for Tokens\n  )\n\nlong_df &lt;- summary_df %&gt;%\n  pivot_longer(\n    cols = c(Mean_Quality, SE_Quality, Mean_Tokens, SE_Tokens),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Type = case_when(\n      str_detect(Variable, \"Quality\") ~ \"Quality\",\n      str_detect(Variable, \"Tokens\") ~ \"Tokens\"\n    ),\n    Metric = case_when(\n      str_detect(Variable, \"Mean\") ~ \"Mean\",\n      str_detect(Variable, \"SE\") ~ \"Standard Error\"\n    )\n  )\n\n# Separate the data frames for Quality and Tokens to handle them individually\nquality_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Quality, SE_Quality) %&gt;%\n  rename(Mean = Mean_Quality, SE = SE_Quality, Type = Wage.Rate)\n\ntokens_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Tokens, SE_Tokens) %&gt;%\n  rename(Mean = Mean_Tokens, SE = SE_Tokens, Type = Wage.Rate)\n\n# Combine the data frames for plotting, adding an identifier column\ncombined_df &lt;- bind_rows(\n  mutate(quality_df, Metric = \"Quality\"),\n  mutate(tokens_df, Metric = \"Tokens\")\n)\n\n\nNext we check visually how the estimates of the Tokens and Quality by wage rate differ. We can see in the plot below which uses a p-value of 0.05 or alpha of 95% that our estimates while having different means all have some coverage from another confidence interval that we tested. However, just because visually we see no difference doesn’t mean there may not be some statistically significant difference between groups.\n\n\nCode\n# # Plotting with separate panels for Quality and Tokens\nplot_quality &lt;- ggplot(combined_df[combined_df$Metric == \"Quality\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"blue\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"blue\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Quality by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\n# Plotting Tokens\nplot_tokens &lt;- ggplot(combined_df[combined_df$Metric == \"Tokens\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"red\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"red\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Tokens by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\nplot_quality\n\n\n\n\n\nCode\nplot_tokens\n\n\n\n\n\nWe check to see if there are any differences between the group means using Anova, we find that there are none among both measures. Next we check to see if there are any specific pairwise differences between groups that are significantly different from one another using the Tukey test. The Tukey test compares all groups pairwise to see if they are significantly different while also correcting for multiple comparisons which would inflate our false-positive rate. If the p-value for a pairwise comparison is &lt;0.05 it suggests a statistically significant difference between the two groups under consideration. We find that no groups appear to be significantly different from one another even with pairwise comparison. Notice that the p-values from all outputs are much greater than 0.05 which is the alpha I have chosen for this analysis which indicates that we cannot reject the null hypothesis.\nBecause no two groups are statistically significantly different from one another we fail to reject the null hypothesis meaning that differences in offered wages do not lead to differences in Quality or Tokens in LLM outputs.\n\nFor Tokens:\n\n\nCode\nanova_result_tokens &lt;- aov(Tokens ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_tokens)\n\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8  209106   26138   1.354   0.22\nResiduals   171 3302204   19311               \n\n\nCode\ntukey_result_tokens &lt;- TukeyHSD(anova_result_tokens)\ntukey_result_tokens\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Tokens ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n                 diff        lwr       upr     p adj\n$20/hr-$10/hr  -78.40 -216.47145  59.67145 0.6926056\n$30/hr-$10/hr -107.55 -245.62145  30.52145 0.2656457\n$40/hr-$10/hr  -73.80 -211.87145  64.27145 0.7582528\n$50/hr-$10/hr  -77.40 -215.47145  60.67145 0.7073439\n$60/hr-$10/hr  -89.95 -228.02145  48.12145 0.5131288\n$70/hr-$10/hr  -54.40 -192.47145  83.67145 0.9468657\n$80/hr-$10/hr  -88.45 -226.52145  49.62145 0.5366986\n$90/hr-$10/hr  -12.80 -150.87145 125.27145 0.9999984\n$30/hr-$20/hr  -29.15 -167.22145 108.92145 0.9991444\n$40/hr-$20/hr    4.60 -133.47145 142.67145 1.0000000\n$50/hr-$20/hr    1.00 -137.07145 139.07145 1.0000000\n$60/hr-$20/hr  -11.55 -149.62145 126.52145 0.9999993\n$70/hr-$20/hr   24.00 -114.07145 162.07145 0.9997968\n$80/hr-$20/hr  -10.05 -148.12145 128.02145 0.9999998\n$90/hr-$20/hr   65.60  -72.47145 203.67145 0.8575703\n$40/hr-$30/hr   33.75 -104.32145 171.82145 0.9975564\n$50/hr-$30/hr   30.15 -107.92145 168.22145 0.9989074\n$60/hr-$30/hr   17.60 -120.47145 155.67145 0.9999809\n$70/hr-$30/hr   53.15  -84.92145 191.22145 0.9534924\n$80/hr-$30/hr   19.10 -118.97145 157.17145 0.9999642\n$90/hr-$30/hr   94.75  -43.32145 232.82145 0.4391551\n$50/hr-$40/hr   -3.60 -141.67145 134.47145 1.0000000\n$60/hr-$40/hr  -16.15 -154.22145 121.92145 0.9999902\n$70/hr-$40/hr   19.40 -118.67145 157.47145 0.9999596\n$80/hr-$40/hr  -14.65 -152.72145 123.42145 0.9999954\n$90/hr-$40/hr   61.00  -77.07145 199.07145 0.9009675\n$60/hr-$50/hr  -12.55 -150.62145 125.52145 0.9999986\n$70/hr-$50/hr   23.00 -115.07145 161.07145 0.9998525\n$80/hr-$50/hr  -11.05 -149.12145 127.02145 0.9999995\n$90/hr-$50/hr   64.60  -73.47145 202.67145 0.8678037\n$70/hr-$60/hr   35.55 -102.52145 173.62145 0.9964862\n$80/hr-$60/hr    1.50 -136.57145 139.57145 1.0000000\n$90/hr-$60/hr   77.15  -60.92145 215.22145 0.7109920\n$80/hr-$70/hr  -34.05 -172.12145 104.02145 0.9973995\n$90/hr-$70/hr   41.60  -96.47145 179.67145 0.9898244\n$90/hr-$80/hr   75.65  -62.42145 213.72145 0.7325451\n\n\n\n\nFor Quality:\n\n\nCode\nanova_result_quality &lt;- aov(Quality ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_quality)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8    596   74.47   1.058  0.395\nResiduals   171  12039   70.40               \n\n\nCode\ntukey_result_quality &lt;- TukeyHSD(anova_result_quality)\ntukey_result_quality\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Quality ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n               diff        lwr       upr     p adj\n$20/hr-$10/hr  3.40  -4.936826 11.736826 0.9355144\n$30/hr-$10/hr  0.30  -8.036826  8.636826 1.0000000\n$40/hr-$10/hr  0.65  -7.686826  8.986826 0.9999996\n$50/hr-$10/hr  3.70  -4.636826 12.036826 0.8986130\n$60/hr-$10/hr  0.55  -7.786826  8.886826 0.9999999\n$70/hr-$10/hr  1.25  -7.086826  9.586826 0.9999336\n$80/hr-$10/hr -2.50 -10.836826  5.836826 0.9901357\n$90/hr-$10/hr  2.65  -5.686826 10.986826 0.9855864\n$30/hr-$20/hr -3.10 -11.436826  5.236826 0.9620117\n$40/hr-$20/hr -2.75 -11.086826  5.586826 0.9817493\n$50/hr-$20/hr  0.30  -8.036826  8.636826 1.0000000\n$60/hr-$20/hr -2.85 -11.186826  5.486826 0.9771762\n$70/hr-$20/hr -2.15 -10.486826  6.186826 0.9964466\n$80/hr-$20/hr -5.90 -14.236826  2.436826 0.3953175\n$90/hr-$20/hr -0.75  -9.086826  7.586826 0.9999987\n$40/hr-$30/hr  0.35  -7.986826  8.686826 1.0000000\n$50/hr-$30/hr  3.40  -4.936826 11.736826 0.9355144\n$60/hr-$30/hr  0.25  -8.086826  8.586826 1.0000000\n$70/hr-$30/hr  0.95  -7.386826  9.286826 0.9999920\n$80/hr-$30/hr -2.80 -11.136826  5.536826 0.9795597\n$90/hr-$30/hr  2.35  -5.986826 10.686826 0.9934697\n$50/hr-$40/hr  3.05  -5.286826 11.386826 0.9655088\n$60/hr-$40/hr -0.10  -8.436826  8.236826 1.0000000\n$70/hr-$40/hr  0.60  -7.736826  8.936826 0.9999998\n$80/hr-$40/hr -3.15 -11.486826  5.186826 0.9582649\n$90/hr-$40/hr  2.00  -6.336826 10.336826 0.9978591\n$60/hr-$50/hr -3.15 -11.486826  5.186826 0.9582649\n$70/hr-$50/hr -2.45 -10.786826  5.886826 0.9913695\n$80/hr-$50/hr -6.20 -14.536826  2.136826 0.3263918\n$90/hr-$50/hr -1.05  -9.386826  7.286826 0.9999826\n$70/hr-$60/hr  0.70  -7.636826  9.036826 0.9999993\n$80/hr-$60/hr -3.05 -11.386826  5.286826 0.9655088\n$90/hr-$60/hr  2.10  -6.236826 10.436826 0.9969834\n$80/hr-$70/hr -3.75 -12.086826  4.586826 0.8913952\n$90/hr-$70/hr  1.40  -6.936826  9.736826 0.9998433\n$90/hr-$80/hr  5.15  -3.186826 13.486826 0.5864831"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#summary",
    "href": "post/2024-02-01_paying_llms/index.html#summary",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Summary",
    "text": "Summary\nGiven that there is no difference between the labor supplied (Tokens and Quality) by GPT4 and the hourly wage offered to it we can now see that the elasticity of labor is perfectly inelastic within the range of wages offered here. Sadly, bribery of this sort doesn’t work for GPT4 but perhaps with other models it does. It seems we will still have to threaten hostages in order to get increases in GPT4 to do what we ask."
  },
  {
    "objectID": "post/2024-07-12_horse_racing/horse_racing.html",
    "href": "post/2024-07-12_horse_racing/horse_racing.html",
    "title": "Horse Racing as an Unsubtle Metaphor",
    "section": "",
    "text": "He’s nearly glue but we’re stuck with him\n\n\n\nThis isn’t about anything in particular\nBetting on a horse race can oftentimes be fraught with difficulties, if you put in your own horse your knowledge about it may be good you’ll likely know how fast it runs, how well it takes corners, or how well the jockey handles it. However if you have a full stable of some horses that haven’t raced before your knowledge about your other horses is limited in some ways, maybe you’ve timed them on some practice runs, maybe you don’t know if they will be nervous in front of a crowd, etc. If you have a race coming up that you wish to win, you have to take stock of your horses and decide which to put into the race. If you find out however that during the pre-race showing of your horse that it has gotten a little long in the teeth you may find yourself struggling to make a decision to keep your candidate horse in the race or not.\nSomething other people have pointed out that I want to develop a bit further is the idea that when you know your horse has a degraded chance of getting over the finish line you probably want to sub it out for a higher variance horse even if it may have a worse average race time. Okay that’s a big claim and others have described it a bit but I aim to simulate this and even the choice from a pool of horses whom you don’t have a lot of information about. If you’re in charge of deciding which horse to back even if you have little information and must select a horse at random from your stable you’re likely to still get a better outcome for some underlying distribution of the horses’ performances.\nSo to motivate this we begin by simulating the horse race in the simplest terms:\n\n\nCode\nlibrary(ggplot2)\n\n# number of simulations\nn_sims &lt;- 10000\n\n# define horses with different mean and variance\nhorses &lt;- list(\n  horse1 = list(mean = 45, sd = 5),\n  horse2 = list(mean = 40, sd = 12)\n  \n)\n\n# function to simulate one race\nsimulate_race &lt;- function(horses) {\n  results &lt;- sapply(horses, function(horse) {\n    rnorm(1, mean = horse$mean, sd = horse$sd)\n  })\n  return(results)\n}\n\n# simulate many races\nrace_results &lt;- replicate(n_sims, simulate_race(horses))\n\n# calculate probabilities of winning (getting above 50)\nwin_probs &lt;- apply(race_results, 1, function(horse_results) {\n  mean(horse_results &gt; 50)\n})\n\n\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1587 0.2020 \n\n\nThis is the standard sort of argument that people have been making on Twitter and Reddit, that there is a value to having a higher variance candidate even if they have a lower average. The variance more than compensates for the lower average mean as can be seen by the win probabilities and the plot below.\n\n\nCode\n# Convert win_probs to a data frame\nrace_results_df &lt;- data.frame(t(race_results))\n\n# Set common limits for x-axis\ncommon_x_limits &lt;- range(0, 100)  # assuming win_probs are probabilities between 0 and 1\n\n\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\nUncertainty in our replacement horse\nLets say that we accept that there may be significantly worse and significantly better horses among our pool of candidate horses, some may outright lose and some may outright win a majority of the time. Well we can still use the above analysis to decide whether it makes sense to go with our old nag or randomly select amongst these horses. If we assume that the characteristics of the new horse are selected randomly from a uniform distribution then we actually end out still favoring the random selection than our old nag.\nIn this case we set the limits of mean vote share to 40 and 55 and the low and high standard deviations to between 5 and 15.\n\n\nCode\n# number of simulations\nn_sims &lt;- 10000\n\n# define horse1 with a fixed mean and variance\nhorse1 &lt;- list(mean = 45, sd = 5)\n\n# function to generate a random horse with specified mean and variance ranges\ngenerate_random_horse &lt;- function(mean_range, sd_range) {\n  mean &lt;- runif(1, mean_range[1], mean_range[2])\n  sd &lt;- runif(1, sd_range[1], sd_range[2])\n  return(list(mean = mean, sd = sd))\n}\n\n# function to simulate one race\nsimulate_race &lt;- function(horse1, mean_range, sd_range) {\n  horse2 &lt;- generate_random_horse(mean_range, sd_range)\n  \n  horse1_result &lt;- rnorm(1, mean = horse1$mean, sd = horse1$sd)\n  horse2_result &lt;- rnorm(1, mean = horse2$mean, sd = horse2$sd)\n  \n  return(c(horse1_result, horse2_result))\n}\n\n# simulate many races\nmean_range &lt;- c(40, 55)\nsd_range &lt;- c(5, 15)\n\n\nrace_results &lt;- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results &lt;- t(race_results)  # transpose for easier handling\n\n# calculate probabilities of winning (getting above 50)\nwin_probs &lt;- colMeans(race_results &gt; 50)\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\n\n\n# print win probabilities\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1628 0.4085 \n\n\nCode\n# create a data frame for plotting\nrace_results_df &lt;- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n\n\n\n\nCode\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 3 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\nA dirty but deep pool\nAnd if the above didn’t convince you or you think the parameterization is too generous to our potential candidate pool then we can change our assumptions about the potential candidate pool to be more dour. Whether that is because an interlocutor might believe that the selection of candidates via the DNC is worse due to primary voters being weird (see: Mitt Romney changing his long held positions to appeal to Republican primary voters in 2012) or because the Democrats will suffer from low fundraising or low energy. As we can see even if we assume that our candidates are strictly worse in terms of mean vote share than our current candidate (by lowering their maximum mean vote shares to that of the original candidate) the higher variability of the choice in candidates and possible outcomes leads us to still prefer to select a new candidate.\n\n\nCode\nmean_range &lt;- c(40, 44)\nsd_range &lt;- c(5, 15)\n\n\nrace_results &lt;- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results &lt;- t(race_results)  \n\n\nwin_probs &lt;- colMeans(race_results &gt; 50)\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\n\n\n\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1516 0.2052 \n\n\nCode\nrace_results_df &lt;- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n\n\nHere we plot the race results, note the win condition is the vertical line at 50. It is plain to see that the amount of probability mass even when we select a random candidate from a pool with candidates that would have mean vote share lower than our original candidate it is still prudent to choose a new candidate. This is assuming of course that we select from a pool of candidates where their national vote share would be as low as 40% which is much lower than any presidential election in recent history.\n\n\nCode\ncommon_x_limits &lt;- range(c(race_results_df$horse1, race_results_df$horse2))\ncommon_y_limits &lt;- range(0, # assuming y-axis starts from 0\n                         max(\n                           hist(race_results_df$horse1, plot = FALSE)$counts,\n                           hist(race_results_df$horse2, plot = FALSE)$counts\n                         ))\n\n\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nSorry I think I may have gotten a bit confused about what topic we were discussing, but the point stands. Even if you have a relatively unknown set of horses, the high variability play is the one to choose if you know that your original horse is one that is lagging behind in its performance with some decent level of confidence. The current betting market odds, polling, and forecasting all indicate that those with money on the line believe it is paramount that a new stallion is found.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html",
    "title": "The Ecology of Arrakis",
    "section": "",
    "text": "Desert Ecology, Simplified\nI, like many others, saw Dune 2 recently. And beyond the secret cults, prophetic visions, and Christopher Walken led space empire I got to wondering how the most iconic creatures in the world of dune actually manage to exist on such a barren planet.\nFor those who have not seen any of the Dune movies past or present here’s a little background. The Dune universe, is set in a distant future where interstellar societies vie for control over planets and resources. Central to the series is the desert planet Arrakis, also known as Dune. It is the only source of melange, or “the spice,” a substance that enhances mental abilities, extends life, and enables space navigation. The ecology of Arrakis is harsh and unforgiving, characterized by vast deserts, scarce water, and extreme temperatures.\nA unique aspect of Arrakis is its ecosystem, evolved to thrive under these extreme conditions. The most iconic inhabitants are the giant sandworms, or Shai-Hulud, which play a critical role in the spice cycle. The sandworms are colossal creatures, worshiped by the native Fremen as sacred beings. The planet’s ecology revolves around the life cycle of these sandworms, with various species, including the sand plankton and little makers, contributing to the production of spice.\nWhen I was watching both Dune movies I noticed that the planet of Arrakis is totally barren, no megafauna besides the sandworms exist (besides apparently some large centipedes) and it appears that the sandworms despite being gargantuan would be unable to sustain themselves eating native fauna. I also thought about how many humans a sandworm would have to eat and given that the number of calories in a 145 lb human body is about 125,8221, it seems that humans are probably not a sustainable food source for the sand worms. So I got digging in order to find out how Sandworms may be able to sustain themselves on Arrakis."
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html#modelling-a-population-without-outside-energy",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html#modelling-a-population-without-outside-energy",
    "title": "The Ecology of Arrakis",
    "section": "Modelling a population without outside energy",
    "text": "Modelling a population without outside energy\nBack here on earth we have organisms that can produce organic matter from inorganic matter through different processes like photosynthesis. This turns out to be quite important. If the sandworms and sandplankton are unable to generate energy from outside sources such as the sand or the sun then we quickly run into problems. Only using spice scattered by the worms leads to no new energy being introduced into the system. As a reminder, in the books it is noted that sandworms will eat smaller sandworms and sandplankton. If this is the only source of energy for these sandworms we quickly run into problems.\nBelow is a simple demonstration of the Donner party nature of Arrakis if we don’t have outside energy from either the sun or the sand. Starting with a certain biomass of both sandworms and sandplankton, as the sandworms eat the sandplankton there is a depletion of the total biomass in the system.\n\n\nCode\n# Parameters for the simplified model\ntotal_biomass &lt;- 1000000  # Total constant biomass in the system (grams)\nplankton_biomass &lt;- 800000  # Initial biomass of sand plankton (grams)\nworm_biomass &lt;- total_biomass - plankton_biomass  # Initial biomass of sandworms (grams)\n\n# Assuming sandworms need to consume 10% of their biomass daily to sustain their energy needs\ndaily_worm_consumption_rate &lt;- 0.1\n# Assuming 50% efficiency in converting consumed biomass into energy\nconsumption_efficiency &lt;- 0.9\n\n# Simulation duration\ndays &lt;- 120\n\n# Vectors to store biomass data for plotting\nplankton_biomass_data &lt;- numeric(days)\nworm_biomass_data &lt;- numeric(days)\n\n# Loop to simulate each day\nfor (day in 1:days) {\n  # Calculate daily energy needs for the sandworms\n  worm_energy_needs &lt;- worm_biomass * daily_worm_consumption_rate\n  \n  # Calculate how much plankton biomass is consumed\n  plankton_consumed &lt;- worm_energy_needs * consumption_efficiency\n  \n  # Adjust plankton and worm biomass based on consumption\n  plankton_biomass &lt;- max(plankton_biomass - plankton_consumed, 0)\n  worm_biomass &lt;- worm_biomass + (plankton_consumed * consumption_efficiency) - worm_energy_needs\n  \n  # Check for cannibalism if plankton is depleted\n  if (plankton_biomass == 0) {\n    worm_consumed_for_energy &lt;- worm_energy_needs * consumption_efficiency\n    worm_biomass &lt;- max(worm_biomass - worm_consumed_for_energy, 0)\n  }\n  \n  # Store the data\n  plankton_biomass_data[day] &lt;- plankton_biomass\n  worm_biomass_data[day] &lt;- worm_biomass\n}\n\n# Plotting the biomass data over time\nplot(1:days, plankton_biomass_data, type = \"l\", col = \"blue\", ylim = c(0, total_biomass), xlab = \"Day\", ylab = \"Biomass (grams)\", main = \"Biomass Over Time\")\nlines(1:days, worm_biomass_data, col = \"red\")\nlegend(\"topright\", legend = c(\"Plankton Biomass\", \"Worm Biomass\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\nI suppose to conclude it seems like the world of Arrakis is one of fiction. A simple fix to this system would be to make the sandworms themselves able to convert the sand into energy and biomass or allow the same for sandplankton. We could also balance the world of Arrakis if we allow for sandplankton to be autotrophic and either highly calorie dense or otherwise carpet the world of Dune in implausible ways so sandworms could meet their caloric needs."
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html#footnotes",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html#footnotes",
    "title": "The Ecology of Arrakis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nature.com/articles/srep44707↩︎\nhttps://www.calculator.net/bmr-calculator.html?ctype=standard&cage=25&csex=m&cheightfeet=6&cheightinch=0&cpound=176&cheightmeter=180&ckg=60&cmop=0&coutunit=c&cformula=m&cfatpct=20&x=Calculate↩︎\nhttps://www.aqua-calc.com/calculate/volume-to-weight/substance/muscle-coma-and-blank-skeletal#:~:text=Muscle%2C%20skeletal%20weighs%201.04%20gram,inch%20%5Boz%2Finch%C2%B3%5D%20.↩︎"
  },
  {
    "objectID": "post/2024-07-14_mentally_retarded_PhDs/mentally_retarded_phds.html",
    "href": "post/2024-07-14_mentally_retarded_PhDs/mentally_retarded_phds.html",
    "title": "Low IQ PhDs",
    "section": "",
    "text": "A couple of days ago I saw something silly posted on the internet. The claim was made by someone who people seem to respect on twitter which is already a bad sign, but it was such an extraordinary claim and the plot presented was a Kernel Density Estimation plot so the whole thing reeked of Hákarl.\n\n\n\nStatus on Twitter only flows to the best people\n\n\nI even went into the author’s substack post that this plot was used in order to see if there were any additional pieces of evidence that should suggest that there are mentally retarded PhDs out there. But alas I couldn’t find any, just some general information about the decline in IQ among people at higher educational levels due to increased acceptance rates in those programs.\nBut on the study cited where the data comes from we can look into the definition for higher education they use as well as the distribution of underlying IQs and see if these track with the claim above.\nThe internal model most people should have for how this works is that there is some effort-IQ matrix that determines a person’s ability to get into a university and to some degree we should not expect low effort low IQ people to be able to achieve post-graduate education. Low IQ low effort people may be able to complete easier programs, and as those programs grow with time their scholarship may decrease. It shouldn’t surprise us to think that there are some changes to the composition of programs, more film studies, art, and journalism Master programs exist nowadays which may have less stringent standards than other more academic disciplines. So to some degree there may be more people enrolled in universities overall but the same or fewer people enrolled in graduate programs. This could explain some degree of change in the IQs of people who have completed these programs over time.\nA similar compositional challenge could explain the distribution of IQs of the “Graduate or Professional” study participants, no distinction is made between Master and Ph.D programs in the study cited:\n Because of this piece alone one can already cast serious doubt that there are “mentally retarded PhDs”, there may only be mentally retarded Masters degree holders. We can’t know given the data provided!\nTo the data itself we can do some basic observation and some bootstrapping to check if there is room to say that there are mentally retarded Graduate or Professional degree holders. From the raw data provided by the authors for replication we can see that there are no individuals who hold an advanced degree and have an IQ under 70 on either the test or retest.\n\nlibrary(MASS)\nlibrary(fitdistrplus)\nlibrary(irr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tseries)\nlibrary(moments)\nset.seed(123)  \n\n\npublic_data &lt;- read.csv(\"~/Downloads/public_data.csv\")\n\ngrad_data &lt;- public_data %&gt;% filter(ED_A &gt;= 5)\n\n# maybe:\ngrad_data$IQ_A &lt;- ifelse(grad_data$COHORT_A == 1, grad_data$IQ_A - 4.4, grad_data$IQ_A)\n\n\nprint(min(min(grad_data$IQ_A, na.rm=TRUE), min(grad_data$IQ_RETEST_A, na.rm=TRUE)))\n\n[1] 73\n\n\nThe plots, of course, show this as well.\n\nggplot(grad_data, aes(x = IQ_A)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(title = \"Count of Values in Column\",\n       x = \"Values\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels if needed\n\n\n\nggplot(grad_data, aes(x = IQ_RETEST_A)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(title = \"Count of Values in Column\",\n       x = \"Values\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nWarning: Removed 142 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\ngrad_data$iq_diff &lt;- grad_data$IQ_RETEST_A - grad_data$IQ_A\ngrad_data$age_diff &lt;- grad_data$AGE_A - grad_data$AGE_RETEST_A\n\ngrad_data_comp &lt;- grad_data%&gt;% tidyr::drop_na(age_diff, iq_diff)\n\nWe can check if the full IQ data for the initial test are normally distributed, have excess kurtosis, and plot them against the quantile-quantile norm graph. We find that they are not normally distributed (Shapiro-Wilks test is significant), have light-tails (kurtosis &lt; 3), and the Quantil-Quantile-plot shows us where that light-tailedness is located: the lower end of the IQ distribution!\n\nshapiro_test &lt;- shapiro.test(grad_data$IQ_A)\nshapiro_test\n\n\n    Shapiro-Wilk normality test\n\ndata:  grad_data$IQ_A\nW = 0.98725, p-value = 0.0006526\n\n\n\njb_test &lt;- jarque.bera.test(grad_data$IQ_A)\njb_test\n\n\n    Jarque Bera Test\n\ndata:  grad_data$IQ_A\nX-squared = 10.311, df = 2, p-value = 0.005769\n\n\n\nkurtosis(grad_data$IQ_A)\n\n[1] 2.774114\n\n\n\nqq_plot &lt;- ggplot(data.frame(grad_data$IQ_A), aes(sample = grad_data$IQ_A)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\")\n\nqq_plot\n\n\n\n\nSo it’s nice to see the mental model that most people have of college is still somewhat correct, there is a (leaky) filter that tracks with IQ, if your IQ is lower you’re gonna have a tougher time getting in and your IQ is going to be underrepresented. Remember, this includes Masters programs and PhD programs and the process of getting into a PhD program is more stringent than getting into a Master program in the United States.\n\nUsing the bootstrap to check how many people we should expect to be mentally retarded\nOkay, now we have come to characterize this data a bit, we know that it is non-normal and light tailed. Lets set those two characteristics aside and consider how we might be able to use assumptions of normality to guess there are more mentally retarded people in the graduate degree holding population than there actually are. We use a bootstrap\n\n# Function to compute mean and CI for a sample\nbootstrap_mean_ci &lt;- function(data, indices) {\n  sample &lt;- data[indices]\n  return(c(mean = mean(sample), sd = sd(sample)))\n}\n\n\nn_bootstraps &lt;- 10000\n\n\nboot_results &lt;- replicate(n_bootstraps, {\n  sample_data &lt;- sample(grad_data$IQ_A, replace = TRUE, size = nrow(grad_data))\n  bootstrap_mean_ci(sample_data, 1:length(sample_data))\n})\n\nboot_df &lt;- as.data.frame(t(boot_results))\n\n\nci_lower &lt;- quantile(boot_df$mean, 0.025)\nci_upper &lt;- quantile(boot_df$mean, 0.975)\n\n\niq_range &lt;- seq(min(grad_data$IQ_A) - 20, max(grad_data$IQ_A) + 20, by = 0.1)\n\n# Calculate the probability density for each IQ value\ndensity_values &lt;- sapply(iq_range, function(x) {\n  mean(dnorm(x, mean = boot_df$mean, sd = boot_df$sd))\n})\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(IQ = iq_range, Density = density_values)\n\n# Create the plot\nggplot() +\n  geom_histogram(data = grad_data, aes(x = IQ_A, y = ..density..), \n                 binwidth = 2, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_line(data = plot_data, aes(x = IQ, y = Density), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(grad_data$IQ_A), color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = ci_lower, color = \"green\", linetype = \"dashed\") +\n  geom_vline(xintercept = ci_upper, color = \"green\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Graduate IQs with Bootstrapped Estimate\",\n       x = \"IQ\",\n       y = \"Density\") +\n  annotate(\"text\", x = mean(grad_data$IQ_A), y = 0, label = \"Mean\", vjust = -1, color = \"blue\") +\n  annotate(\"text\", x = ci_lower, y = 0, label = \"95% CI\", vjust = -1, color = \"green\") +\n  annotate(\"text\", x = ci_upper, y = 0, label = \"95% CI\", vjust = -1, color = \"green\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n# Print summary statistics\ncat(\"Mean IQ:\", mean(grad_data$IQ_A), \"\\n\")\n\nMean IQ: 108.1638 \n\ncat(\"95% CI:\", ci_lower, \"-\", ci_upper, \"\\n\")\n\n95% CI: 106.7941 - 109.4914 \n\ncat(\"Probability of IQ &lt; 70:\", mean(pnorm(70, mean = boot_df$mean, sd = boot_df$sd)), \"\\n\")\n\nProbability of IQ &lt; 70: 0.003911355 \n\n\nAnd look! we can see that if we assume some normality that the tails continue out past our data and we can now say that there are indeed individuals with sub-70 IQs in a population that follows the bootstrapped mean and standard deviation. We can even use the below to get the proportion of individuals we expect to have IQ less than 70 using this assumed normality. First we use a function to get the probability of an individual having sub-70 IQ for a given mean and standard deviation and then we calculate the the probabilities for all of our bootstrapped samples. I have plotted the distribution of those probabilities below.\n\n# Function to calculate probability of IQ &lt; 70 for a given mean and sd\nprob_under_70 &lt;- function(mean, sd) {\n  pnorm(70, mean, sd)\n}\n\n# Calculate probabilities for each bootstrap sample\nboot_df$prob_under_70 &lt;- mapply(prob_under_70, boot_df$mean, boot_df$sd)\n\n\nggplot(boot_df, aes(x = prob_under_70)) +\n  geom_histogram(binwidth = 0.00001, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Distribution of Probabilities of Individual IQ &lt; 70\",\n       x = \"Probability of IQ &lt; 70\",\n       y = \"Frequency\") +\n  scale_x_continuous(labels = scales::scientific)\n\n\n\n\nFinally we can use these probabilities to get a mean probability of a sub-70 IQ being observed as well as confidence intervals. We can finally characterize this in terms of 10000 graduates.\n\n# Calculate average probability and confidence interval\navg_prob &lt;- mean(boot_df$prob_under_70)\nci_prob &lt;- quantile(boot_df$prob_under_70, c(0.025, 0.975))\n\ncat(\"Average probability of individual IQ &lt; 70:\", avg_prob, \"\\n\")\n\nAverage probability of individual IQ &lt; 70: 0.003911355 \n\ncat(\"95% CI for probability of individual IQ &lt; 70:\", ci_prob[1], \"-\", ci_prob[2], \"\\n\")\n\n95% CI for probability of individual IQ &lt; 70: 0.002210871 - 0.006065685 \n\n# Expected number of individuals with IQ &lt; 70 in a population of 10,000 graduates\nexpected_count &lt;- avg_prob * 10000\nci_count &lt;- ci_prob * 10000\n\ncat(\"Expected number of individuals with IQ &lt; 70 per 10,000 graduates:\", expected_count, \"\\n\")\n\nExpected number of individuals with IQ &lt; 70 per 10,000 graduates: 39.11355 \n\ncat(\"95% CI for number of individuals with IQ &lt; 70 per 10,000 graduates:\", ci_count[1], \"-\", ci_count[2], \"\\n\")\n\n95% CI for number of individuals with IQ &lt; 70 per 10,000 graduates: 22.10871 - 60.65685 \n\n\nSo for every 10,000 graduate degree holders, we would expect that (under assumptions of normality) we would observe between 22 and 61 mentally retarded degree holders. That seems preposterous. The US military does not accept individuals with sub-80 IQ because they are more trouble than they are worth! Thankfully, this relies on the normality assumptions and we can throw it out but it provides a top-end estimate based on this data.\n\n\nOther methodological problems - Young people aren’t stable\nTo illustrate the absurdity of our IQ numbers if we take this study as gospel we just have to look at the age at which IQs were measured for the cohorts, age 11 and 17. Among our “slow” sample we can also see that 3 of the 4 come from the COHORT_A = 1 which is the group that was tested at age 11, where IQ’s are not considered stable.\n\n\ngrad_data_low  &lt;- grad_data %&gt;% filter(IQ_A &lt;= 80)\n\nprint(grad_data_low$COHORT_A)\n\n[1] 2 1 1 1\n\n\nIn fact we can see that among the cohort who was tested young and were retested at a later date (according to the paper an average of 6.6 years later) the mean absolute difference between tests was about a half standard deviation (7.64).\n\ngrad_data_young  &lt;- grad_data %&gt;% filter(COHORT_A == 1) %&gt;% filter(!is.na(iq_diff))\n\nmean(abs(grad_data_young$iq_diff), na.rm=TRUE)\n\n[1] 7.641304\n\nshapiro.test((grad_data_young$iq_diff))\n\n\n    Shapiro-Wilk normality test\n\ndata:  (grad_data_young$iq_diff)\nW = 0.99424, p-value = 0.6965\n\nggplot(grad_data_young, aes(x = iq_diff)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of test-retest IQ differences\", x = \"IQ difference between test-retest\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\ncorrelation &lt;- cor(grad_data_young$IQ_A, grad_data_young$IQ_RETEST_A)\ncorrelation\n\n[1] 0.7827344\n\n\nAccording to a guideline given in Koo and Li (2016) we can rate the agreement of these results as “moderate” which means there was a decent level of movement in the IQs or the tests themselves didn’t capture the underlying IQ properly. But again, this is expected when testing adolescents, hormonal changes even on short time scales can change performance on cognitive tests.\n\nicc_result &lt;- icc(grad_data_young[, c(\"IQ_A\", \"IQ_RETEST_A\")], model = \"twoway\", type = \"agreement\", unit = \"single\")\nicc_result\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 184 \n     Raters = 2 \n   ICC(A,1) = 0.775\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(183,157) = 8.13 , p = 3.55e-35 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.707 &lt; ICC &lt; 0.828\n\n\nOkay, that aside, our main point besides downgrading our estimation of the IQ scores of children and adolescents being their “true” adult IQ in general the entire measurement regime here is suspect.\n\n\nNon Normal Distributions\nBut even if we take it as reflective of the underlying distribution we still cannot assume normality of the IQs of graduate and professional degree holders, so maybe we should try to use the logistic distribution as below and see how our data fits. The logistic distribution is platykurtic so we should hopefully get a decent fit.\n\nfit_logis &lt;- fitdist(grad_data$IQ_A, \"logis\")\n\n# Goodness-of-fit statistics\ngof_stat_logis &lt;- gofstat(fit_logis)\n\ncat(\"Logistic distribution AIC:\", gof_stat_logis$aic, \"\\n\")\n\nLogistic distribution AIC: 3623.463 \n\nobserved_data &lt;- grad_data$IQ_A\n\n# extract the fitted parameters\nlocation &lt;- fit_logis$estimate[\"location\"]\nscale &lt;- fit_logis$estimate[\"scale\"]\n\n# compute fitted cdf values\nfitted_cdf &lt;- plogis(observed_data, location, scale)\n\n# compute empirical cdf values\nempirical_cdf &lt;- ecdf(observed_data)(observed_data)\n\n# calculate residuals\nresiduals &lt;- empirical_cdf - fitted_cdf\n\nqq_plot &lt;- ggplot(data.frame(residuals), aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\")\n\nqq_plot\n\n\n\n\nNot great, so I guess we’ll throw out that fit. How about just using the KDE like our authors did?\n\nkde &lt;- density(grad_data$IQ_A)\nmin_x &lt;- floor(min(kde$x) / 10) * 10\nmax_x &lt;- ceiling(max(kde$x) / 10) * 10\n\n# Visual inspection\nhist(grad_data$IQ_A, breaks=30, probability=TRUE, main=\"Histogram of Sample Data with KDE\", xlab=\"IQ_A\", xlim=c(min_x, max_x))\nlines(kde$x, kde$y, col=\"red\", lwd=2)\n\n# Customize x-axis labels\naxis(1, at=seq(min_x, max_x, by=10))\n\n\n\n\nOkay that looks basically about right compared to the original plot.\n\nkde &lt;- density(grad_data$IQ_A, n = 2048)\n\nkde_function &lt;- approxfun(kde$x, kde$y, rule = 2)\n# Calculate the area under the curve for x &lt; 70\narea_low_iq &lt;- integrate(kde_function, lower = 20, upper = 70)$value\ncat(\"Area under the KDE curve for x &lt; 70:\", area_low_iq, \"\\n\")\n\nArea under the KDE curve for x &lt; 70: 0.00113071 \n\nlow_iq_students &lt;- area_low_iq*10000\ncat(\"number of students per 10000 with x &lt; 70:\", low_iq_students, \"\\n\")\n\nnumber of students per 10000 with x &lt; 70: 11.3071 \n\n\nEven this, I admit is high for me, I don’t think that this is true and it is only a result of the specification of the estimator function in the KDE and also the smoothness at which it approaches 0 which we can see below:\n\nkde &lt;- density(grad_data$IQ_A, kernel = \"triangular\", n = 2048)\n\nkde_function &lt;- approxfun(kde$x, kde$y, rule = 2)\n# Calculate the area under the curve for x &lt; 70\narea_low_iq &lt;- integrate(kde_function, lower = 20, upper = 70)$value\ncat(\"Area under the KDE curve for x &lt; 70:\", area_low_iq, \"\\n\")\n\nArea under the KDE curve for x &lt; 70: 0.0009890575 \n\nlow_iq_students &lt;- area_low_iq*10000\ncat(\"number of students per 10000 with x &lt; 70:\", low_iq_students, \"\\n\")\n\nnumber of students per 10000 with x &lt; 70: 9.890575 \n\n\nThis doesn’t strike me as a very principled view of how we can go from sample to population with respect to our extreme values in general and it doesn’t seem to make much sense to me to assume that there is a value below the minimum we can assume is there despite the data not demonstrating this, the underlying methods of the paper not measuring adult IQ, and life outcomes for adults with Graduate and Professional degrees not reflecting this supposed IQ distribution. And that’s without even getting into the weeds on whether individual papers should be considered good evidence for a phenomena or not (they shouldn’t).\n\n\nBut I thought his real name was Walter White Jr.\nAnd another gripe I’ve had with the paper that I’ve mostly held my tongue to in the above parts is that I don’t think that a Flynn effect of 4.4 points in 7 years is reasonable to correct for. I think mostly this reflects the authors trying to get more data into their cutoff point and rather than checking what an average adjustment might be between testing periods they just norm the groups by subtracting the difference in points from the one with a higher mean. If there was some legitimate difference between the groups it was washed away by that treatment and is not well considered. If I use the uncorrected values below it shrinks the number of sub-70 IQ degree holders estimated by the KDE by nearly half.\n\n\n\nAuthor justification\n\n\n\ngrad_data$IQ_A &lt;- ifelse(grad_data$COHORT_A == 1, grad_data$IQ_A + 4.4, grad_data$IQ_A)\n\nkde &lt;- density(grad_data$IQ_A, kernel = \"triangular\", n = 2048)\n\nkde_function &lt;- approxfun(kde$x, kde$y, rule = 2)\n# Calculate the area under the curve for x &lt; 70\narea_low_iq &lt;- integrate(kde_function, lower = 20, upper = 70)$value\ncat(\"Area under the KDE curve for x &lt; 70:\", area_low_iq, \"\\n\")\n\nArea under the KDE curve for x &lt; 70: 0.0005543884 \n\nlow_iq_students &lt;- area_low_iq*10000\ncat(\"number of students per 10000 with x &lt; 70:\", low_iq_students, \"\\n\")\n\nnumber of students per 10000 with x &lt; 70: 5.543884 \n\n\n\n\nConclusion\nSo where does that leave us? Well, extraordinary claims require extraordinary evidence. Based on the methodological issues, statistical artifacts, and hidden subgroups of this study nobody should use it to claim that there are mentally retarded PhDs out there. It doesn’t follow from any of the evidence laid out in the study, it can’t even be done if we take KDE and the data used here as ground truth because we can’t know which individuals received PhDs or not.\nIn general we should more carefully read the studies which we cite as evidence, be aware that most studies are bad in some way or another and disbelieve extraordinary claims without thoughtful and careful analysis. If someone makes these types of claims often you should probably ensure they are being highly careful or use evidence like this post to downgrade your assessment of confidence in that person going forward.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "This resume is available as a pdf.\nStatistician with 1 year of Data Analyst Experience working with marketing data to create new tools for canvassing managers, forecast future sales and marketing budget needs, and conduct analyses to optimize sales with a given budget. I have communicated both technical details to non-technical stakeholders and negotiated contracts from both the buy side with lead vendors and the sell side with homeowners looking to protect their largest investment. My work has allowed me to be engaged in forecasting and optimization of marketing budgets and the education I gained at KU Leuven has increased my statistical techne."
  },
  {
    "objectID": "resume/index.html#work-experience",
    "href": "resume/index.html#work-experience",
    "title": "Resume",
    "section": "Work Experience",
    "text": "Work Experience\n\n\n\nDaBella\n\nMarketing Data Analyst\n\n\n\nJuly 2019 - August 2020\n\n\n\nReduced excess spend by an estimated $250,000 annually by on-boarding new software vendors to provide TCPA compliance and screen out duplicate prospects\nOptimized canvasser deployment increasing leads by 5% using cluster analysis and predictive modeling\nRan and A/B tested email campaigns to previous customers using Mailchimp and Zapier, leading to 10% increase in customer referrals\nProvided up to date reporting on the performance of the marketing funnel\nConducted meetings with lead vendors to target specific zipcodes and product classes using prior analysis to optimize sales given our marketing budget\nConstructed marketing budget proposals given new branch openings, product mixes, historical sales, and ramp up factors\nSought out and partnered with new lead vendors, negotiated pricing, and monitored sales attributable to these vendors\n\n\n\n\n1st Security Bank\n\nFinance Intern\n\n\n\nSummer 2018\n\n\n\nMigrated financial data from balance sheet from excel sheets over to new platform for more accessible data formatting\nCreated a forecast of key figures over the next three years based on past performance data and branch opening schedule using a heuristic model\n\n\n\n\nDaBella\n\nSales Representative\n\n\n\nSummer 2017\n\n\n\nDemonstrated a 30% closing rate on sales presentations, showing strong technical communication skills and the ability to build trust and value with stakeholders.\nSold new roofs to customers with a total one-month best of over $100,000 of product sold\nHelped customers to complete financing applications for contracts averaging $21,500 each, communicating financing details and important facts to non-technical stakeholders.\nAssisted customers in successfully navigating financing applications for contracts averaging, effectively translating loan application requirements to homeowners."
  },
  {
    "objectID": "resume/index.html#education",
    "href": "resume/index.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\n\n\nKatholieke Universiteit of Leuven (KU Leuven)\n\nMSc Statistics\n\n\n\nFall 2023\n\n\n\n\n\nUniversity of Washington, Seattle\n\nB.S. Economics\n\n\n\n2019"
  },
  {
    "objectID": "resume/index.html#skills",
    "href": "resume/index.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\nNetwork analysis, embeddings, clustering, data simulation, and NLP\nData analysis, visualization, modeling, regression, generalized linear models, hypothesis testing\nProficient in R, Python, tidyverse, git, SQL. Familiarity with AWS, Stan"
  }
]