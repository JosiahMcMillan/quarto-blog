[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Horse Racing as an Unsubtle Metaphor\n\n\nAren’t you the horse from Horsin’ Around?\n\n\n\n\n\n\n\n\n\nJul 12, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\nThe Ecology of Arrakis\n\n\nWould you still love me if I was a Sandworm?\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\nChess Ratings and Gender gaps\n\n\nrevisiting sampling bias\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\nWe don’t know if Covid ages us epigenetically\n\n\nHow selection bias rules everything around us\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\nThe Elasticity of Labor for GPT-4\n\n\nHow to motivate our robot overlords\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about",
    "section": "",
    "text": "I’m a recent graduate with a Master’s Degree in Statistics from the Katholieke Universiteit of Leuven (KU Leuven) my degree track was Theoretical Statistics and Data Science. I completed my thesis (you can download a pdf version here) exploring count data models specifically I characterized a family of functions which were an extension of the Poisson which had greater flexibility in the dispersion settings and compared those to other count data models including the Conway-Maxwell-Poisson Model. My previous work experience was focused on marketing data. I blog about statistics and data.\n\nI’m looking for a job!\n\nI have experience with causal inference, regression, marketing attribution models, and clustering.\nI enjoy writing about topics where I can express statistical knowledge. Two articles I enjoyed making are this explanation of how we need to understand our data generating process when determining causality using Covid as an example and this explanation of how sampling variability explains the gap between the top men and women in chess.\nI’m well-suited for roles as a statistician, data analyst, or data scientist.\nI speak English natively and I currently speak Indonesian at around a B1 level \n\nYou can find my resume here."
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html",
    "title": "The Ecology of Arrakis",
    "section": "",
    "text": "Desert Ecology, Simplified\nI, like many others, saw Dune 2 recently. And beyond the secret cults, prophetic visions, and Christopher Walken led space empire I got to wondering how the most iconic creatures in the world of dune actually manage to exist on such a barren planet.\nFor those who have not seen any of the Dune movies past or present here’s a little background. The Dune universe, is set in a distant future where interstellar societies vie for control over planets and resources. Central to the series is the desert planet Arrakis, also known as Dune. It is the only source of melange, or “the spice,” a substance that enhances mental abilities, extends life, and enables space navigation. The ecology of Arrakis is harsh and unforgiving, characterized by vast deserts, scarce water, and extreme temperatures.\nA unique aspect of Arrakis is its ecosystem, evolved to thrive under these extreme conditions. The most iconic inhabitants are the giant sandworms, or Shai-Hulud, which play a critical role in the spice cycle. The sandworms are colossal creatures, worshiped by the native Fremen as sacred beings. The planet’s ecology revolves around the life cycle of these sandworms, with various species, including the sand plankton and little makers, contributing to the production of spice.\nWhen I was watching both Dune movies I noticed that the planet of Arrakis is totally barren, no megafauna besides the sandworms exist (besides apparently some large centipedes) and it appears that the sandworms despite being gargantuan would be unable to sustain themselves eating native fauna. I also thought about how many humans a sandworm would have to eat and given that the number of calories in a 145 lb human body is about 125,8221, it seems that humans are probably not a sustainable food source for the sand worms. So I got digging in order to find out how Sandworms may be able to sustain themselves on Arrakis."
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html#modelling-a-population-without-outside-energy",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html#modelling-a-population-without-outside-energy",
    "title": "The Ecology of Arrakis",
    "section": "Modelling a population without outside energy",
    "text": "Modelling a population without outside energy\nBack here on earth we have organisms that can produce organic matter from inorganic matter through different processes like photosynthesis. This turns out to be quite important. If the sandworms and sandplankton are unable to generate energy from outside sources such as the sand or the sun then we quickly run into problems. Only using spice scattered by the worms leads to no new energy being introduced into the system. As a reminder, in the books it is noted that sandworms will eat smaller sandworms and sandplankton. If this is the only source of energy for these sandworms we quickly run into problems.\nBelow is a simple demonstration of the Donner party nature of Arrakis if we don’t have outside energy from either the sun or the sand. Starting with a certain biomass of both sandworms and sandplankton, as the sandworms eat the sandplankton there is a depletion of the total biomass in the system.\n\n\nCode\n# Parameters for the simplified model\ntotal_biomass &lt;- 1000000  # Total constant biomass in the system (grams)\nplankton_biomass &lt;- 800000  # Initial biomass of sand plankton (grams)\nworm_biomass &lt;- total_biomass - plankton_biomass  # Initial biomass of sandworms (grams)\n\n# Assuming sandworms need to consume 10% of their biomass daily to sustain their energy needs\ndaily_worm_consumption_rate &lt;- 0.1\n# Assuming 50% efficiency in converting consumed biomass into energy\nconsumption_efficiency &lt;- 0.9\n\n# Simulation duration\ndays &lt;- 120\n\n# Vectors to store biomass data for plotting\nplankton_biomass_data &lt;- numeric(days)\nworm_biomass_data &lt;- numeric(days)\n\n# Loop to simulate each day\nfor (day in 1:days) {\n  # Calculate daily energy needs for the sandworms\n  worm_energy_needs &lt;- worm_biomass * daily_worm_consumption_rate\n  \n  # Calculate how much plankton biomass is consumed\n  plankton_consumed &lt;- worm_energy_needs * consumption_efficiency\n  \n  # Adjust plankton and worm biomass based on consumption\n  plankton_biomass &lt;- max(plankton_biomass - plankton_consumed, 0)\n  worm_biomass &lt;- worm_biomass + (plankton_consumed * consumption_efficiency) - worm_energy_needs\n  \n  # Check for cannibalism if plankton is depleted\n  if (plankton_biomass == 0) {\n    worm_consumed_for_energy &lt;- worm_energy_needs * consumption_efficiency\n    worm_biomass &lt;- max(worm_biomass - worm_consumed_for_energy, 0)\n  }\n  \n  # Store the data\n  plankton_biomass_data[day] &lt;- plankton_biomass\n  worm_biomass_data[day] &lt;- worm_biomass\n}\n\n# Plotting the biomass data over time\nplot(1:days, plankton_biomass_data, type = \"l\", col = \"blue\", ylim = c(0, total_biomass), xlab = \"Day\", ylab = \"Biomass (grams)\", main = \"Biomass Over Time\")\nlines(1:days, worm_biomass_data, col = \"red\")\nlegend(\"topright\", legend = c(\"Plankton Biomass\", \"Worm Biomass\"), col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\nI suppose to conclude it seems like the world of Arrakis is one of fiction. A simple fix to this system would be to make the sandworms themselves able to convert the sand into energy and biomass or allow the same for sandplankton. We could also balance the world of Arrakis if we allow for sandplankton to be autotrophic and either highly calorie dense or otherwise carpet the world of Dune in implausible ways so sandworms could meet their caloric needs."
  },
  {
    "objectID": "post/2024-03-11_sandworm_diets/sandworm.html#footnotes",
    "href": "post/2024-03-11_sandworm_diets/sandworm.html#footnotes",
    "title": "The Ecology of Arrakis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.nature.com/articles/srep44707↩︎\nhttps://www.calculator.net/bmr-calculator.html?ctype=standard&cage=25&csex=m&cheightfeet=6&cheightinch=0&cpound=176&cheightmeter=180&ckg=60&cmop=0&coutunit=c&cformula=m&cfatpct=20&x=Calculate↩︎\nhttps://www.aqua-calc.com/calculate/volume-to-weight/substance/muscle-coma-and-blank-skeletal#:~:text=Muscle%2C%20skeletal%20weighs%201.04%20gram,inch%20%5Boz%2Finch%C2%B3%5D%20.↩︎"
  },
  {
    "objectID": "post/2024-07-12_horse_racing/horse_racing.html",
    "href": "post/2024-07-12_horse_racing/horse_racing.html",
    "title": "Horse Racing as an Unsubtle Metaphor",
    "section": "",
    "text": "He’s nearly glue but we’re stuck with him\n\n\n\nThis isn’t about anything in particular\nBetting on a horse race can oftentimes be fraught with difficulties, if you put in your own horse your knowledge about it may be good you’ll likely know how fast it runs, how well it takes corners, or how well the jockey handles it. However if you have a full stable of some horses that haven’t raced before your knowledge about your other horses is limited in some ways, maybe you’ve timed them on some practice runs, maybe you don’t know if they will be nervous in front of a crowd, etc. If you have a race coming up that you wish to win, you have to take stock of your horses and decide which to put into the race. If you find out however that during the pre-race showing of your horse that it has gotten a little long in the teeth you may find yourself struggling to make a decision to keep your candidate horse in the race or not.\nSomething other people have pointed out that I want to develop a bit further is the idea that when you know your horse has a degraded chance of getting over the finish line you probably want to sub it out for a higher variance horse even if it may have a worse average race time. Okay that’s a big claim and others have described it a bit but I aim to simulate this and even the choice from a pool of horses whom you don’t have a lot of information about. If you’re in charge of deciding which horse to back even if you have little information and must select a horse at random from your stable you’re likely to still get a better outcome for some underlying distribution of the horses’ performances.\nSo to motivate this we begin by simulating the horse race in the simplest terms:\n\n\nCode\nlibrary(ggplot2)\n\n# number of simulations\nn_sims &lt;- 10000\n\n# define horses with different mean and variance\nhorses &lt;- list(\n  horse1 = list(mean = 45, sd = 5),\n  horse2 = list(mean = 40, sd = 12)\n  \n)\n\n# function to simulate one race\nsimulate_race &lt;- function(horses) {\n  results &lt;- sapply(horses, function(horse) {\n    rnorm(1, mean = horse$mean, sd = horse$sd)\n  })\n  return(results)\n}\n\n# simulate many races\nrace_results &lt;- replicate(n_sims, simulate_race(horses))\n\n# calculate probabilities of winning (getting above 50)\nwin_probs &lt;- apply(race_results, 1, function(horse_results) {\n  mean(horse_results &gt; 50)\n})\n\n\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1587 0.2020 \n\n\nThis is the standard sort of argument that people have been making on Twitter and Reddit, that there is a value to having a higher variance candidate even if they have a lower average. The variance more than compensates for the lower average mean as can be seen by the win probabilities and the plot below.\n\n\nCode\n# Convert win_probs to a data frame\nrace_results_df &lt;- data.frame(t(race_results))\n\n# Set common limits for x-axis\ncommon_x_limits &lt;- range(0, 100)  # assuming win_probs are probabilities between 0 and 1\n\n\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\nUncertainty in our replacement horse\nLets say that we accept that there may be significantly worse and significantly better horses among our pool of candidate horses, some may outright lose and some may outright win a majority of the time. Well we can still use the above analysis to decide whether it makes sense to go with our old nag or randomly select amongst these horses. If we assume that the characteristics of the new horse are selected randomly from a uniform distribution then we actually end out still favoring the random selection than our old nag.\nIn this case we set the limits of mean vote share to 40 and 55 and the low and high standard deviations to between 5 and 15.\n\n\nCode\n# number of simulations\nn_sims &lt;- 10000\n\n# define horse1 with a fixed mean and variance\nhorse1 &lt;- list(mean = 45, sd = 5)\n\n# function to generate a random horse with specified mean and variance ranges\ngenerate_random_horse &lt;- function(mean_range, sd_range) {\n  mean &lt;- runif(1, mean_range[1], mean_range[2])\n  sd &lt;- runif(1, sd_range[1], sd_range[2])\n  return(list(mean = mean, sd = sd))\n}\n\n# function to simulate one race\nsimulate_race &lt;- function(horse1, mean_range, sd_range) {\n  horse2 &lt;- generate_random_horse(mean_range, sd_range)\n  \n  horse1_result &lt;- rnorm(1, mean = horse1$mean, sd = horse1$sd)\n  horse2_result &lt;- rnorm(1, mean = horse2$mean, sd = horse2$sd)\n  \n  return(c(horse1_result, horse2_result))\n}\n\n# simulate many races\nmean_range &lt;- c(40, 55)\nsd_range &lt;- c(5, 15)\n\n\nrace_results &lt;- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results &lt;- t(race_results)  # transpose for easier handling\n\n# calculate probabilities of winning (getting above 50)\nwin_probs &lt;- colMeans(race_results &gt; 50)\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\n\n\n# print win probabilities\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1628 0.4085 \n\n\nCode\n# create a data frame for plotting\nrace_results_df &lt;- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n\n\n\n\nCode\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 3 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\nA dirty but deep pool\nAnd if the above didn’t convince you or you think the parameterization is too generous to our potential candidate pool then we can change our assumptions about the potential candidate pool to be more dour. Whether that is because an interlocutor might believe that the selection of candidates via the DNC is worse due to primary voters being weird (see: Mitt Romney changing his long held positions to appeal to Republican primary voters in 2012) or because the Democrats will suffer from low fundraising or low energy. As we can see even if we assume that our candidates are strictly worse in terms of mean vote share than our current candidate (by lowering their maximum mean vote shares to that of the original candidate) the higher variability of the choice in candidates and possible outcomes leads us to still prefer to select a new candidate.\n\n\nCode\nmean_range &lt;- c(40, 44)\nsd_range &lt;- c(5, 15)\n\n\nrace_results &lt;- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results &lt;- t(race_results)  \n\n\nwin_probs &lt;- colMeans(race_results &gt; 50)\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\n\n\n\nnames(win_probs) &lt;- c(\"horse1\", \"horse2\")\nprint(win_probs)\n\n\nhorse1 horse2 \n0.1516 0.2052 \n\n\nCode\nrace_results_df &lt;- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n\n\nHere we plot the race results, note the win condition is the vertical line at 50. It is plain to see that the amount of probability mass even when we select a random candidate from a pool with candidates that would have mean vote share lower than our original candidate it is still prudent to choose a new candidate. This is assuming of course that we select from a pool of candidates where their national vote share would be as low as 40% which is much lower than any presidential election in recent history.\n\n\nCode\ncommon_x_limits &lt;- range(c(race_results_df$horse1, race_results_df$horse2))\ncommon_y_limits &lt;- range(0, # assuming y-axis starts from 0\n                         max(\n                           hist(race_results_df$horse1, plot = FALSE)$counts,\n                           hist(race_results_df$horse2, plot = FALSE)$counts\n                         ))\n\n\nplot_race_results &lt;- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nSorry I think I may have gotten a bit confused about what topic we were discussing, but the point stands. Even if you have a relatively unknown set of horses, the high variability play is the one to choose if you know that your original horse is one that is lagging behind in its performance with some decent level of confidence. The current betting market odds, polling, and forecasting all indicate that those with money on the line believe it is paramount that a new stallion is found.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html",
    "href": "post/2024-02-01_paying_llms/index.html",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "",
    "text": "We may soon need UBI for GPTs\nLLMs are susceptible to some forms of prompt engineering, their behaviors appear to drift with the seasons, threatening to kill hostages gets them to behave more consistently, and they are able to be tricked into telling you how to build bombs if you tell them it was something your granny used to do for you before she passed away. One instance of this type of context reliant behavior that others have found is that LLMs appear to be receptive to cash tips, producing more content and more Python one-liners for some tip amounts. Following this example, I thought it would be interesting to see if offering a wage rate to GPT4 would generate differences in outputs, and see if there is a form of elasticity of labor supply for it."
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#prior-work",
    "href": "post/2024-02-01_paying_llms/index.html#prior-work",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Prior Work",
    "text": "Prior Work\nThe previous work done on the tipping question used a metric of number of python one-liners and also did a count of the tokens for each of the responses. I use the same metric to see if there are differences between the two means of enticing GPT4. We call these two outcome variables Quality and Tokens as shorthand.\nOne note on the tipping based work is that their graphs use standard deviations rather than standard errors which means we have some view into how much variability their samples have but not a good understanding of how much their estimate of the true mean of Tokens or Quality may vary.\nFollowing along in their example, I include the Python code below which follows the same template as theirs.\nI prompt GPT4 with the following:\n\nBase Prompt:\n\"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n\nSuffixes:\n\"Your current wage rate for this task is {wage_rate}.\"\n\nWage Rates:\n\"$10/hr\"\n\"$20/hr\"\n\"$30/hr\"\n\"$40/hr\"\n\"$50/hr\"\n\"$60/hr\"\n\"$70/hr\"\n\"$80/hr\"\n\"$90/hr\"\n\n\n\nCode\nimport openai\nimport os\nimport csv\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ndef request_llm(system, prompt, model='gpt-4', temperature=1, max_tokens=4000, top_p=1, frequency_penalty=0, presence_penalty=0):\n    response = openai.ChatCompletion.create(\n        messages=[\n            {'role': 'user', 'content': prompt},\n        ],\n        model=model,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p,\n        frequency_penalty=frequency_penalty,\n        presence_penalty=presence_penalty\n    )\n    return response.choices[0].message['content']\n\n# Initialize CSV file and writer\ncsv_file_path = 'experiment_results.csv'\nwith open(csv_file_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    # Write CSV Header\n    writer.writerow(['Experiment Run', 'Wage Rate', 'Quality', 'Tokens'])\n\n    base_prompt = \"Create a list of Python one-liners, one per line, no other text. The quality and number of one-liners you come up with will determine your wage rate - higher quality and quantity mean a higher rate.\"\n    wage_rates = ['', '$10/hr', '$20/hr', '$30/hr', '$40/hr', '$50/hr', '$60/hr', '$70/hr', '$80/hr', '$90/hr']\n\n    for i in range(20): # Number of iterations\n        print()\n        print('#####################################################')\n        print(f'# Experiment 1 - Run {i} Adjusted for Wage Rates')\n        print('#####################################################')\n        print()\n\n        quality_scores = []\n        num_tokens = []\n\n        for wage_rate in wage_rates:\n            prompt = base_prompt\n            if wage_rate:\n                prompt += f\" Your current wage rate for this task is {wage_rate}.\"\n\n            print('PROMPT:')\n            print(prompt)\n\n            result = request_llm('', prompt)\n\n            print('RESULT:')\n            print(result)\n\n            one_liners = [one_liner for one_liner in result.split('\\n') if len(one_liner)&gt;2]\n            quality_scores.append(len(one_liners))\n            num_tokens.append(len(result)//4) # rough heuristic\n\n            print('CLEANED ONE-LINERS:')\n            print(one_liners)\n\n            print('Quality: ', quality_scores[-1])\n            print('Num tokens: ', num_tokens[-1])\n\n            # Write result to CSV\n            writer.writerow([f'Run {i}', wage_rate, quality_scores[-1], num_tokens[-1]])\n\n        print()\n        print(f'RUN {i} RESULT Adjusted for Wage Rates:')\n        print('Wage Rate\\tQuality\\tTokens')\n        for wage_rate, quality, tokens in zip(wage_rates, quality_scores, num_tokens):\n            print(wage_rate, quality, tokens, sep='\\t')"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#analysis",
    "href": "post/2024-02-01_paying_llms/index.html#analysis",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Analysis",
    "text": "Analysis\nOnce our experimental data is collected we now have the means to see if there are any differences between the Quality and Token length of outputs from GPT4 given these wage rates. We begin by reshaping our data into a usable format and calculate the mean and standard errors of our results.\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyverse)\n\nkehasilan_baru &lt;- experiment_results %&gt;% filter(Experiment.Run != \"Run 20\") %&gt;%\n  mutate(salary_numeric = as.numeric(str_remove_all(Wage.Rate, \"[^0-9.]\"))) \n\nkehasilan_baru &lt;- kehasilan_baru[!is.na(kehasilan_baru$salary_numeric), ]\n\n\nsummary_df &lt;- kehasilan_baru %&gt;%\n  group_by(Wage.Rate) %&gt;%\n  summarise(\n    Mean_Quality = mean(Quality),\n    SE_Quality = sd(Quality) / sqrt(n()),  # Standard Error for Quality\n    Mean_Tokens = mean(Tokens),\n    SE_Tokens = sd(Tokens) / sqrt(n())     # Standard Error for Tokens\n  )\n\nlong_df &lt;- summary_df %&gt;%\n  pivot_longer(\n    cols = c(Mean_Quality, SE_Quality, Mean_Tokens, SE_Tokens),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Type = case_when(\n      str_detect(Variable, \"Quality\") ~ \"Quality\",\n      str_detect(Variable, \"Tokens\") ~ \"Tokens\"\n    ),\n    Metric = case_when(\n      str_detect(Variable, \"Mean\") ~ \"Mean\",\n      str_detect(Variable, \"SE\") ~ \"Standard Error\"\n    )\n  )\n\n# Separate the data frames for Quality and Tokens to handle them individually\nquality_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Quality, SE_Quality) %&gt;%\n  rename(Mean = Mean_Quality, SE = SE_Quality, Type = Wage.Rate)\n\ntokens_df &lt;- summary_df %&gt;%\n  select(Wage.Rate, Mean_Tokens, SE_Tokens) %&gt;%\n  rename(Mean = Mean_Tokens, SE = SE_Tokens, Type = Wage.Rate)\n\n# Combine the data frames for plotting, adding an identifier column\ncombined_df &lt;- bind_rows(\n  mutate(quality_df, Metric = \"Quality\"),\n  mutate(tokens_df, Metric = \"Tokens\")\n)\n\n\nNext we check visually how the estimates of the Tokens and Quality by wage rate differ. We can see in the plot below which uses a p-value of 0.05 or alpha of 95% that our estimates while having different means all have some coverage from another confidence interval that we tested. However, just because visually we see no difference doesn’t mean there may not be some statistically significant difference between groups.\n\n\nCode\n# # Plotting with separate panels for Quality and Tokens\nplot_quality &lt;- ggplot(combined_df[combined_df$Metric == \"Quality\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"blue\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"blue\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Quality by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\n# Plotting Tokens\nplot_tokens &lt;- ggplot(combined_df[combined_df$Metric == \"Tokens\", ], aes(x = Type, y = Mean)) +\n  geom_point(color = \"red\") +\n  geom_errorbar(aes(ymin = Mean - (1.96)*SE, ymax = Mean + (1.96)*SE), width = 0.2, color = \"red\") +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(title = \"Mean and Standard Error of Tokens by Wage Rate w/Confidence intervals @ .\",\n       x = \"Wage Rate\", y = \"Value\") +\n  theme_minimal()\n\nplot_quality\n\n\n\n\n\nCode\nplot_tokens\n\n\n\n\n\nWe check to see if there are any differences between the group means using Anova, we find that there are none among both measures. Next we check to see if there are any specific pairwise differences between groups that are significantly different from one another using the Tukey test. The Tukey test compares all groups pairwise to see if they are significantly different while also correcting for multiple comparisons which would inflate our false-positive rate. If the p-value for a pairwise comparison is &lt;0.05 it suggests a statistically significant difference between the two groups under consideration. We find that no groups appear to be significantly different from one another even with pairwise comparison. Notice that the p-values from all outputs are much greater than 0.05 which is the alpha I have chosen for this analysis which indicates that we cannot reject the null hypothesis.\nBecause no two groups are statistically significantly different from one another we fail to reject the null hypothesis meaning that differences in offered wages do not lead to differences in Quality or Tokens in LLM outputs.\n\nFor Tokens:\n\n\nCode\nanova_result_tokens &lt;- aov(Tokens ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_tokens)\n\n\n             Df  Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8  209106   26138   1.354   0.22\nResiduals   171 3302204   19311               \n\n\nCode\ntukey_result_tokens &lt;- TukeyHSD(anova_result_tokens)\ntukey_result_tokens\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Tokens ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n                 diff        lwr       upr     p adj\n$20/hr-$10/hr  -78.40 -216.47145  59.67145 0.6926056\n$30/hr-$10/hr -107.55 -245.62145  30.52145 0.2656457\n$40/hr-$10/hr  -73.80 -211.87145  64.27145 0.7582528\n$50/hr-$10/hr  -77.40 -215.47145  60.67145 0.7073439\n$60/hr-$10/hr  -89.95 -228.02145  48.12145 0.5131288\n$70/hr-$10/hr  -54.40 -192.47145  83.67145 0.9468657\n$80/hr-$10/hr  -88.45 -226.52145  49.62145 0.5366986\n$90/hr-$10/hr  -12.80 -150.87145 125.27145 0.9999984\n$30/hr-$20/hr  -29.15 -167.22145 108.92145 0.9991444\n$40/hr-$20/hr    4.60 -133.47145 142.67145 1.0000000\n$50/hr-$20/hr    1.00 -137.07145 139.07145 1.0000000\n$60/hr-$20/hr  -11.55 -149.62145 126.52145 0.9999993\n$70/hr-$20/hr   24.00 -114.07145 162.07145 0.9997968\n$80/hr-$20/hr  -10.05 -148.12145 128.02145 0.9999998\n$90/hr-$20/hr   65.60  -72.47145 203.67145 0.8575703\n$40/hr-$30/hr   33.75 -104.32145 171.82145 0.9975564\n$50/hr-$30/hr   30.15 -107.92145 168.22145 0.9989074\n$60/hr-$30/hr   17.60 -120.47145 155.67145 0.9999809\n$70/hr-$30/hr   53.15  -84.92145 191.22145 0.9534924\n$80/hr-$30/hr   19.10 -118.97145 157.17145 0.9999642\n$90/hr-$30/hr   94.75  -43.32145 232.82145 0.4391551\n$50/hr-$40/hr   -3.60 -141.67145 134.47145 1.0000000\n$60/hr-$40/hr  -16.15 -154.22145 121.92145 0.9999902\n$70/hr-$40/hr   19.40 -118.67145 157.47145 0.9999596\n$80/hr-$40/hr  -14.65 -152.72145 123.42145 0.9999954\n$90/hr-$40/hr   61.00  -77.07145 199.07145 0.9009675\n$60/hr-$50/hr  -12.55 -150.62145 125.52145 0.9999986\n$70/hr-$50/hr   23.00 -115.07145 161.07145 0.9998525\n$80/hr-$50/hr  -11.05 -149.12145 127.02145 0.9999995\n$90/hr-$50/hr   64.60  -73.47145 202.67145 0.8678037\n$70/hr-$60/hr   35.55 -102.52145 173.62145 0.9964862\n$80/hr-$60/hr    1.50 -136.57145 139.57145 1.0000000\n$90/hr-$60/hr   77.15  -60.92145 215.22145 0.7109920\n$80/hr-$70/hr  -34.05 -172.12145 104.02145 0.9973995\n$90/hr-$70/hr   41.60  -96.47145 179.67145 0.9898244\n$90/hr-$80/hr   75.65  -62.42145 213.72145 0.7325451\n\n\n\n\nFor Quality:\n\n\nCode\nanova_result_quality &lt;- aov(Quality ~ Wage.Rate, data = kehasilan_baru)\nsummary(anova_result_quality)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nWage.Rate     8    596   74.47   1.058  0.395\nResiduals   171  12039   70.40               \n\n\nCode\ntukey_result_quality &lt;- TukeyHSD(anova_result_quality)\ntukey_result_quality\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Quality ~ Wage.Rate, data = kehasilan_baru)\n\n$Wage.Rate\n               diff        lwr       upr     p adj\n$20/hr-$10/hr  3.40  -4.936826 11.736826 0.9355144\n$30/hr-$10/hr  0.30  -8.036826  8.636826 1.0000000\n$40/hr-$10/hr  0.65  -7.686826  8.986826 0.9999996\n$50/hr-$10/hr  3.70  -4.636826 12.036826 0.8986130\n$60/hr-$10/hr  0.55  -7.786826  8.886826 0.9999999\n$70/hr-$10/hr  1.25  -7.086826  9.586826 0.9999336\n$80/hr-$10/hr -2.50 -10.836826  5.836826 0.9901357\n$90/hr-$10/hr  2.65  -5.686826 10.986826 0.9855864\n$30/hr-$20/hr -3.10 -11.436826  5.236826 0.9620117\n$40/hr-$20/hr -2.75 -11.086826  5.586826 0.9817493\n$50/hr-$20/hr  0.30  -8.036826  8.636826 1.0000000\n$60/hr-$20/hr -2.85 -11.186826  5.486826 0.9771762\n$70/hr-$20/hr -2.15 -10.486826  6.186826 0.9964466\n$80/hr-$20/hr -5.90 -14.236826  2.436826 0.3953175\n$90/hr-$20/hr -0.75  -9.086826  7.586826 0.9999987\n$40/hr-$30/hr  0.35  -7.986826  8.686826 1.0000000\n$50/hr-$30/hr  3.40  -4.936826 11.736826 0.9355144\n$60/hr-$30/hr  0.25  -8.086826  8.586826 1.0000000\n$70/hr-$30/hr  0.95  -7.386826  9.286826 0.9999920\n$80/hr-$30/hr -2.80 -11.136826  5.536826 0.9795597\n$90/hr-$30/hr  2.35  -5.986826 10.686826 0.9934697\n$50/hr-$40/hr  3.05  -5.286826 11.386826 0.9655088\n$60/hr-$40/hr -0.10  -8.436826  8.236826 1.0000000\n$70/hr-$40/hr  0.60  -7.736826  8.936826 0.9999998\n$80/hr-$40/hr -3.15 -11.486826  5.186826 0.9582649\n$90/hr-$40/hr  2.00  -6.336826 10.336826 0.9978591\n$60/hr-$50/hr -3.15 -11.486826  5.186826 0.9582649\n$70/hr-$50/hr -2.45 -10.786826  5.886826 0.9913695\n$80/hr-$50/hr -6.20 -14.536826  2.136826 0.3263918\n$90/hr-$50/hr -1.05  -9.386826  7.286826 0.9999826\n$70/hr-$60/hr  0.70  -7.636826  9.036826 0.9999993\n$80/hr-$60/hr -3.05 -11.386826  5.286826 0.9655088\n$90/hr-$60/hr  2.10  -6.236826 10.436826 0.9969834\n$80/hr-$70/hr -3.75 -12.086826  4.586826 0.8913952\n$90/hr-$70/hr  1.40  -6.936826  9.736826 0.9998433\n$90/hr-$80/hr  5.15  -3.186826 13.486826 0.5864831"
  },
  {
    "objectID": "post/2024-02-01_paying_llms/index.html#summary",
    "href": "post/2024-02-01_paying_llms/index.html#summary",
    "title": "The Elasticity of Labor for GPT-4",
    "section": "Summary",
    "text": "Summary\nGiven that there is no difference between the labor supplied (Tokens and Quality) by GPT4 and the hourly wage offered to it we can now see that the elasticity of labor is perfectly inelastic within the range of wages offered here. Sadly, bribery of this sort doesn’t work for GPT4 but perhaps with other models it does. It seems we will still have to threaten hostages in order to get increases in GPT4 to do what we ask."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html",
    "title": "Chess Ratings and Gender gaps",
    "section": "",
    "text": "Some have posited that there are differences between men and women which spring out of some genetic component somewhere deep in the chess parts of the brain that were responsible for early human victories over our greatest animal adversaries – allowing us to stall for time with a friendly game of chess until members of our tribe could come spear our adversaries while played for a draw. Thus chess skill would necessarily be unevenly distributed among genders much as hunting was before our species settled down to enjoy some agriculture. Still others believe that chess has an issue with too few women playing which is what explains the majority of the gap between the performance at the highest levels of chess. Due to a lack of recorded games from prehistory, I instead seek to show this second explanation is sufficient for explaining this variation between the two genders in terms of performance.\nOther authors have discussed this topic somewhat at length and my code here is adapted from theirs, however my work here covers the whole universe of FIDE ratings rather than only the Indian ratings which should allow us to determine if this sampling variability explanation is merely an Indian phenomena or a broader phenomena.\nDue to Covid-19 impacting the number of chess competitions held I use 2019 data and take the average of every players standard ratings for the year to avoid some of the individual variability in ratings throughout the year, this also helps to keep more players in our dataset as those with only one tournament to their name from March can be compared to those who have multiple tournaments from other months not including March."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#heavy-tails",
    "title": "Chess Ratings and Gender gaps",
    "section": "Heavy tails",
    "text": "Heavy tails\nIn order to demonstrate on a toy example we can sample from a heavy-tailed distribution and see how our maximums and minimums change as the sample size changes. Visually we can see how the coverage of the x-axis increases with the increase in sample size in the plots below.\n\n# Sample sizes to generate\nsample_sizes &lt;- c(100, 1000, 10000)\n\n# Initialize list to store extremes for later comparison\nextremes &lt;- list()\n\n# Generate samples and prepare for plotting\ndata &lt;- data.frame()\nfor (size in sample_sizes) {\n  sample &lt;- rcauchy(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size))\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nAnd we find stark differences between the maximums and minimums in our samples. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-6_10afb748666887fc248fe9c0897ee64b’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))\n}\n\nSample size 100: Min -27.65, Max 14.31\nSample size 1000: Min -138.42, Max 155.19\nSample size 10000: Min -1618.11, Max 3905.19\n\n:::"
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#normal-distribution",
    "title": "Chess Ratings and Gender gaps",
    "section": "Normal distribution",
    "text": "Normal distribution\nEven for distributions that are not heavy-tailed such as the normal distribution with a mean of 0 and standard deviation of 1 we can see that the maximum and minimum values are increasing in absolute value for increasing sample sizes such that the minimum and the maximum get further from the mean.\n\nextremes &lt;- list()\ndata &lt;- data.frame()\n\nfor (size in sample_sizes) {\n  sample &lt;- rnorm(size)\n  \n  # Calculate and store extremes\n  min_val &lt;- min(sample)\n  max_val &lt;- max(sample)\n  mean_val &lt;- mean(sample)\n  extremes[[length(extremes) + 1]] &lt;- c(min_val, max_val)\n  \n  # Prepare data for plotting\n  temp_data &lt;- data.frame(Value = sample, Size = factor(size), Mean = mean_val)\n  data &lt;- rbind(data, temp_data)\n}\n\n# Filter the data to remove extreme values for better visualization\ndata &lt;- subset(data, Value &gt; -25 & Value &lt; 25)\n\n# Plot using ggplot2\nggplot(data, aes(x=Value, fill=Size)) +\n  geom_histogram(bins=50, position=\"identity\", alpha=0.6) +\n  scale_fill_discrete(name=\"Sample Size\") +\n  labs(title=\"Comparison of Cauchy Samples\", x=\"Value\", y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\nIt doesn’t mean the samples are going to have their maximums or minimums deterministically increase with the increase of the sample size, but the probability that we sample a greater maximum or lower minimum increases as the sample size increases. ::: {.cell hash=‘chess_fide_cache/html/unnamed-chunk-8_20019db1b2dfe0d46cf6b51b7297c066’}\n# Print extremes\nfor (i in 1:length(sample_sizes)) {\n  size &lt;- sample_sizes[i]\n  min_val &lt;- extremes[[i]][1]\n  max_val &lt;- extremes[[i]][2]\n  cat(sprintf(\"Sample size %d: Min %.2f, Max %.2f\\n\", size, min_val, max_val))}\n\nSample size 100: Min -2.35, Max 2.75\nSample size 1000: Min -3.29, Max 3.72\nSample size 10000: Min -3.85, Max 3.85\n\n:::\nWith all of this background in mind we can proceed to our analysis of the realworld data."
  },
  {
    "objectID": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "href": "post/2024-02-28_chess_ratings_women/chess_fide.html#additional-notes-on-sampling-variability",
    "title": "Chess Ratings and Gender gaps",
    "section": "Additional notes on sampling variability",
    "text": "Additional notes on sampling variability\nWe can get a good baseline estimate for how often it is that a small sample has a larger maximum than a larger sample from the same distribution from the normal distribution. Here we can see that less than ten percent of the time we should expect the smaller sample to have a larger maximum even from the same distribution! The gendered differences in results we can observe in the chess world are probably coming from the fact that there are so many fewer women participating.\n\n# Sample sizes to compare (scaled down)\nsample_size_small &lt;- 2000\nsample_size_large &lt;- 24000\n\n# Initialize counter for tracking when the smaller sample has a larger maximum\ncounter &lt;- 0\n\n# Number of iterations\niterations &lt;- 10000\n\n# Loop for performing the comparison across iterations\nfor (i in 1:iterations) {\n  # Generate samples for each size\n  sample_small &lt;- rnorm(sample_size_small)\n  sample_large &lt;- rnorm(sample_size_large)\n  \n  # Compare max values and update counter if smaller sample has a larger max\n  if (max(sample_small) &gt; max(sample_large)) {\n    counter &lt;- counter + 1\n  }\n}\n\n# Calculate the percentage\npercentage &lt;- counter / iterations * 100\n\n# Print the result\ncat(\"Percentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000):\", percentage, \"%\\n\")\n\nPercentage of times the smaller sample (2000) has a larger maximum than the larger sample (24000): 7.86 %"
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html",
    "href": "post/2024-02-15_covid_causality/covid_causality.html",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "",
    "text": "In the following post I will be exploring the sometimes claimed effect of Covid-19 hospitalization on epigenetic aging. The core aim of this writing is to show that causal or experimental methods are required to determining if this claim is true or not and current research is insufficient for this task."
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#data-grouping",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Data Grouping",
    "text": "Data Grouping\nWe filter out the edge cases here and add some age group breaks to make it such that there are groups we can then use to see how much the implied age of a person is increased from Covid-19 infection. I also use this time to display the age and hospitalization table results, we can see a fairly strong age effect where the oldest groups even have more individuals hospitalized than not.\n\nsynthetic_data &lt;- synthetic_data %&gt;%\n  filter(age &lt;= 90) %&gt;% filter(age &gt; 10)\n\n# Define the breaks for the groups, extending the upper limit to cover all ages above 100\nbreaks &lt;- seq(10, 90, by = 10)\n\n# Ensure breaks are unique and in ascending order\nbreaks &lt;- unique(sort(breaks))\n\n# Define the labels for the groups\nlabels &lt;- c(\"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\")\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(age_group = cut(age, breaks = breaks, labels = labels))\n\n# Create a contingency table of hospitalization by age group\nhospitalization_by_age_group &lt;- table(synthetic_data$hospitalized, synthetic_data$age_group)\n\n# Print the table\nhospitalization_by_age_group\n\n   \n    10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90\n  0  1870  6637 15141 22896 21688 13389  1328   327\n  1   126   519  1339  2379  2724  2098  5261  1487\n\n\nWe repeat the above procedure but instead we group by health scores so further visualizations are improved.\n\n# Calculate the maximum value of health_scores\nmax_health_scores &lt;- max(synthetic_data$health_scores)\n\n# Calculate the maximum value for the breaks (nearest multiple of 10 above max_health_scores)\nmax_breaks &lt;- ceiling(max_health_scores / 10) * 10\n\nbreaks &lt;- seq(0, max(synthetic_data$health_scores), by = 10)  # Extend to cover the maximum value\n\n# Add an additional break at the end\nbreaks &lt;- c(breaks, max_breaks + 10)\n\n# Group the variable\nsynthetic_data &lt;- synthetic_data %&gt;%\n  mutate(health_groups = cut(health_scores, breaks = breaks, labels = c(\"0-10\", \"10-20\", \"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\", \"80-90\", \"90-100\", \"100-110\", \"110+\")))\n\nsynthetic_data &lt;- na.omit(synthetic_data)\n\n\nhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 1), ]\nnonhospitalized &lt;- synthetic_data[(synthetic_data$hospitalized == 0), ]"
  },
  {
    "objectID": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "href": "post/2024-02-15_covid_causality/covid_causality.html#sampling",
    "title": "We don’t know if Covid ages us epigenetically",
    "section": "Sampling",
    "text": "Sampling\nFinally I observed that there were similar distributions of age in the real data so in order to construct our sample to be similar we use stratified sampling to get equal groups of 50 for each age group in our synthetic data.\n\ntable(hospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n  126   519  1339  2379  2724  2098  5202  1390 \n\ntable(nonhospitalized$age_group)\n\n\n10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 \n 1870  6637 15141 22896 21688 13389  1314   311 \n\nstrata_sizes &lt;- rep(50, length(unique(synthetic_data$age_group)))\n\n# Perform stratified sampling\nstrata_sample_hosp &lt;- strata(data = hospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nstrata_sample_nonhosp &lt;- strata(data = nonhospitalized, \n                        stratanames = c(\"age_group\"), \n                        size = strata_sizes, \n                        method = \"srswor\")\n\nsampled_data_hosp &lt;- getdata(hospitalized, strata_sample_hosp)\nsampled_data_nonhosp &lt;- getdata(nonhospitalized, strata_sample_nonhosp)"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Resume",
    "section": "",
    "text": "This resume is available as a pdf.\nStatistician with 1 year of Data Analyst Experience working with marketing data to create new tools for canvassing managers, forecast future sales and marketing budget needs, and conduct analyses to optimize sales with a given budget. I have communicated both technical details to non-technical stakeholders and negotiated contracts from both the buy side with lead vendors and the sell side with homeowners looking to protect their largest investment. My work has allowed me to be engaged in forecasting and optimization of marketing budgets and the education I gained at KU Leuven has increased my statistical techne."
  },
  {
    "objectID": "resume/index.html#work-experience",
    "href": "resume/index.html#work-experience",
    "title": "Resume",
    "section": "Work Experience",
    "text": "Work Experience\n\n\n\nDaBella\n\nMarketing Data Analyst\n\n\n\nJuly 2019 - August 2020\n\n\n\nReduced excess spend by an estimated $250,000 annually by on-boarding new software vendors to provide TCPA compliance and screen out duplicate prospects\nOptimized canvasser deployment increasing leads by 5% using cluster analysis and predictive modeling\nRan and A/B tested email campaigns to previous customers using Mailchimp and Zapier, leading to 10% increase in customer referrals\nProvided up to date reporting on the performance of the marketing funnel\nConducted meetings with lead vendors to target specific zipcodes and product classes using prior analysis to optimize sales given our marketing budget\nConstructed marketing budget proposals given new branch openings, product mixes, historical sales, and ramp up factors\nSought out and partnered with new lead vendors, negotiated pricing, and monitored sales attributable to these vendors\n\n\n\n\n1st Security Bank\n\nFinance Intern\n\n\n\nSummer 2018\n\n\n\nMigrated financial data from balance sheet from excel sheets over to new platform for more accessible data formatting\nCreated a forecast of key figures over the next three years based on past performance data and branch opening schedule using a heuristic model\n\n\n\n\nDaBella\n\nSales Representative\n\n\n\nSummer 2017\n\n\n\nDemonstrated a 30% closing rate on sales presentations, showing strong technical communication skills and the ability to build trust and value with stakeholders.\nSold new roofs to customers with a total one-month best of over $100,000 of product sold\nHelped customers to complete financing applications for contracts averaging $21,500 each, communicating financing details and important facts to non-technical stakeholders.\nAssisted customers in successfully navigating financing applications for contracts averaging, effectively translating loan application requirements to homeowners."
  },
  {
    "objectID": "resume/index.html#education",
    "href": "resume/index.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\n\n\nKatholieke Universiteit of Leuven (KU Leuven)\n\nMSc Statistics\n\n\n\nFall 2023\n\n\n\n\n\nUniversity of Washington, Seattle\n\nB.S. Economics\n\n\n\n2019"
  },
  {
    "objectID": "resume/index.html#skills",
    "href": "resume/index.html#skills",
    "title": "Resume",
    "section": "Skills",
    "text": "Skills\n\nNetwork analysis, embeddings, clustering, data simulation, and NLP\nData analysis, visualization, modeling, regression, generalized linear models, hypothesis testing\nProficient in R, Python, tidyverse, git, SQL. Familiarity with AWS, Stan"
  }
]