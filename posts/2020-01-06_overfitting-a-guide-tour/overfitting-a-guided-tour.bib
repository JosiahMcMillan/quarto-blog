
@book{hastie_elements_2008,
	title = {The {Elements} of {Statistical} {Learning}},
	language = {en},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2008},
	file = {Hastie et al. - The Elements of Statistical Learning.pdf:C\:\\Users\\alex\\Zotero\\storage\\RFR6F63C\\Hastie et al. - The Elements of Statistical Learning.pdf:application/pdf}
}

@article{piantadosi_one_2018,
	title = {One parameter is always enough},
	volume = {8},
	issn = {2158-3226},
	url = {http://aip.scitation.org/doi/10.1063/1.5031956},
	doi = {10.1063/1.5031956},
	language = {en},
	number = {9},
	urldate = {2019-01-06},
	journal = {AIP Advances},
	author = {Piantadosi, Steven T.},
	month = sep,
	year = {2018},
	pages = {095118},
	file = {Piantadosi - 2018 - One parameter is always enough.pdf:C\:\\Users\\alex\\Zotero\\storage\\Q544L8P2\\Piantadosi - 2018 - One parameter is always enough.pdf:application/pdf}
}

@article{shao_linear_1993,
	title = {Linear {Model} {Selection} by {Cross}-validation},
	volume = {88},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476299},
	doi = {10.1080/01621459.1993.10476299},
	language = {en},
	number = {422},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Shao, Jun},
	month = jun,
	year = {1993},
	pages = {486--494},
	file = {Shao - 1993 - Linear Model Selection by Cross-validation.pdf:C\:\\Users\\alex\\Zotero\\storage\\Z8XP3799\\Shao - 1993 - Linear Model Selection by Cross-validation.pdf:application/pdf}
}

@article{belkin_reconciling_2018,
	title = {Reconciling modern machine learning and the bias-variance trade-off},
	url = {http://arxiv.org/abs/1812.11118},
	abstract = {The question of generalization in machine learning—how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample—is revisited in light of the recent breakthroughs in modern machine learning technology. The classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the ﬁt on the training sample reﬂects performance out-of-sample. However, it is now common practice to ﬁt highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. How can the classical understanding of generalization be reconciled with these observations from modern machine learning practice? In this paper, we bridge the two regimes by exhibiting a new “double descent” risk curve that extends the traditional U-shaped bias-variance curve beyond the point of interpolation. Speciﬁcally, the curve shows that as soon as the model complexity is high enough to achieve interpolation on the training sample—a point that we call the “interpolation threshold”—the risk of suitably chosen interpolating predictors from these models can, in fact, be decreasing as the model complexity increases, often below the risk achieved using non-interpolating models. The double descent risk curve is demonstrated for a broad range of models, including neural networks and random forests, and a mechanism for producing this behavior is posited.},
	language = {en},
	urldate = {2019-08-24},
	journal = {arXiv:1812.11118 [cs, stat]},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.11118},
	file = {Belkin et al. - 2018 - Reconciling modern machine learning and the bias-v.pdf:C\:\\Users\\alex\\Zotero\\storage\\RKE6YZTY\\Belkin et al. - 2018 - Reconciling modern machine learning and the bias-v.pdf:application/pdf}
}

@article{lei_cross-validation_2017,
	title = {Cross-{Validation} with {Confidence}},
	url = {http://arxiv.org/abs/1703.07904},
	abstract = {Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overﬁtting models, due to the ignorance of the uncertainty in the testing sample. We develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. When used for regularizing tuning parameter selection, the method can provide a further trade-oﬀ between prediction accuracy and model interpretability. We demonstrate the performance of the proposed method in several simulated and real data examples.},
	language = {en},
	urldate = {2019-09-22},
	journal = {arXiv:1703.07904 [stat]},
	author = {Lei, Jing},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07904},
	keywords = {done},
	annote = {Comment: 35 pages, 5 figures},
	file = {Lei - 2017 - Cross-Validation with Confidence.pdf:C\:\\Users\\alex\\Zotero\\storage\\CCSII9MQ\\Lei - 2017 - Cross-Validation with Confidence.pdf:application/pdf}
}

@techreport{wasserman_stat_nodate,
	title = {{STAT} 401 {Lecture} {Notes} 21: {Model} {Selection}},
	url = {http://www.stat.cmu.edu/~larry/=stat401/lecture-21.pdf},
	urldate = {2019-09-20},
	author = {Wasserman, Larry},
	file = {lecture-21.pdf:C\:\\Users\\alex\\Zotero\\storage\\DC3G62RX\\lecture-21.pdf:application/pdf}
}

@article{stone_cross-validatory_1974,
	title = {Cross-{Validatory} {Choice} and {Assessment} of {Statistical} {Predictions}},
	volume = {36},
	issn = {00359246},
	url = {http://doi.wiley.com/10.1111/j.2517-6161.1974.tb00994.x},
	doi = {10.1111/j.2517-6161.1974.tb00994.x},
	abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
	language = {en},
	number = {2},
	urldate = {2019-09-30},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Stone, M.},
	month = jan,
	year = {1974},
	pages = {111--133},
	file = {Stone - 1974 - Cross-Validatory Choice and Assessment of Statisti.pdf:C\:\\Users\\alex\\Zotero\\storage\\V2MPD78B\\Stone - 1974 - Cross-Validatory Choice and Assessment of Statisti.pdf:application/pdf}
}

@article{roberts_cross-validation_2017,
	title = {Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure},
	volume = {40},
	issn = {09067590},
	url = {http://doi.wiley.com/10.1111/ecog.02881},
	doi = {10.1111/ecog.02881},
	language = {en},
	number = {8},
	urldate = {2019-10-02},
	journal = {Ecography},
	author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
	month = aug,
	year = {2017},
	pages = {913--929},
	file = {Roberts et al. - 2017 - Cross-validation strategies for data with temporal.pdf:C\:\\Users\\alex\\Zotero\\storage\\TURHZ6ZL\\Roberts et al. - 2017 - Cross-validation strategies for data with temporal.pdf:application/pdf}
}

@article{arlot_survey_2010,
	title = {A survey of cross-validation procedures for model selection},
	volume = {4},
	issn = {1935-7516},
	url = {http://projecteuclid.org/euclid.ssu/1268143839},
	doi = {10.1214/09-SS054},
	abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
	language = {en},
	number = {0},
	urldate = {2019-10-06},
	journal = {Statistics Surveys},
	author = {Arlot, Sylvain and Celisse, Alain},
	year = {2010},
	pages = {40--79},
	file = {Arlot and Celisse - 2010 - A survey of cross-validation procedures for model .pdf:C\:\\Users\\alex\\Zotero\\storage\\4UFLTLNL\\Arlot and Celisse - 2010 - A survey of cross-validation procedures for model .pdf:application/pdf}
}

@article{dudoit_asymptotics_2005,
	title = {Asymptotics of cross-validated risk estimation in estimator selection and performance assessment},
	volume = {2},
	issn = {15723127},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1572312705000158},
	doi = {10.1016/j.stamet.2005.02.003},
	abstract = {Risk estimation is an important statistical question for the purposes of selecting a good estimator (i.e., model selection) and assessing its performance (i.e., estimating generalization error). This article introduces a general framework for cross-validation and derives distributional properties of cross-validated risk estimators in the context of estimator selection and performance assessment. Arbitrary classes of estimators are considered, including density estimators and predictors for both continuous and polychotomous outcomes. Results are provided for general full data loss functions (e.g., absolute and squared error, indicator, negative log density). A broad deﬁnition of cross-validation is used in order to cover leave-one-out cross-validation, V-fold cross-validation, Monte Carlo crossvalidation, and bootstrap procedures. For estimator selection, ﬁnite sample risk bounds are derived and applied to establish the asymptotic optimality of crossvalidation, in the sense that a selector based on a cross-validated risk estimator performs asymptotically as well as an optimal oracle selector based on the risk under the true, unknown data generating distribution. The asymptotic results are derived under the assumption that the size of the validation sets converges to inﬁnity and hence do not cover leave-one-out cross-validation. For performance assessment, cross-validated risk estimators are shown to be consistent and asymptotically linear for the risk under the true data generating distribution and conﬁdence intervals are derived for this unknown risk. Unlike previously published results, the theorems derived in this and our related articles apply to general data generating distributions, loss functions (i.e., parameters), estimators, and crossvalidation procedures.},
	language = {en},
	number = {2},
	urldate = {2019-10-06},
	journal = {Statistical Methodology},
	author = {Dudoit, Sandrine and van der Laan, Mark J.},
	month = jul,
	year = {2005},
	keywords = {to-read},
	pages = {131--154},
	file = {Dudoit and van der Laan - 2005 - Asymptotics of cross-validated risk estimation in .pdf:C\:\\Users\\alex\\Zotero\\storage\\KNXYEQFW\\Dudoit and van der Laan - 2005 - Asymptotics of cross-validated risk estimation in .pdf:application/pdf}
}

@article{wager_cross-validation_2019,
	title = {Cross-{Validation}, {Risk} {Estimation}, and {Model} {Selection}},
	url = {http://arxiv.org/abs/1909.11696},
	abstract = {Cross-validation is a popular non-parametric method for evaluating the accuracy of a predictive rule. The usefulness of cross-validation depends on the task we want to employ it for. In this note, I discuss a simple non-parametric setting, and ﬁnd that cross-validation is asymptotically uninformative about the expected test error of any given predictive rule, but allows for asymptotically consistent model selection. The reason for this phenomenon is that the leading-order error term of cross-validation doesn’t depend on the model being evaluated, and so cancels out when we compare two models. This note was prepared as a comment on a paper by Rosset and Tibshirani, forthcoming in the Journal of the American Statistical Association.},
	language = {en},
	urldate = {2019-10-18},
	journal = {arXiv:1909.11696 [stat]},
	author = {Wager, Stefan},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.11696},
	annote = {Comment: This note was prepared as a comment on a paper by Rosset and Tibshirani, forthcoming in the Journal of the American Statistical Association},
	file = {Wager - 2019 - Cross-Validation, Risk Estimation, and Model Selec.pdf:C\:\\Users\\alex\\Zotero\\storage\\EH6JX76C\\Wager - 2019 - Cross-Validation, Risk Estimation, and Model Selec.pdf:application/pdf}
}

@article{rosset_fixed-x_2017,
	title = {From {Fixed}-{X} to {Random}-{X} {Regression}: {Bias}-{Variance} {Decompositions}, {Covariance} {Penalties}, and {Prediction} {Error} {Estimation}},
	shorttitle = {From {Fixed}-{X} to {Random}-{X} {Regression}},
	url = {http://arxiv.org/abs/1704.08160},
	abstract = {In statistical prediction, classical approaches for model selection and model evaluation based on covariance penalties are still widely used. Most of the literature on this topic is based on what we call the “Fixed-X” assumption, where covariate values are assumed to be nonrandom. By contrast, it is often more reasonable to take a “Random-X” view, where the covariate values are independently drawn for both training and prediction. To study the applicability of covariance penalties in this setting, we propose a decomposition of Random-X prediction error in which the randomness in the covariates contributes to both the bias and variance components. This decomposition is general, but we concentrate on the fundamental case of least squares regression. We prove that in this setting the move from Fixed-X to Random-X prediction results in an increase in both bias and variance. When the covariates are normally distributed and the linear model is unbiased, all terms in this decomposition are explicitly computable, which yields an extension of Mallows’ Cp that we call RCp. RCp also holds asymptotically for certain classes of nonnormal covariates. When the noise variance is unknown, plugging in the usual unbiased estimate leads to an approach that we call RCp, which is closely related to Sp (Tukey 1967), and GCV (Craven and Wahba 1978). For excess bias, we propose an estimate based on the “shortcut-formula” for ordinary cross-validation (OCV), resulting in an approach we call RCp+. Theoretical arguments and numerical simulations suggest that RCp+ is typically superior to OCV, though the diﬀerence is small. We further examine the Random-X error of other popular estimators. The surprising result we get for ridge regression is that, in the heavily-regularized regime, Random-X variance is smaller than Fixed-X variance, which can lead to smaller overall Random-X error.},
	language = {en},
	urldate = {2019-10-18},
	journal = {arXiv:1704.08160 [stat]},
	author = {Rosset, Saharon and Tibshirani, Ryan J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.08160},
	file = {Rosset and Tibshirani - 2017 - From Fixed-X to Random-X Regression Bias-Variance.pdf:C\:\\Users\\alex\\Zotero\\storage\\N5652IXC\\Rosset and Tibshirani - 2017 - From Fixed-X to Random-X Regression Bias-Variance.pdf:application/pdf}
}

@book{massart_concentration_2003,
	title = {Concentration {Inequalities} and {Model} {Selection}},
	url = {http://www.cmap.polytechnique.fr/~merlet/articles/probas_massart_stf03.pdf},
	urldate = {2019-10-21},
	author = {Massart, Pascal},
	year = {2003},
	file = {probas_massart_stf03.pdf:C\:\\Users\\alex\\Zotero\\storage\\JVUVSF8A\\probas_massart_stf03.pdf:application/pdf}
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1507.04544},
	doi = {10.1007/s11222-016-9696-4},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
	number = {5},
	urldate = {2020-01-01},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	month = sep,
	year = {2017},
	note = {arXiv: 1507.04544},
	pages = {1413--1432},
	file = {arXiv Fulltext PDF:C\:\\Users\\alex\\Zotero\\storage\\MXRDR4RR\\Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alex\\Zotero\\storage\\VPJGIDUD\\1507.html:text/html}
}

@article{bengio_no_2004,
	title = {No {Unbiased} {Estimator} of the {Variance} of {K}-{Fold} {Cross}-{Validation}},
	abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don’t take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is conﬁrmed by numerical experiments in which the three components of the variance are compared when the difﬁculty of the learning problem and the number of folds are varied.},
	language = {en},
	author = {Bengio, Yoshua and Grandvalet, Yves},
	year = {2004},
	pages = {17},
	file = {Bengio and Grandvalet - No Unbiased Estimator of the Variance of K-Fold Cr.pdf:C\:\\Users\\alex\\Zotero\\storage\\26BNUQIQ\\Bengio and Grandvalet - No Unbiased Estimator of the Variance of K-Fold Cr.pdf:application/pdf}
}

@article{benavoli_time_2017,
	title = {Time for a {Change}: a {Tutorial} for {Comparing} {Multiple} {Classiﬁers} {Through} {Bayesian} {Analysis}},
	abstract = {The machine learning community adopted the use of null hypothesis signiﬁcance testing (NHST) in order to ensure the statistical validity of results. Many scientiﬁc ﬁelds however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, oﬀer better—more sound and useful—alternatives for it.},
	language = {en},
	author = {Benavoli, Alessio and Corani, Giorgio and Demˇsar, Janez and Zaﬀalon, Marco},
	year = {2017},
	pages = {36},
	file = {Benavoli et al. - Time for a Change a Tutorial for Comparing Multip.pdf:C\:\\Users\\alex\\Zotero\\storage\\VY47PS4H\\Benavoli et al. - Time for a Change a Tutorial for Comparing Multip.pdf:application/pdf}
}
