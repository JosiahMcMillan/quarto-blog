
@book{shalizi_advanced_2018,
	title = {Advanced {Data} {Analysis} from an {Elementary} {Point} of {View}},
	url = {https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf},
	language = {en},
	author = {Shalizi, Cosma Rohilla},
	year = {2018},
	file = {Shalizi - Advanced Data Analysis from an Elementary Point of.pdf:C\:\\Users\\alex\\Zotero\\storage\\4LYE8CXZ\\Shalizi - Advanced Data Analysis from an Elementary Point of.pdf:application/pdf}
}

@book{shao_mathematical_2003,
	address = {New York},
	edition = {2nd ed},
	series = {Springer texts in statistics},
	title = {Mathematical statistics},
	isbn = {978-0-387-95382-3},
	language = {en},
	publisher = {Springer},
	author = {Shao, Jun},
	year = {2003},
	file = {Shao - 2003 - Mathematical statistics.pdf:C\:\\Users\\alex\\Zotero\\storage\\ZUXEWAAD\\Shao - 2003 - Mathematical statistics.pdf:application/pdf}
}

@article{baumgaertner_model-centric_2018,
	title = {A {Model}-{Centric} {Analysis} of {Openness}, {Replication}, and {Reproducibility}},
	url = {http://arxiv.org/abs/1811.04525},
	abstract = {The literature on the reproducibility crisis presents several putative causes for the proliferation of irreproducible results, including HARKing, p-hacking and publication bias. Without a theory of reproducibility, however, it is diﬃcult to determine whether these putative causes can explain most irreproducible results. Drawing from an historically informed conception of science that is open and collaborative, we identify the components of an idealized experiment and analyze these components as a precursor to develop such a theory. Openness, we suggest, has long been intuitively proposed as a solution to irreproducibility. However, this intuition has not been validated in a theoretical framework. Our concern is that the under-theorizing of these concepts can lead to ﬂawed inferences about the (in)validity of experimental results or integrity of individual scientists. We use probabilistic arguments and examine how openness of experimental components relates to reproducibility of results. We show that there are some impediments to obtaining reproducible results that precede many of the causes often cited in literature on the reproducibility crisis. For example, even if erroneous practices such as HARKing, p-hacking, and publication bias were absent at the individual and system level, reproducibility may still not be guaranteed.},
	language = {en},
	urldate = {2018-11-16},
	journal = {arXiv:1811.04525 [stat]},
	author = {Baumgaertner, Bert and Devezer, Berna and Buzbas, Erkan O. and Nardin, Luis G.},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.04525},
	keywords = {done},
	annote = {Comment: 22 pages, 2 figures. All authors contributed equally. Under review},
	file = {Baumgaertner et al. - 2018 - A Model-Centric Analysis of Openness, Replication,.pdf:C\:\\Users\\alex\\Zotero\\storage\\WUDFM8HD\\Baumgaertner et al. - 2018 - A Model-Centric Analysis of Openness, Replication,.pdf:application/pdf}
}

@book{harrell_regression_2015,
	address = {Cham Heidelberg New York},
	edition = {Second edition},
	series = {Springer series in statistics},
	title = {Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis},
	isbn = {978-3-319-19424-0 978-3-319-19425-7},
	shorttitle = {Regression modeling strategies},
	language = {en},
	publisher = {Springer},
	author = {Harrell, Frank E.},
	year = {2015},
	note = {OCLC: 922304565},
	keywords = {slow-burn},
	annote = {RMS
 },
	file = {Harrell - 2015 - Regression modeling strategies with applications .pdf:C\:\\Users\\alex\\Zotero\\storage\\JNAT6CDZ\\Harrell - 2015 - Regression modeling strategies with applications .pdf:application/pdf}
}

@article{silberzahn_many_2018,
	title = {Many {Analysts}, {One} {Data} {Set}: {Making} {Transparent} {How} {Variations} in {Analytic} {Choices} {Affect} {Results}},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Many {Analysts}, {One} {Data} {Set}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
	language = {en},
	number = {3},
	urldate = {2019-11-06},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	month = sep,
	year = {2018},
	pages = {337--356},
	file = {Silberzahn et al. - 2018 - Many Analysts, One Data Set Making Transparent Ho.pdf:C\:\\Users\\alex\\Zotero\\storage\\S2QELF2Q\\Silberzahn et al. - 2018 - Many Analysts, One Data Set Making Transparent Ho.pdf:application/pdf}
}

@article{navarro_between_2018,
	title = {Between the devil and the deep blue sea: {Tensions} between scientific judgement and statistical model selection},
	url = {https://link.springer.com/article/10.1007/s42113-018-0019-z},
	abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leave-one-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientiﬁc perspective. What goal does model selection serve when all models are 6 known to be systematically wrong? How might “toy problems” tell a misleading story? How does the scientiﬁc goal of explanation align with (or diﬀer from) traditional statistical concerns? I do not oﬀer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
	language = {en},
	author = {Navarro, Danielle J},
	year = {2018},
	pages = {12},
	file = {Navarro - Between the devil and the deep blue sea Tensions .pdf:C\:\\Users\\alex\\Zotero\\storage\\5CWKUUC8\\Navarro - Between the devil and the deep blue sea Tensions .pdf:application/pdf}
}

@article{van_calster_calibration_2016,
	title = {A calibration hierarchy for risk models was defined: from utopia to empirical data},
	volume = {74},
	issn = {08954356},
	shorttitle = {A calibration hierarchy for risk models was defined},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435615005818},
	doi = {10.1016/j.jclinepi.2015.12.005},
	abstract = {Objective: Calibrated risk models are vital for valid decision support. We deﬁne four levels of calibration and describe implications for model development and external validation of predictions. Study Design and Setting: We present results based on simulated data sets.
Results: A common deﬁnition of calibration is ‘‘having an event rate of R\% among patients with a predicted risk of R\%,’’ which we refer to as ‘‘moderate calibration.’’ Weaker forms of calibration only require the average predicted risk (mean calibration) or the average prediction effects (weak calibration) to be correct. ‘‘Strong calibration’’ requires that the event rate equals the predicted risk for every covariate pattern. This implies that the model is fully correct for the validation setting. We argue that this is unrealistic: the model type may be incorrect, the linear predictor is only asymptotically unbiased, and all nonlinear and interaction effects should be correctly modeled. In addition, we prove that moderate calibration guarantees nonharmful decision making. Finally, results indicate that a ﬂexible assessment of calibration in small validation data sets is problematic.
Conclusion: Strong calibration is desirable for individualized decision support but unrealistic and counter productive by stimulating the development of overly complex models. Model development and external validation should focus on moderate calibration. Ó 2016 Elsevier Inc. All rights reserved.},
	language = {en},
	urldate = {2020-02-02},
	journal = {Journal of Clinical Epidemiology},
	author = {Van Calster, Ben and Nieboer, Daan and Vergouwe, Yvonne and De Cock, Bavo and Pencina, Michael J. and Steyerberg, Ewout W.},
	month = jun,
	year = {2016},
	keywords = {done},
	pages = {167--176},
	file = {Van Calster et al. - 2016 - A calibration hierarchy for risk models was define.pdf:C\:\\Users\\alex\\Zotero\\storage\\HJGHFZF3\\Van Calster et al. - 2016 - A calibration hierarchy for risk models was define.pdf:application/pdf}
}

@techreport{devezer_case_2020,
	type = {preprint},
	title = {The case for formal methodology in scientific reform},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.04.26.048306},
	abstract = {Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, some of these reform attempts suffer from the same mistakes and over-generalizations they purport to address. Considering the costs of allowing false claims to become canonized, we argue for more rigor and nuance in methodological reform. By way of example, we present a formal analysis of three common claims in the metascientific literature: (a) That reproducibility is the cornerstone of science; (b) That data must not be used twice in any analysis; and (c) That exploratory projects lead to bad statistical inference. We show that none of these three claims are correct in general and we explore when they do and do not hold.},
	language = {en},
	urldate = {2020-04-28},
	institution = {Scientific Communication and Education},
	author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Buzbas, Erkan Ozge},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.26.048306},
	file = {Devezer et al. - 2020 - The case for formal methodology in scientific refo.pdf:C\:\\Users\\alex\\Zotero\\storage\\PTM8DDEF\\Devezer et al. - 2020 - The case for formal methodology in scientific refo.pdf:application/pdf}
}

@book{mayo_statistical_2018,
	edition = {1},
	title = {Statistical {Inference} as {Severe} {Testing}: {How} to {Get} {Beyond} the {Statistics} {Wars}},
	isbn = {978-1-107-28618-4 978-1-107-05413-4 978-1-107-66464-7},
	shorttitle = {Statistical {Inference} as {Severe} {Testing}},
	url = {https://www.cambridge.org/core/product/identifier/9781107286184/type/book},
	language = {en},
	urldate = {2020-04-29},
	publisher = {Cambridge University Press},
	author = {Mayo, Deborah G.},
	month = sep,
	year = {2018},
	doi = {10.1017/9781107286184},
	file = {Mayo - 2018 - Statistical Inference as Severe Testing How to Ge.pdf:C\:\\Users\\alex\\Zotero\\storage\\ALRJTX79\\Mayo - 2018 - Statistical Inference as Severe Testing How to Ge.pdf:application/pdf}
}

@article{ghosh_ancillary_nodate,
	title = {Ancillary {Statistics}: {A} {Review}},
	abstract = {In a parametric statistical model, a function of the data is said to be ancillary if its distribution does not depend on the parameters in the model. The concept of ancillary statistics is one of R. A. Fisher’s fundamental contributions to statistical inference. Fisher motivated the principle of conditioning on ancillary statistics by an argument based on relevant subsets, and by a closely related argument on recovery of information. Conditioning can also be used to reduce the dimension of the data to that of the parameter of interest, and conditioning on ancillary statistics ensures that no information about the parameter is lost in this reduction.},
	language = {en},
	author = {Ghosh, M and Reid, N and Fraser, D A S},
	pages = {24},
	file = {Ghosh et al. - ANCILLARY STATISTICS A REVIEW.pdf:C\:\\Users\\alex\\Zotero\\storage\\MHHF4SG4\\Ghosh et al. - ANCILLARY STATISTICS A REVIEW.pdf:application/pdf}
}

@phdthesis{darnieder_bayesian_2011,
	title = {Bayesian {Methods} for {Data}-{Dependent} {Priors}},
	url = {https://etd.ohiolink.edu/!etd.send_file?accession=osu1306344172&disposition=inline},
	urldate = {2020-04-29},
	author = {Darnieder, William Francis},
	year = {2011},
	file = {!etd.pdf:C\:\\Users\\alex\\Zotero\\storage\\NCRDD3BV\\!etd.pdf:application/pdf}
}

@article{gelman_statistical_2014,
	title = {The {Statistical} {Crisis} in {Science}},
	url = {http://www.stat.columbia.edu/~gelman/research/published/ForkingPaths.pdf},
	urldate = {2020-05-01},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2014},
	file = {ForkingPaths.pdf:C\:\\Users\\alex\\Zotero\\storage\\MTQTAXTS\\ForkingPaths.pdf:application/pdf}
}