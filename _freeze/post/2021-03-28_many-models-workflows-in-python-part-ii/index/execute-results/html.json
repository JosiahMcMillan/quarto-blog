{
  "hash": "640d211697c0f4400e6af740a1e58b1d",
  "result": {
    "markdown": "---\ntitle: many models workflows in python ii\nsubtitle: |\n  a tidymodels workflow in python using list columns in pandas dataframes, now with hyperparameter sweep and parallelism\ndate: '2021-03-28'\ncategories:\n  - python\n  - workflow\n  - tidymodels\n---\n\nIn this followup to [my earlier post](../2020-08-25_many-models-workflows-in-python-part-i/) on modeling workflows in Python, I demonstrate how to integrate sample splitting, parallel processing, exception handling and caching into many-models workflows. I also discuss some differences between exploration/inference-centric workflows and tuning-centric workflows.\n\n## Motivating example\n\nWe will work with the [Palmer Penguin](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-07-28) dataset, which contains various biological measures on three species of penguins. Our goal will be to cluster the penguins into groups that correspond to their species using their bill length, bill depth, flipper length and body mass. We'll use a Gaussian Mixture Model for clustering. Caveat: my goal here is to demonstrate a workflow, not good science.\n\nWe'll start by pulling the data into Python and taking a look.\n\n::: {.cell .column-body-outset execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\npenguins = pd.read_csv('https://tinyurl.com/palmerpenguincsv').dropna()\n\nclustering_features = [\n    'bill_length_mm',\n    'bill_depth_mm',\n    'flipper_length_mm',\n    'body_mass_g'\n]\n\npenguins\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190.0</td>\n      <td>3650.0</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>Chinstrap</td>\n      <td>Dream</td>\n      <td>55.8</td>\n      <td>19.8</td>\n      <td>207.0</td>\n      <td>4000.0</td>\n      <td>male</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>340</th>\n      <td>Chinstrap</td>\n      <td>Dream</td>\n      <td>43.5</td>\n      <td>18.1</td>\n      <td>202.0</td>\n      <td>3400.0</td>\n      <td>female</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>Chinstrap</td>\n      <td>Dream</td>\n      <td>49.6</td>\n      <td>18.2</td>\n      <td>193.0</td>\n      <td>3775.0</td>\n      <td>male</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>Chinstrap</td>\n      <td>Dream</td>\n      <td>50.8</td>\n      <td>19.0</td>\n      <td>210.0</td>\n      <td>4100.0</td>\n      <td>male</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>Chinstrap</td>\n      <td>Dream</td>\n      <td>50.2</td>\n      <td>18.7</td>\n      <td>198.0</td>\n      <td>3775.0</td>\n      <td>female</td>\n      <td>2009</td>\n    </tr>\n  </tbody>\n</table>\n<p>333 rows Ã— 8 columns</p>\n</div>\n```\n:::\n:::\n\n\nOur next step is to define a helper function to fit GMM estimators for us. I'm going to use `sklearn`'s [GMM implementation](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html), which uses the EM algorithm for estimation. The estimator has two key hyperparameters that we'll want to explore:\n\n1. `n_components`: the number of clusters to find\n2. `covariance_type`: the assumed structure of the covariances of each cluster\n\nNothing in the code turns on these hyperparameters dramatically, but it probably worth reading the documentation linked above if haven't seen GMMs before.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import List\nfrom sklearn.mixture import GaussianMixture\n\n# some comments:\n# \n# 1. I highly recommend using type hints, since it'll help you keep\n#   keep track of what's happening in each column of the eventual\n#   data frame of models\n#\n# 2. Passing hyperparameters as keyword arguments is a generally usefu\n#   pattern that make it easy to swap out estimators without having\n#   to modify a lot of code that relies on estimators having\n#   particular hyperparameters.\n#\n# 3. Here I specify the columns of the data to use for modeling by\n#   passing a list of column names. This is extremely limited and\n#   will result in technical debt if you need to do any sort of \n#   interesting work to build a design matrix. Alternatives here\n#   might be a list of sklearn Transformers or a Patsy formula.\n#   In R you would use a formula object or perhaps a recipe\n#   from the recipes packages.\n#\n\ndef fit_gmm(\n    train : pd.DataFrame,\n    features : List[chr],\n    **hyperparameters\n) -> GaussianMixture:\n    \n    # the hyperparameters are: n_components: int, covariance_type: str\n    gmm = GaussianMixture(\n        **hyperparameters,\n        n_init=5,\n        random_state=27\n    )\n    \n    gmm.fit(train[features])\n    return gmm\n\nfit_gmm(\n    penguins,\n    features=clustering_features,\n    n_components=3,\n    covariance_type='full'\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nGaussianMixture(n_components=3, n_init=5, random_state=27)\n```\n:::\n:::\n\n\nNow let's build some infrastructure so that we can use sample splitting to evaluate how well our models perform out of sample. My approach below is heavily inspired by [rsample](https://rsample.tidymodels.org/). At the end of this post I have some longer commentary about this choice and why I haven't used `sklearn` built-in resampling infrastructure.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\nclass VFoldCV:\n    \n    def __init__(self, data : pd.DataFrame, num_folds : int = 10):\n        \n        self.data = data\n        self.num_folds = num_folds\n        permuted_indices = np.random.permutation(len(data))\n        self.indices = np.array_split(permuted_indices, num_folds)\n        \n    def __iter__(self):\n        for test_indices in self.indices:\n            test = self.data.iloc[test_indices]\n            train = self.data[~self.data.index.isin(test_indices)]\n            yield train, test\n\nresamples = VFoldCV(penguins, num_folds=5)\n\nfor fold_index, (train, test) in enumerate(resamples):\n    print(f\"Fold {fold_index} has {len(train)} train and {len(test)} test observations.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFold 0 has 267 train and 67 test observations.\nFold 1 has 267 train and 67 test observations.\nFold 2 has 267 train and 67 test observations.\nFold 3 has 272 train and 66 test observations.\nFold 4 has 270 train and 66 test observations.\n```\n:::\n:::\n\n\nNow that we have cross validation folds, we want to create a grid of parameters to explore over. Here we leverage `sklearn`'s [ParameterGrid](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html) object, which basically takes a Cartesian product that we can iterate over, as well as coerce to a data frame.\n\n::: {.cell .column-body-outset execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.model_selection import ParameterGrid\n\nparam_grid = ParameterGrid({\n    'n_components': range(2, 11),\n    'covariance_type': ['full', 'tied', 'diag', 'spherical']\n})\n\nmodel_grid = pd.DataFrame(param_grid)\nmodel_grid.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>covariance_type</th>\n      <th>n_components</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>full</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>full</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>full</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>full</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>full</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we are going to do something a little tricky. We are going to create a new column of `model_grid` that contains the results from fitting a GMM to each split from the CV object. In particular, each element in this column will itself be a list with `num_folds` elements. That is, we will create a list-column `cv_fits` were each element of `model_grid.cv_fits` is itself a list. To work with these structures it is easiest to define a helper function that fits a GMM to each CV split for a single combination of hyperparameters. \n\n::: {.cell .column-body-outset execution_count=5}\n``` {.python .cell-code}\ndef fit_gmm_vfold(\n    folds: VFoldCV, \n    **hyperparameters\n) -> List[GaussianMixture]:\n    \n    fits = [\n        fit_gmm(train, **hyperparameters)\n        for train, test in folds \n    ]\n    return fits\n\nmodel_grid['cv_fits'] = [\n    fit_gmm_vfold(\n        resamples,\n        features=clustering_features,\n        **hyperparameters\n    )\n    for hyperparameters in param_grid\n]\n\nmodel_grid.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>covariance_type</th>\n      <th>n_components</th>\n      <th>cv_fits</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>full</td>\n      <td>2</td>\n      <td>[GaussianMixture(n_components=2, n_init=5, ran...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>full</td>\n      <td>3</td>\n      <td>[GaussianMixture(n_components=3, n_init=5, ran...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>full</td>\n      <td>4</td>\n      <td>[GaussianMixture(n_components=4, n_init=5, ran...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>full</td>\n      <td>5</td>\n      <td>[GaussianMixture(n_components=5, n_init=5, ran...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>full</td>\n      <td>6</td>\n      <td>[GaussianMixture(n_components=6, n_init=5, ran...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Note**: To get our cross validated fits, we iterated over `param_grid` and stored results in `model_grid`. It's essential that the row indices match up between these objects. Here they match by construction, but be careful if you start to play with this structure. For example, one alternative (and frequently convenient approach) here is to create a `model_grid` object based on `itertools.product(param_grid, resamples)` rather than just `param_grid`. This avoids nesting lists in list-columns, at the cost of inefficiency in terms of storage. This route is more fiddly than it looks.\n\nAnyway, now that we have training fits, we want to compute out of sample performance estimates. In our case, we'll use a measure of clustering quality known at the Adjusted Rand Score. Again we use nested list comprehensions to get out of sample estimates for all CV splits.\n\n::: {.cell .column-body-outset execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import adjusted_rand_score\n\ndef oos_vfold_ars(\n    folds: VFoldCV, \n    fits: List[GaussianMixture],\n    features : List[chr]\n) -> List[float]:\n    \n    ars = [\n        adjusted_rand_score(\n            test.species.astype('category').values.codes,\n            fit.predict(test[features])\n        ) for (_, test), fit in zip(folds, fits)\n    ]\n    \n    return ars\n\nmodel_grid['cv_ars'] = [\n    oos_vfold_ars(\n        resamples, \n        fits, \n        features=clustering_features\n    )\n    for fits in model_grid.cv_fits\n]\n\nmodel_grid.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>covariance_type</th>\n      <th>n_components</th>\n      <th>cv_fits</th>\n      <th>cv_ars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>full</td>\n      <td>2</td>\n      <td>[GaussianMixture(n_components=2, n_init=5, ran...</td>\n      <td>[0.5836118654689785, 0.6393736747675747, 0.756...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>full</td>\n      <td>3</td>\n      <td>[GaussianMixture(n_components=3, n_init=5, ran...</td>\n      <td>[0.5374807204772191, 0.8828965612773291, 0.634...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>full</td>\n      <td>4</td>\n      <td>[GaussianMixture(n_components=4, n_init=5, ran...</td>\n      <td>[0.8165589464394102, 0.7493049764689503, 0.766...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>full</td>\n      <td>5</td>\n      <td>[GaussianMixture(n_components=5, n_init=5, ran...</td>\n      <td>[0.568213184510058, 0.6712028055910916, 0.6516...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>full</td>\n      <td>6</td>\n      <td>[GaussianMixture(n_components=6, n_init=5, ran...</td>\n      <td>[0.6975236240511652, 0.5929008009414364, 0.637...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we can compare models by expanding the `cv_ars` column and comparing out of sample performance measures. Here I'm just going to visualize the results, but you could also fit a mixed model of the form `adjusted_rand_index ~ hyperparameter_combination + (1|cv_fold)` or something along those lines if you want to be fancier.\n\nIt's worth pausing a moment to comment on the `cv_ars` column. In my previous post, I introduced a helper function `unnest()` that you can use to expand a list-column that contains data frames. That `unnest()` function does not work with list-columns of lists, and instead we use the `pandas.Series.explode()` method, which is like an extremely limited version of `tidyr::unnest()`. Importantly, `pandas.Series.explode()` is not very clever about types, so you may need to coerce types after expanding list columns, as I do below.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncross_validated_ars = (\n    model_grid\n    .explode('cv_ars')\n)\n\nprint(cross_validated_ars.dtypes, '\\n')\n\ncross_validated_ars[\"cv_ars\"] = pd.to_numeric(cross_validated_ars[\"cv_ars\"])\n\nprint(cross_validated_ars.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncovariance_type    object\nn_components        int64\ncv_fits            object\ncv_ars             object\ndtype: object \n\ncovariance_type     object\nn_components         int64\ncv_fits             object\ncv_ars             float64\ndtype: object\n```\n:::\n:::\n\n\nNow that we have numeric data we can plot it.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplot = sns.lineplot(\n    data=cross_validated_ars.reset_index(),\n    x='n_components', y='cv_ars',\n    hue='covariance_type', errorbar=None\n)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=589 height=429}\n:::\n:::\n\n\nHigher Adjusted Rand Scores are better, so this initial pass of modeling suggests that we should use three clusters. This is reassuring, since we know there are three species of penguin in the dataset!\n\nNote that I'm taking a very informal approach to model selection here and a real analysis should be more careful (in slightly more detail: the difference between CV for model selection and CV for risk estimation is germane here).\n\nThere are other approaches to model selection that we could take. For example, we could compare across hyperparameters with in-sample BIC, which is the approach taken in the R package `mclust`. We'll do this briefly for illustrative purposes, and start incorporating some fancier tools along the way:\n\n1. **Parallelization**  (via `joblib`): Modeling fitting across hyperparameter is embarassingly parallel, so this will make things faster.\n\n2. **Exception handling** (via a function decorator): In practice, lots of estimators will run into numerical or data issues that you can safely ignore. In particular, when model fitting fails, it's okay fine to just return a `np.nan` (in R, an `NA`-like object) and use the results from whatever estimators did succeed, propagating the `np.nan` forward.\n\n3. **Caching** (via `joblib`): In interactive workflows it's easy to kill your Jupyter kernel or crash your computer or do something dumb that forces you to restart your Python session. It is really frustrating to have to refit your models everytime this happens. Caching models to disk (also known as memoization) prevents you from having to re-run all your models everytime you break Jupyter, etc, etc. If you use caching, you will have to be careful about cache invalidation, which is one of the most notoriously difficult problems in computing.\n\nFull disclosure: I did not get parallelization and caching to work together for this post ([bug report](https://github.com/joblib/joblib/issues/1169)), but it has worked for me in the past. I'm going to include some commented out caching code in the following for you to play with as you see fit. See the `joblib` [documentation](https://joblib.readthedocs.io/en/latest/auto_examples/nested_parallel_memory.html#sphx-glr-auto-examples-nested-parallel-memory-py) for more examples of how to combine parallel mapping with caching.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom joblib import Parallel, delayed  # parallelism\nfrom functools import wraps           # nicer exception handling\nfrom joblib import Memory             # caching\n\n# setup caching\ncache_dir = \"./model_cache\"\nmemory = Memory(cache_dir, verbose=0)\n\n# NOTE: here `n_components` will exceed the number\n# of observations for some hyperparameter combinations\n# which will cause errors. here i'm artificially introducing\n# errors; in real life you'll have to supply your own\n\nfancy_param_grid = ParameterGrid({\n    'n_components': range(2, 400),\n    'covariance_type': ['full', 'tied', 'diag', 'spherical']\n})\n\nfancy_model_grid = pd.DataFrame(fancy_param_grid)\nfancy_model_grid.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>covariance_type</th>\n      <th>n_components</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1587</th>\n      <td>spherical</td>\n      <td>395</td>\n    </tr>\n    <tr>\n      <th>1588</th>\n      <td>spherical</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>1589</th>\n      <td>spherical</td>\n      <td>397</td>\n    </tr>\n    <tr>\n      <th>1590</th>\n      <td>spherical</td>\n      <td>398</td>\n    </tr>\n    <tr>\n      <th>1591</th>\n      <td>spherical</td>\n      <td>399</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we define our function decorator for handling exceptions in list comprehensions. I'm basically copying [`purrr::safely()`](https://purrr.tidyverse.org/reference/safely.html) from R here. Exception handling is essential because it's painful when you fit 9 out of 10 models using a list-comprehension, and then the final model fails and you have to re-run the first 9 models (caching also helps with this when you can get it to work).\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef safely(func, otherwise=np.nan):\n    # traditionally you'd use @wraps here,\n    # but this seems to interact poorly with parallelism\n    # for the time being\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except:\n            return otherwise\n    return wrapper\n\n# if bad models become np.nan, downstream calls that use models\n# need to handle np.nan input. any easy way to do this is use\n# @safely again to just continue propagate np.nan\n\n@safely\ndef get_bic(fit):\n    return fit.bic(penguins[clustering_features])\n    \nfit_safely = safely(fit_gmm)\n\n# to combine parallelism and caching, use something like:\n# fit_cached = memory.cache(fit_safely)\n\n# create persistent pool of workers and use them for fitting\nwith Parallel(n_jobs=20) as parallel:\n    \n    fancy_model_grid['fit'] = parallel(\n        delayed(fit_safely)(penguins, clustering_features, **hyperparameters)\n        for hyperparameters in fancy_param_grid\n    )\n    \n    fancy_model_grid['bic'] = parallel(\n        delayed(get_bic)(fit) for fit in fancy_model_grid.fit\n    )\n```\n:::\n\n\nNow we can compare BIC across our absurdly large number of models. `seaborn` will automatically drop `np.nan()` values, so our `@safely` approach plays very nice here.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nplot = sns.lineplot(\n    data=fancy_model_grid,\n    x='n_components', y='bic', hue='covariance_type'\n)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=610 height=429}\n:::\n:::\n\n\nIn this plot lower BIC is better. We see than this in-sample approach to model selection prefers models with as many clusters as observations. This would clearly correspond to overfitting in our case.\n\n## Returning to the big picture\n\nAt this point, you might be starting to wonderful why you would want to use a many-models workflow at all. All I've done in this blog post is some grid searching over hyperparameters, and we could easily recreate everything in this blog post with `GridSearchCV` and some custom scoring functions.\n\nThe problem is that `GridSeachCV` (and other related implementations) are not that flexible. Last summer, I had (1) custom data splitting, (2) custom estimators, and (3) wanted to compute high dimensional summaries for each model I fit. And I want to save my fits so that I could investigate them individually, rather than throwing them out as soon as I know about their predictive performance. `GridSearchCV` just can't handle this, and, by and large, the data science ecosystem in Python doesn't either.\n\nWe can imagine that there are two contrasting modeling workflows. First, there's a many-models workflow, which is especially appropriate for research, inference and sensitivity analysis. It's interactive, and not particularly focused on compute efficiency. Then, there's a hyperparameter tuning workflow, which has a simple goal: predict well. Tools for tuning workflows are typically developed by machine learners who  want to train models as computationally efficiently as possible. Because these practitioners emphasize prediction accuracy over all else, it can be hard to re-purpose tools for tuning workflows to learn about models beyond their predictive accuracy[^tidymodels_aside].\n\nHopefully this post highlights some design patterns you can use when existing infrastructure isn't a good fit for your Python modeling needs. I'm very curious to hear about other approaches people take this problem.\n\n[^tidymodels_aside]: **Aside about `tidymodels`**: Early work in the `tidymodels` ecosystem focused on low level infrastructure that facilitated many-models workflows. I still use this infrastructure a lot, especially combined with [`targets`](https://docs.ropensci.org/targets/), which is a `make` variant for R that plays nicely with modeling workflows, and `tidymodels` inspired much of the approach I took in the post above. However, current work in the `tidymodels` ecosystem focuses on high level infrastructure for tuning hyperparameters in predictive modeling scenarios. This mixture of general purpose low level infrastructure and more prediction specific high level infrastructure leads to some interesting discussions like [this one](https://community.rstudio.com/t/mixed-effect-models-and-anova-in-the-tidyverse/48029), where someone asks how to use `tidymodels` to analyze designed experiments, which `tidymodels` [doesn't really provide any tools for](https://community.rstudio.com/t/mixed-effect-models-and-anova-in-the-tidyverse/48029/7?u=alexpghayes).\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}