{
  "hash": "cdf30e1c5aa6321f5dbed90107321ee5",
  "result": {
    "markdown": "---\ntitle: \"implementing the super learner with tidymodels\"\nsubtitle: |\n  a demonstration of low levels tidymodels infrastructure to build sophisticated tools in a hurry\ndate: \"2019-04-13\"\nbibliography: implementing-the-super-learner-with-tidymodels.bib\ncategories: [tidymodels, stacking, methods, rstats]\n---\n\n\n## Summary\n\nIn this post I demonstrate how to implement the Super Learner using [`tidymodels`](https://github.com/tidymodels/) infrastructure. The Super Learner is an ensembling strategy that relies on cross-validation to determine how to combine predictions from many models. `tidymodels` provides low-level predictive modeling infrastructure that makes the implementation rather slick. The goal of this post is to show how you can use this infrastructure to build new methods with consistent, tidy behavior. You'll get the most out of this post if you've used [`rsample`](https://tidymodels.github.io/rsample/), [`recipes`](https://tidymodels.github.io/recipes/) and [`parsnip`](https://tidymodels.github.io/parsnip/) before and are comfortable working with list-columns.\n\n## How do I fit the super learner?\n\nThe Super Learner is an ensembling strategy with nice optimality properties. It's also not too terrible to implement:\n\n1. Fit a library of predictive models $f_1, ..., f_n$ on the full data set\n2. Get heldout predictions from $f_1, ..., f_n$ using k-fold cross-validation\n3. Train a *metalearner* on the heldout predictions\n\nThen when you want to predict on new data, you first run the data through $f_1, ..., f_n$, then take these predictions and send them through the metalearner.\n\nI'll walk through this step by step in R code, and then we'll wrap it up into a slightly more reusable function.\n\n## Implementation\n\nYou'll want to load the requisite packages with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyr)\nlibrary(tune)\nlibrary(modeldata)\n\nlibrary(furrr)\n\nopts <- furrr_options(\n  seed = TRUE\n)\n\n# use `plan(sequential)` to effectively convert all\n# subsequent `future_map*` calls to `map*`\n# calls. this will result in sequential execution of \n# embarassingly parallel model fitting procedures\n# but may prevent R from getting angry at parallelism\n\nplan(multicore)  \n\nset.seed(27)  # the one true seed\n```\n:::\n\n\nWe'll build a super learner on the `iris` dataset, which is build into R. `iris` looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- as_tibble(iris)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n```\n:::\n:::\n\n\nWe want to predict `Species` based on `Sepal.Length`, `Sepal.Width`, `Petal.Length` and `Petal.Width`. While this data isn't terribly exciting, multiclass classification is the most general case to deal with. Our code should just work for binary classification, and will require only minor modifications for regression problems.\n\n## Step 1: Fitting the library of predictive models\n\nFirst we need to fit a library of predictive models on the full data set. We'll use [`parsnip`](https://tidymodels.github.io/parsnip/) to specify the models, and [`dials`](https://tidymodels.github.io/dials/) to specify hyperparameter grids. Both `parsnip` and `dials` get loaded when you call `library(tidymodels)`.\n\nFor now we record the model we want to use. I'm going to fit C5.0 classification trees, where each tree has different hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- decision_tree(\n  mode = \"classification\",\n  min_n = tune(),\n  tree_depth = tune()\n) %>%\n  set_engine(\"C5.0\")\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: C5.0 \n```\n:::\n:::\n\n\nIf you look at `?decision_tree`, you'll see that we need to specify two hyperparameters, `min_n` and `tree_depth`, for the C5.0 engine. To do this we'll create a random hyperparameter grid using `dials`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhp_grid <- grid_random(\n  min_n() %>% range_set(c(2, 20)),\n  tree_depth(),\n  size = 10\n)\n\nhp_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   min_n tree_depth\n   <int>      <int>\n 1     6         15\n 2    19         11\n 3    10         14\n 4     9          8\n 5    20          1\n 6    17         12\n 7     2          1\n 8     2         14\n 9     2         15\n10     4          8\n```\n:::\n:::\n\n\nNow we create a [`tibble`](https://tibble.tidyverse.org/) with a list-column of completed model specifications (C5.0 trees where we've specified the hyperparameter values). It'll be useful to keep track of precisely which tree we're working with, so we also add a `model_id` column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspec_df <- merge(model, hp_grid) %>%  # tune::merge(), formerly dials::merge()\n  dplyr::rename(spec = x) %>% \n  mutate(model_id = row_number())\n\nspec_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   spec      model_id\n   <list>       <int>\n 1 <spec[?]>        1\n 2 <spec[?]>        2\n 3 <spec[?]>        3\n 4 <spec[?]>        4\n 5 <spec[?]>        5\n 6 <spec[?]>        6\n 7 <spec[?]>        7\n 8 <spec[?]>        8\n 9 <spec[?]>        9\n10 <spec[?]>       10\n```\n:::\n:::\n\n\nNow that we've specified our library of models, we'll describe the data design we'd like to use with a [`recipe`](https://tidymodels.github.io/recipes/). For giggles, we'll use the first two principle components:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe <- data %>% \n  recipe(Species ~ .) %>% \n  step_pca(all_predictors(), num_comp = 2)\n\nrecipe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nPCA extraction with all_predictors()\n```\n:::\n:::\n\n\nNow we can wrap up the first step and fit each of these trees on the full dataset. Here I use [`furrr::future_map()`](https://davisvaughan.github.io/furrr/) to do this in parallel, taking advantage of the embarrassingly parallel nature of model fitting. We pass `opts` to `.options` to get into the habit of using parallelism-safe random number generation, which will be important in just a moment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprepped <- prep(recipe, training = data)\n\nx <- juice(prepped, all_predictors())\ny <- juice(prepped, all_outcomes())\n\nfull_fits <- spec_df %>% \n  mutate(fit = future_map(spec, fit_xy, x, y, .options = opts))\n\nfull_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   spec      model_id fit     \n   <list>       <int> <list>  \n 1 <spec[?]>        1 <fit[+]>\n 2 <spec[?]>        2 <fit[+]>\n 3 <spec[?]>        3 <fit[+]>\n 4 <spec[?]>        4 <fit[+]>\n 5 <spec[?]>        5 <fit[+]>\n 6 <spec[?]>        6 <fit[+]>\n 7 <spec[?]>        7 <fit[+]>\n 8 <spec[?]>        8 <fit[+]>\n 9 <spec[?]>        9 <fit[+]>\n10 <spec[?]>       10 <fit[+]>\n```\n:::\n:::\n\n\n## Step 2: Getting holdout predictions\n\nWe'll use [`rsample`](https://tidymodels.github.io/rsample/) to generate the resampled datasets for 10-fold cross-validation, like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(data, v = 10)\n```\n:::\n\n\nWe will want to fit a model on each fold, which is a mapping operation like before. We define a helper that will fit one of our trees (defined by a `parsnip` model specification) on a given fold, and pass the data in the form of a trained `recipe` object, which we call `prepped`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_on_fold <- function(spec, prepped) {\n  \n  x <- juice(prepped, all_predictors())\n  y <- juice(prepped, all_outcomes())\n  \n  fit_xy(spec, x, y)\n}\n```\n:::\n\n\nNow we create a `tibble` containing all combinations of the cross-validation resamples and all the tree specifications:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note that tidyr::crossing() used to work for this operation but no\n# longer does\ncrossed <- expand_grid(folds, spec_df)\ncrossed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 4\n   splits           id     spec      model_id\n   <list>           <chr>  <list>       <int>\n 1 <split [135/15]> Fold01 <spec[?]>        1\n 2 <split [135/15]> Fold01 <spec[?]>        2\n 3 <split [135/15]> Fold01 <spec[?]>        3\n 4 <split [135/15]> Fold01 <spec[?]>        4\n 5 <split [135/15]> Fold01 <spec[?]>        5\n 6 <split [135/15]> Fold01 <spec[?]>        6\n 7 <split [135/15]> Fold01 <spec[?]>        7\n 8 <split [135/15]> Fold01 <spec[?]>        8\n 9 <split [135/15]> Fold01 <spec[?]>        9\n10 <split [135/15]> Fold01 <spec[?]>       10\n# … with 90 more rows\n```\n:::\n:::\n\n\nThe fitting procedure is then the longest part of the whole process, and looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_fits <- crossed %>%\n  mutate(\n    prepped = future_map(splits, prepper, recipe, .options = opts),\n    fit = future_map2(spec, prepped, fit_on_fold, .options = opts)\n  )\n```\n:::\n\n\nNow that we have our fits, we need to get holdout predictions. Recall that we trained each fit on the `analysis()` set, but we want to get holdout predictions using the `assessment()` set. There are a lot of moving pieces here, so we define a prediction helper function that includes the original row number of each prediction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_helper <- function(fit, new_data, recipe) {\n  \n  # new_data can either be an rsample::rsplit object\n  # or a data frame of genuinely new data\n  \n  if (inherits(new_data, \"rsplit\")) {\n    obs <- as.integer(new_data, data = \"assessment\")\n    \n    # never forget to bake when predicting with recipes!\n    new_data <- bake(recipe, assessment(new_data))\n  } else {\n    obs <- 1:nrow(new_data)\n    new_data <- bake(recipe, new_data)\n  }\n  \n  # if you want to generalize this code to a regression\n  # super learner, you'd need to set `type = \"response\"` here\n  \n  predict(fit, new_data, type = \"prob\") %>% \n    tibble::add_column(obs = obs, .before = TRUE)\n}\n```\n:::\n\n\nNow we use our helper to get predictions for each fold, for each hyperparameter combination. First we get the complete set of predictions for each fold and save them in a list-column called `raw_preds`. Then, since the predictions are perfectly correlated (`.pred_setosa + .pred_versicolor + .pred_virginica = 1`), we drop the last column of predictions to avoid issues with metalearners sensitive to colinearity. Finally, the `preds` column will be a list-column, so we `unnest()` to take a look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop_last_column <- function(df) df[, -ncol(df)]\n\nholdout_preds <- cv_fits %>% \n  mutate(\n    raw_preds = future_pmap(list(fit, splits, prepped), predict_helper, .options = opts),\n    preds = future_map(raw_preds, drop_last_column, .options = opts)\n  )\n\nholdout_preds %>% \n  unnest(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,500 × 10\n   splits           id     spec      model_id prepped  fit      raw_preds   obs\n   <list>           <chr>  <list>       <int> <list>   <list>   <list>    <int>\n 1 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     15\n 2 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     23\n 3 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     29\n 4 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     36\n 5 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     37\n 6 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     48\n 7 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     56\n 8 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     59\n 9 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     70\n10 <split [135/15]> Fold01 <spec[?]>        1 <recipe> <fit[+]> <tibble>     89\n# … with 1,490 more rows, and 2 more variables: .pred_setosa <dbl>,\n#   .pred_versicolor <dbl>\n```\n:::\n:::\n\n\nNow we have to shape this into something we can train a metalearner on, which means we need:\n\n- 1 row per original observation\n- 1 column per regression tree and outcome category\n\nGetting data into this kind of tidy format is exactly what `tidyr` excels at. Here we need to go from a long format to a wide format, which will often be the case when working with models in list columns[^multiclass_is_hard]. \n\n[^multiclass_is_hard]: Note that multiclass prediction is hardest to deal with because we have multiple prediction columns. For binary classification and regression, we'd only have a single column containing predictions, making the tidying easier.\n\nThe new [`pivot_wider()`](https://tidyr.tidyverse.org/dev/articles/pivot.html) function exactly solves this our reshaping problem once we realize that:\n\n- The row number of each observation in the original dataset is in the `obs` column\n- The `.pred_*` columns contain the values of interest\n- The `model_id` column identifies what the names of the new columns should be.\n\nWe're going to need to use this operation over and over again, so we'll put it into a function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspread_nested_predictions <- function(data) {\n  data %>% \n    unnest(preds) %>% \n    pivot_wider(\n      id_cols = obs,\n      names_from = model_id,\n      values_from = contains(\".pred\")\n    )\n}\n\nholdout_preds <- spread_nested_predictions(holdout_preds)\nholdout_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 21\n     obs .pred_setosa_1 .pred_setosa_2 .pred_setosa_3 .pred_setosa_4\n   <int>          <dbl>          <dbl>          <dbl>          <dbl>\n 1    15        0.985          0.985          0.985          0.985  \n 2    23        0.985          0.985          0.985          0.985  \n 3    29        0.985          0.985          0.985          0.985  \n 4    36        0.985          0.985          0.985          0.985  \n 5    37        0.985          0.985          0.985          0.985  \n 6    48        0.985          0.985          0.985          0.985  \n 7    56        0.00758        0.00758        0.00758        0.00758\n 8    59        0.00758        0.00758        0.00758        0.00758\n 9    70        0.00758        0.00758        0.00758        0.00758\n10    89        0.00758        0.00758        0.00758        0.00758\n# … with 140 more rows, and 16 more variables: .pred_setosa_5 <dbl>,\n#   .pred_setosa_6 <dbl>, .pred_setosa_7 <dbl>, .pred_setosa_8 <dbl>,\n#   .pred_setosa_9 <dbl>, .pred_setosa_10 <dbl>, .pred_versicolor_1 <dbl>,\n#   .pred_versicolor_2 <dbl>, .pred_versicolor_3 <dbl>,\n#   .pred_versicolor_4 <dbl>, .pred_versicolor_5 <dbl>,\n#   .pred_versicolor_6 <dbl>, .pred_versicolor_7 <dbl>,\n#   .pred_versicolor_8 <dbl>, .pred_versicolor_9 <dbl>, …\n```\n:::\n:::\n\n\nWe're almost ready to fit a the metalearning model on top of these predictions, but first we need to join these predictions back to the original dataset using `obs` to recover the labels!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta_train <- data %>% \n  mutate(obs = row_number()) %>% \n  right_join(holdout_preds, by = \"obs\") %>% \n  select(Species, contains(\".pred\"))\n\nmeta_train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 21\n   Species .pred_setosa_1 .pred_setosa_2 .pred_setosa_3 .pred_setosa_4\n   <fct>            <dbl>          <dbl>          <dbl>          <dbl>\n 1 setosa           0.985          0.985          0.985          0.985\n 2 setosa           0.986          0.986          0.986          0.986\n 3 setosa           0.986          0.986          0.986          0.986\n 4 setosa           0.986          0.986          0.986          0.986\n 5 setosa           0.985          0.985          0.985          0.985\n 6 setosa           0.985          0.985          0.985          0.985\n 7 setosa           0.986          0.986          0.986          0.986\n 8 setosa           0.985          0.985          0.985          0.985\n 9 setosa           0.986          0.986          0.986          0.986\n10 setosa           0.986          0.986          0.986          0.986\n# … with 140 more rows, and 16 more variables: .pred_setosa_5 <dbl>,\n#   .pred_setosa_6 <dbl>, .pred_setosa_7 <dbl>, .pred_setosa_8 <dbl>,\n#   .pred_setosa_9 <dbl>, .pred_setosa_10 <dbl>, .pred_versicolor_1 <dbl>,\n#   .pred_versicolor_2 <dbl>, .pred_versicolor_3 <dbl>,\n#   .pred_versicolor_4 <dbl>, .pred_versicolor_5 <dbl>,\n#   .pred_versicolor_6 <dbl>, .pred_versicolor_7 <dbl>,\n#   .pred_versicolor_8 <dbl>, .pred_versicolor_9 <dbl>, …\n```\n:::\n:::\n\n\n## Step 3: Fit the metalearner\n\nI'm going to use a multinomial regression as the metalearner. You can use any metalearner that does multiclass classification here, but I'm going with something simple because I don't want to obscure the logic with additional hyperparameter search here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# these settings correspond to multinomial regression\n# with a small ridge penalty. the ridge penalty makes\n# sure this doesn't explode when the number of columns  \n# of heldout predictions is greater than the number of\n# observations in the original data set\n#\n# in practice, you'll probably want to avoid base learner\n# libraries that large due to difficulties estimating\n# the relative performance of the base learners\n\nmetalearner <- multinom_reg(penalty = 0.01, mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  fit(Species ~ ., meta_train)\n```\n:::\n\n\nThat's it! We've fit the super learner! Just like the training process, prediction itself proceeds involves two separate stages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data <- head(iris)\n\n# run the new data through the library of base learners first\n\nbase_preds <- full_fits %>% \n  mutate(  \n    raw_preds = future_map(fit, predict_helper, new_data, prepped, .options = opts),\n    preds = future_map(raw_preds, drop_last_column, .options = opts)\n  ) %>% \n  spread_nested_predictions()\n\n# then through the metalearner\n\npredict(metalearner, base_preds, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  .pred_setosa .pred_versicolor .pred_virginica\n         <dbl>            <dbl>           <dbl>\n1        0.966           0.0209          0.0127\n2        0.966           0.0209          0.0127\n3        0.966           0.0209          0.0127\n4        0.966           0.0209          0.0127\n5        0.966           0.0209          0.0127\n6        0.966           0.0209          0.0127\n```\n:::\n:::\n\n\n## Putting it all together\n\nNow we can take all the code we've written up and encapsulate it into a single function (still relying on the helper functions we defined above).\n\nNote that this is a reference implementation and in practice I recommend following the [tidymodels recommendations](https://github.com/tidymodels/model-implementation-principles) when implementing new methods. Luckily, we do end up inherit a fair amount of nice consistency from `parsnip` itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Fit the super learner!\n#'\n#' @param library A data frame with a column `spec` containing\n#'   complete `parsnip` model specifications for the base learners \n#'   and a column `model_id`.\n#' @param recipe An untrained `recipe` specifying data design\n#' @param meta_spec A singe `parsnip` model specification\n#'   for the metalearner.\n#' @param data The dataset to fit the super learner on.\n#'\n#' @return A list with class `\"super_learner\"` and three elements:\n#'\n#'   - `full_fits`: A tibble with list-column `fit` of fit\n#'     base learners as parsnip `model_fit` objects\n#'\n#'   - `metalearner`: The metalearner as a single parsnip\n#'     `model_fit` object\n#'\n#'   - `recipe`: A trained version of the original recipe\n#'\nsuper_learner <- function(library, recipe, meta_spec, data) {\n  \n  folds <- vfold_cv(data, v = 5)\n  \n  cv_fits <- expand_grid(folds, library) %>%\n    mutate(\n      prepped = future_map(splits, prepper, recipe, .options = opts),\n      fit = future_pmap(list(spec, prepped), fit_on_fold, .options = opts)\n    )\n  \n  prepped <- prep(recipe, training = data)\n  \n  x <- juice(prepped, all_predictors())\n  y <- juice(prepped, all_outcomes())\n  \n  full_fits <- library %>% \n    mutate(fit = future_map(spec, fit_xy, x, y, .options = opts))\n  \n  holdout_preds <- cv_fits %>% \n    mutate(\n      raw_preds = future_pmap(list(fit, splits, prepped), predict_helper, .options = opts),\n      preds = future_map(raw_preds, drop_last_column, .options = opts)\n    ) %>% \n    spread_nested_predictions() %>% \n    select(-obs)\n  \n  metalearner <- fit_xy(meta_spec, holdout_preds, y)\n  \n  sl <- list(full_fits = full_fits, metalearner = metalearner, recipe = prepped)\n  class(sl) <- \"super_learner\"\n  sl\n}\n```\n:::\n\n\nWe also write an S3 predict method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict.super_learner <- function(x, new_data, type = c(\"class\", \"prob\")) {\n  \n  type <- rlang::arg_match(type)\n  \n  new_preds <- x$full_fits %>% \n    mutate(\n      raw_preds = future_map(fit, predict_helper, new_data, x$recipe, .options = opts),\n      preds = future_map(raw_preds, drop_last_column, .options = opts)\n    ) %>% \n    spread_nested_predictions() %>% \n    select(-obs)\n    \n  predict(x$metalearner, new_preds, type = type)\n}\n```\n:::\n\n\nOur helpers do assume that we're working on a classification problem, but other than this we pretty much only rely on the `parsnip` API. This means we can mix and match parts to our hearts desire and things should still work[^in_practice]. For example, we can build off the `parsnip` [classification vignette](https://tidymodels.github.io/parsnip/articles/articles/Classification.html), which starts like so:\n\n[^in_practice]: Fair warning: I have found a lot of bugs in the wrappers around modeling functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we have to do this because `modeldata` doesn't use lazy data loading\ndata(\"credit_data\")  \n\ndata_split <- credit_data %>% \n  na.omit() %>% \n  initial_split(strata = \"Status\", prop = 0.75)\n\ncredit_train <- training(data_split)\ncredit_test  <- testing(data_split)\n\ncredit_recipe <- recipe(Status ~ ., data = credit_train) %>%\n  step_center(all_numeric()) %>%\n  step_scale(all_numeric())\n```\n:::\n\n\nBut now let's fit a Super Learner based on a stack of MARS fits instead of a neural net. You could also mix in other arbitrary models[^why_bad_performance]. First we take a moment to set up the specification:\n\n[^why_bad_performance]: For some reason, I got poor test performance when I tried this, and I'm not sure why. I've [asked Erin Ledell on Twitter](https://twitter.com/alexpghayes/status/1117225953783164929) and will update if I get a response, hopefully including a more full example here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# needed due to a namespace bug at the moment,\n# but not in general\nlibrary(earth)  \n\ncredit_model <- mars(mode = \"classification\", prune_method = \"backward\") %>% \n  set_engine(\"earth\")\n\ncredit_hp_grid <- grid_random(\n  num_terms() %>% range_set(c(1, 30)),\n  prod_degree(),\n  size = 5\n)\n\ncredit_library <- merge(credit_model, credit_hp_grid) %>% \n  dplyr::rename(spec = x) %>% \n  mutate(model_id = row_number())\n\ncredit_meta <- multinom_reg(penalty = 0, mixture = 1) %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n\nNow we do the actual fitting and take a quick coffee break:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredit_sl <- super_learner(\n  credit_library,\n  credit_recipe,\n  credit_meta,\n  credit_train\n)\n```\n:::\n\n\nSince we inherit the tidymodels `predict()` conventions, getting a holdout ROC curve is as easy as: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(credit_sl, credit_test, type = \"prob\")\n\npred %>% \n  bind_cols(credit_test) %>% \n  roc_curve(Status, .pred_bad) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n## Wrap up\n\nThat's it! We've fit a clever ensembling technique in a few lines of code! Hopefully the concepts are clear and you can start to play with ensembling on your own. I should note that this post uses *a ton* of different `tidymodels` abstractions, which can be intimidating. The goal here is to demonstrate how to integrate all of various components together into a big picture. If you aren't familiar with the individual `tidymodels` packages, my impression is that the best way to gain this familiarity is by gradually working through the various tidymodels vignettes.\n\nIn practice, it is a bit of relief to be done with this post. I've been playing around with implementing the Super Learner since summer 2017, but each time I gave it a shot things got messy much faster than I anticipated and I kicked the task down the line. Only recently have the tools to make the Super Learner implementation so pleasant come to life[^unstable]. Thanks [Max](https://twitter.com/topepos) and [Davis](https://twitter.com/dvaughan32)! \n\n[^unstable]: The newness of these tools also means that some of them aren't entirely stable, however, and I found some bugs while writing this post.\n\nIf you want to use the Super Learner in practice, I believe the [`sl3`](https://tlverse.org/sl3/) package is the most actively developed. There's also Eric Polley's classic [`SuperLearner`](https://github.com/ecpolley/SuperLearner) package, which may be more full featured than `sl3` at the moment. Also be sure to check out [`h2o::automl()`](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html#code-examples), which makes stacking about as painless as can be if you just need results!\n\n## References\n\nIf you're new to the Super Learner, I recommend starting with @ledell_intro_2015. Section 2.2 of @ledell_scalable_2015 is similar but goes into more detail. @van_der_laan_super_2007 is the original Super Learner paper and contains the proof the oracle property, an optimality result. @polley_super_2010 discusses the Super Learner from a more applied point of view, with some simulations demonstrating performance. @van_der_laan_targeted_2018 is a comprehensive reference on both the Super Learner and TMLE. The Super Learner papers and book are targeted at a research audience with a high level of mathematical background, and are not easy reading. @wolpert_stacked_1992 is another often cited paper on stacking that is more approachable.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}