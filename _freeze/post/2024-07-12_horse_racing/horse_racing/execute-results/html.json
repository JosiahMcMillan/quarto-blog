{
  "hash": "ceb153185c2929334203c13539434d92",
  "result": {
    "markdown": "---\ntitle: \"Horse Racing as an Unsubtle Metaphor\"\nsubtitle: | \n  Aren't you the horse from Horsin' Around?\ndate: \"2024-07-12\"\ncategories: [betting, simulation, elections]\neditor: visual\ncache: true\ndf_print: tibble\nformat: html\ncode-fold: true\n---\n\n\n![He's nearly glue but he's stuck with us](./images/horse_politician.webp.png){width=80%}\n\n\n# This isn't about anything in particular\nBetting on a horse race can oftentimes be fraught with difficulties, if you put in your own horse your knowledge about it may be good you'll likely know how fast it runs, how well it takes corners, or how well the jockey handles it. However if you have a full stable of some horses that haven't raced before your knowledge about your other horses is limited in some ways, maybe you've timed them on some practice runs, maybe you don't know if they will be nervous in front of a crowd, etc. If you have a race coming up that you wish to win, you have to take stock of your horses and decide which to put into the race. If you find out however that during the pre-race showing of your horse that it has gotten a little long in the teeth you may find yourself struggling to make a decision to keep your ~~candidate~~ horse in the race or not.\n\nSomething other people have pointed out that I want to develop a bit further is the idea that when you know your horse has a degraded chance of getting over the finish line you probably want to sub it out for a higher variance horse *even if it may have a worse average race time*. Okay that's a big claim and others have described it a bit but I aim to simulate this and even the choice from a pool of horses whom you don't have a lot of information about. If you're in charge of deciding which horse to back even if you have little information and must select a horse at random from your stable you're likely to still get a better outcome for some underlying distribution of the horses' performances.\n\nSo to motivate this we begin by simulating the horse race in the simplest terms:\n\n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-1_210fa68a6b1049cad5b8e4693c8101d3'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# number of simulations\nn_sims <- 10000\n\n# define horses with different mean and variance\nhorses <- list(\n  horse1 = list(mean = 45, sd = 5),\n  horse2 = list(mean = 40, sd = 12)\n  \n)\n\n# function to simulate one race\nsimulate_race <- function(horses) {\n  results <- sapply(horses, function(horse) {\n    rnorm(1, mean = horse$mean, sd = horse$sd)\n  })\n  return(results)\n}\n\n# simulate many races\nrace_results <- replicate(n_sims, simulate_race(horses))\n\n# calculate probabilities of winning (getting above 50)\nwin_probs <- apply(race_results, 1, function(horse_results) {\n  mean(horse_results > 50)\n})\n\n\nprint(win_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhorse1 horse2 \n0.1587 0.2020 \n```\n:::\n:::\n\nThis is the standard sort of argument that people have been making on Twitter and Reddit, that there is a value to having a higher variance candidate even if they have a lower average. The variance more than compensates for the lower average mean as can be seen by the win probabilities and the plot below.\n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-2_4db36592859b9c325dcad1de6b0a2d67'}\n\n```{.r .cell-code}\n# Convert win_probs to a data frame\nrace_results_df <- data.frame(t(race_results))\n\n# Set common limits for x-axis\ncommon_x_limits <- range(0, 100)  # assuming win_probs are probabilities between 0 and 1\n\n\nplot_race_results <- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n```\n:::\n\n::: {.cell-output-display}\n![](horse_racing_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n# Uncertainty in our replacement horse\nLets say that we accept that there may be significantly worse and significantly better horses among our pool of candidate horses, some may outright lose and some may outright win a majority of the time. Well we can still use the above analysis to decide whether it makes sense to go with our old nag or randomly select amongst these horses. If we assume that the characteristics of the new horse are selected randomly from a uniform distribution then we actually end out still favoring the random selection than our old nag. \n\nIn this case we set the limits of mean vote share to 40 and 55 and the low and high standard deviations to between 5 and 15. \n\n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-3_f87622433a11d8aff0f118b5a38027bf'}\n\n```{.r .cell-code}\n# number of simulations\nn_sims <- 10000\n\n# define horse1 with a fixed mean and variance\nhorse1 <- list(mean = 45, sd = 5)\n\n# function to generate a random horse with specified mean and variance ranges\ngenerate_random_horse <- function(mean_range, sd_range) {\n  mean <- runif(1, mean_range[1], mean_range[2])\n  sd <- runif(1, sd_range[1], sd_range[2])\n  return(list(mean = mean, sd = sd))\n}\n\n# function to simulate one race\nsimulate_race <- function(horse1, mean_range, sd_range) {\n  horse2 <- generate_random_horse(mean_range, sd_range)\n  \n  horse1_result <- rnorm(1, mean = horse1$mean, sd = horse1$sd)\n  horse2_result <- rnorm(1, mean = horse2$mean, sd = horse2$sd)\n  \n  return(c(horse1_result, horse2_result))\n}\n\n# simulate many races\nmean_range <- c(40, 55)\nsd_range <- c(5, 15)\n\n\nrace_results <- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results <- t(race_results)  # transpose for easier handling\n\n# calculate probabilities of winning (getting above 50)\nwin_probs <- colMeans(race_results > 50)\nnames(win_probs) <- c(\"horse1\", \"horse2\")\n\n\n# print win probabilities\nnames(win_probs) <- c(\"horse1\", \"horse2\")\nprint(win_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhorse1 horse2 \n0.1628 0.4085 \n```\n:::\n\n```{.r .cell-code}\n# create a data frame for plotting\nrace_results_df <- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n```\n:::\n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-4_92772b31e3fc2b55e6013084b8f55283'}\n\n```{.r .cell-code}\nplot_race_results <- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 3 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n```\n:::\n\n::: {.cell-output-display}\n![](horse_racing_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n# A dirty but deep pool\nAnd if the above didn't convince you or you think the parameterization is too generous to our potential candidate pool then we can change our assumptions about the potential candidate pool to be more dour. Whether that is because an interlocutor might believe that the selection of candidates via the DNC is worse due to primary voters being weird (see: Mitt Romney changing his long held positions to appeal to Republican primary voters in 2012) or because the Democrats will suffer from low fundraising or low energy. As we can see even if we assume that our candidates are strictly worse in terms of mean vote share than our current candidate (by lowering their maximum mean vote shares to that of the original candidate) the higher variability of the choice in candidates and possible outcomes leads us to still prefer to select a new candidate.\n\n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-5_d8d984f65f1e49ef9d5b1ef5491166f3'}\n\n```{.r .cell-code}\nmean_range <- c(40, 44)\nsd_range <- c(5, 15)\n\n\nrace_results <- replicate(n_sims, simulate_race(horse1, mean_range, sd_range))\nrace_results <- t(race_results)  \n\n\nwin_probs <- colMeans(race_results > 50)\nnames(win_probs) <- c(\"horse1\", \"horse2\")\n\n\n\nnames(win_probs) <- c(\"horse1\", \"horse2\")\nprint(win_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhorse1 horse2 \n0.1516 0.2052 \n```\n:::\n\n```{.r .cell-code}\nrace_results_df <- data.frame(\n  horse1 = race_results[, 1],\n  horse2 = race_results[, 2]\n)\n```\n:::\n\n\nHere we plot the race results, note the win condition is the vertical line at 50. It is plain to see that the amount of probability mass even when we select a random candidate from a pool with candidates that would have mean vote share lower than our original candidate it is still prudent to choose a new candidate. This is assuming of course that we select from a pool of candidates where their national vote share would be as low as 40% which is much lower than any presidential election in recent history. \n\n::: {.cell hash='horse_racing_cache/html/unnamed-chunk-6_a5ebd260467a26db10578741a32328aa'}\n\n```{.r .cell-code}\ncommon_x_limits <- range(c(race_results_df$horse1, race_results_df$horse2))\ncommon_y_limits <- range(0, # assuming y-axis starts from 0\n                         max(\n                           hist(race_results_df$horse1, plot = FALSE)$counts,\n                           hist(race_results_df$horse2, plot = FALSE)$counts\n                         ))\n\n\nplot_race_results <- ggplot(race_results_df, aes(x = horse1, fill = \"horse1\")) +\n  geom_histogram(binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_histogram(data = race_results_df, aes(x = horse2, fill = \"horse2\"), \n                 binwidth = 1, alpha = 0.5, position = \"identity\") +\n  geom_vline(xintercept = 50, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Race Results\", x = \"Score\", y = \"Frequency\") +\n  xlim(common_x_limits) +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +  # custom fill colors\n  theme_minimal()\n\nplot_race_results\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\nRemoved 2 rows containing missing values (`geom_bar()`).\n```\n:::\n\n::: {.cell-output-display}\n![](horse_racing_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nSorry I think I may have gotten a bit confused about what topic we were discussing, but the point stands. Even if you have a relatively unknown set of horses, the high variability play is the one to choose if you know that your original horse is one that is lagging behind in its performance with some decent level of confidence. The current betting market odds, polling, and forecasting all indicate that those with money on the line believe it is paramount that a new stallion is found.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}