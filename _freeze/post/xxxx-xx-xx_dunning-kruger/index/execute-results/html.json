{
  "hash": "f09fa056a119f012e8b8073887644747",
  "result": {
    "markdown": "---\ntitle: \"TODO\"\nsubtitle: |\n  TODO\ndate: \"2024-01-24\"\ncategories: [statistical software]\nbibliography: dunning-kruger.bib\ndraft: true\nexecute:\n  echo: false\n  message: false\n  warning: false\nfig-align: center\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 929 × 3\n    SAIQ    IQ overconfidence\n   <dbl> <dbl>          <dbl>\n 1    85  69.2          15.8 \n 2    85  82.4           2.62\n 3    85  71.8          13.2 \n 4    85  71.8          13.2 \n 5    95  65.1          29.9 \n 6    95  65.1          29.9 \n 7    95  78.9          16.1 \n 8    95  91.7           3.30\n 9    95  65.1          29.9 \n10    95  65.1          29.9 \n# ℹ 919 more rows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   89.0      3.94       22.6  1.59e-90   81.3      96.8  \n2 IQ             0.341    0.0385      8.88 3.47e-18    0.266     0.417\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         term   estimate std.error statistic      p.value   conf.low  conf.high\n1 (Intercept) 89.0363936 4.4752814 19.895150 1.275013e-73 80.2535360 97.8192513\n2          IQ  0.3414424 0.0433767  7.871563 9.747387e-15  0.2563145  0.4265703\n   df outcome\n1 927    SAIQ\n2 927    SAIQ\n```\n:::\n:::\n\n\nhttps://economicsfromthetopdown.com/2022/04/08/the-dunning-kruger-effect-is-autocorrelation/\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nGAMLSS-RS iteration 1: Global Deviance = 7477.735 \nGAMLSS-RS iteration 2: Global Deviance = 7478.219 \nGAMLSS-RS iteration 3: Global Deviance = 7478.291 \nGAMLSS-RS iteration 4: Global Deviance = 7478.305 \nGAMLSS-RS iteration 5: Global Deviance = 7478.306 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n******************************************************************\nFamily:  c(\"NO2\", \"Normal with variance\") \n\nCall:  gamlss(formula = SAIQ ~ pb(IQ), sigma.formula = ~pb(IQ),  \n    family = NO2(sigma.link = \"identity\"), data = data_no_na) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  identity\nMu Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 88.80157    4.11472  21.581   <2e-16 ***\npb(IQ)       0.34382    0.03998   8.601   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  identity\nSigma Coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 359.1662    82.2696   4.366 1.41e-05 ***\npb(IQ)       -1.7046     0.7949  -2.144   0.0323 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNOTE: Additive smoothing terms exist in the formulas: \n i) Std. Error for smoothers are for the linear effect only. \nii) Std. Error for the linear terms maybe are not accurate. \n------------------------------------------------------------------\nNo. of observations in the fit:  929 \nDegrees of Freedom for the fit:  5.722073\n      Residual Deg. of Freedom:  923.2779 \n                      at cycle:  5 \n \nGlobal Deviance:     7478.306 \n            AIC:     7489.75 \n            SBC:     7517.411 \n******************************************************************\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,858 × 7\n   mu_hat mu_se param sigma_hat sigma_se sigma_low sigma_high\n    <dbl> <dbl> <chr>     <dbl>    <dbl>     <dbl>      <dbl>\n 1   113. 1.40  1            NA       NA        NA         NA\n 2   117. 0.912 1            NA       NA        NA         NA\n 3   113. 1.30  1            NA       NA        NA         NA\n 4   113. 1.30  1            NA       NA        NA         NA\n 5   111. 1.55  1            NA       NA        NA         NA\n 6   111. 1.55  1            NA       NA        NA         NA\n 7   116. 1.03  1            NA       NA        NA         NA\n 8   120. 0.614 1            NA       NA        NA         NA\n 9   111. 1.55  1            NA       NA        NA         NA\n10   111. 1.55  1            NA       NA        NA         NA\n# ℹ 1,848 more rows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 929 × 3\n   mu_hat mu_se param\n    <dbl> <dbl> <chr>\n 1   113. 1.40  mu   \n 2   117. 0.912 mu   \n 3   113. 1.30  mu   \n 4   113. 1.30  mu   \n 5   111. 1.55  mu   \n 6   111. 1.55  mu   \n 7   116. 1.03  mu   \n 8   120. 0.614 mu   \n 9   111. 1.55  mu   \n10   111. 1.55  mu   \n# ℹ 919 more rows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [929 × 3] (S3: tbl_df/tbl/data.frame)\n $ mu_hat: num [1:929] 113 117 113 113 111 ...\n $ mu_se : Named num [1:929] 1.396 0.912 1.298 1.298 1.551 ...\n  ..- attr(*, \"names\")= chr [1:929] \"1\" \"2\" \"3\" \"4\" ...\n $ param : chr [1:929] \"mu\" \"mu\" \"mu\" \"mu\" ...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 929 × 2\n     fit se.fit\n   <dbl>  <dbl>\n 1  113.  1.40 \n 2  117.  0.912\n 3  113.  1.30 \n 4  113.  1.30 \n 5  111.  1.55 \n 6  111.  1.55 \n 7  116.  1.03 \n 8  120.  0.614\n 9  111.  1.55 \n10  111.  1.55 \n# ℹ 919 more rows\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nthree ways to operationalize dunning kruger \n\n- mean\n- variance\n- some measure of risk\n\nhttps://twitter.com/blair_fix/status/1728755894899679580\nhttps://economicsfromthetopdown.com/2022/04/08/the-dunning-kruger-effect-is-autocorrelation/\n\n\nif it really cared to assess variability as a function of IQ it's using pretty naive/borderline inappropriate methods\n\nin general, a correlation tells you only the sign of the regression line\nbut for a lot of this dunning-kruger stuff the slope is what matters\n\nhttps://www.scientificamerican.com/article/the-dunning-kruger-effect-isnt-what-you-think-it-is/\n\nA paper called\n\n> The Dunning-Kruger effect is (mostly) a statistical artefact: Valid approaches to testing the hypothesis with individual differences data.\n\nhas been doing the Twitter rounds recently. I read the paper. It provides evidence for the Dunning-Kruger effect.\n\n## What's the what\n\n> The Dunning-Kruger hypothesis states that the degree to which people can estimate their ability accurately depends, in part, upon possessing the ability in question.\n\nTo test this hypothesis, the paper authors asked people to guess their IQ and then gave them actual IQ tests. Then they analyzed this data and came to the conclusion the Dunning-Kruger effect is not present in their data.\n\nThe core of the article comes down to the following diagram. Essentially, the traditional argument in favor of the Dunning-Kruger effect is graphical, and comes in the form of the plots in panels A and B. However, these plots dichotomize objective IQ and turn out to be misleading, especially in the presence of [measurement error][dlm]. The arguments against this methodology are worthwhile but I will skip over them to consider evidence for the Dunning-Kruger effect itself.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](../../../../../../img/dunning-kruger.png)\n:::\n:::\n\n\nWhat we care about is whether or not overconfidence varies as a function of skill. For this particular dataset, we operationalize\n\n\\[\n  \\mathrm{overconfidence}_i = \\mathrm{SAIQ}_i - \\mathrm{IQ}_i\n\\]\n\nand ask if overconfidence varies with IQ. This turns out to be equivalent to regressing IQ on self-assessed IQ and checking if the slope of the IQ term differs from one.\n\n@gignac2020 does not report the regression `SAIQ ~ IQ`, nor did they publish their data, but they do report the sample averages and standard deviations of `SAIQ` and `IQ`, as well as the sample size and the correlation between `SAIQ` and `IQ`. This lets us back out the results of a simple linear regression (thanks [Twitter](https://twitter.com/alexpghayes/status/1346546757858177025) for the lazy math assist).\n\nAnyway I did the calculations and we get the following regression table:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term      estimate std.error conf.low conf.high\n  <chr>        <dbl>     <dbl>    <dbl>     <dbl>\n1 intercept   89.0      3.94     88.8      89.3  \n2 slope        0.342    0.0385    0.339     0.344\n```\n:::\n:::\n\n\nNote that the slope is less than one, and the confidence interval is tight around the point estimate of 0.34. This means that, on average, for every one unit increase in `IQ`, someone's self-assessed IQ goes up by 0.34. To see how this is evidence of Dunning-Kruger, we can plot the regression.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNote that average person with an IQ of 80 self-assesses their IQ to be 116; that is, they are overconfident by 36 IQ points. The average person with an IQ of 100 self-assesses their IQ to be 123; they are only off by 23 IQ points. The average person with an IQ of 120 self-assesses their IQ to be 130; they are only off by 10 IQ points. Everyone is overconfident, but, in this dataset, people with higher IQs are less overconfident. This becomes even more clear if we look at fitted values of self-assessed SAIQ minus actual IQ, which I plot in the right panel.\n\nShould we trust this regression? I'm cautiously optimistic. Eye balling panel C from the figure in the paper above, but things look pretty much ideal. There could still be measurement error issues such that OLS is not reliable here, but it's certainly going to be way better than the weird quartile plots from before.\n\nHow does @gignac2020 come to the conclusion that there is no evidence of Dunning-Kruger effect in this data? Well, they claim the Dunning-Kruger effect should show up as a non-linearity in the regression function, and then fail to find evidence of a non-linear conditional expectation. This reasoning doesn't quite work because you can have a linear regression function that is consist with the Dunning-Kruger hypothesis, as I pointed out above.\n\n## TL; DR\n\nThe previous quartile based approach to demonstrate the presence of Dunning-Kruger has problems. However, simple linear regression on the data reported in @gignac2020 is still strongly suggestive of a Dunning-Kruger effect.\n\nIf you'd like to double check my code, it is available [here](https://gist.github.com/alexpghayes/466066ab4d94fee42ffacdf3821fa8ac).\n\n[dlm]: https://dlm-econometrics.blogspot.com/2020/12/nonclassical-classics.html\n\nhttps://gist.github.com/alexpghayes/466066ab4d94fee42ffacdf3821fa8ac\n\nhttps://osf.io/dg547/\n\nwill update in a moment but i was blind and it turns out the full data is available; if you model either SAIQ or IQ as observed with error you still infer slope < 1 with fairly high probability\n\nthe original dk methodology is silly, but the dk takedown articles have been just as ridiculous, if not moreso, in my experience\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}