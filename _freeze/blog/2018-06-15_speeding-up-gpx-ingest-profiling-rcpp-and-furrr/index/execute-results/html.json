{
  "hash": "690acbacf2bf8b77279e9eac1e8602ca",
  "result": {
    "markdown": "---\ntitle: \"speeding up GPX ingest: profiling, Rcpp and furrr\"\nsubtitle:\n  profiling your way to happiness, or possibly bikeshedding\ndate: \"2018-06-15\"\nexecute: \n  error: true\n---\n\nThis post is a casual case study in speeding up R code. I work through several iterations of a function to read and process GPS running data from Strava stored in the GPX format. Along the way I describe how to visualize code bottlenecks with `profvis` and briefly touch on fast compiled code with `Rcpp` and parallelization with `furrr`.\n\n## The problem: tidying trajectories in GPX files\n\nI record my runs on my phone using Strava. Strava stores the GPS recordings in GPX files, which are XML files that follow some additional conventions. They start with some metadata and then contain a list of GPS readings taken at one second intervals with longitude, latitude, elevation and timestap information. I wanted to approximate my speed at each time point in the GPS record, as well as my distance traveled since the previous GPS recordings.\n\nBelow I have an example of a GPX file that contains three GPS readings. First I create a vector that contains the names off my GPX files, and then I subset to the files that contain running data. I choose to work with the third run as a canonical example, and show a subset of the recording with three GPS readings.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────── tidyverse 1.3.1.9000 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.5.9000     ✔ purrr   0.3.4.9000\n✔ tibble  3.1.7.9000     ✔ dplyr   1.0.8.9000\n✔ tidyr   1.2.0.9000     ✔ stringr 1.4.0.9000\n✔ readr   2.1.2.9000     ✔ forcats 0.5.1.9000\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nhere() starts at /dabox/hayes/quarto-blog\n```\n:::\n\n```{.r .cell-code}\n# file contain run data\nact_files <- dir(here::here(\"posts/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr/2018-04-17-activities-alex\"),\n                 full.names = TRUE)\nrun_files <- act_files[str_detect(act_files, \"Run\")]\n\n# example file we'll work with\nfname <- run_files[3]\n\n# subset of example\nall <- read_lines(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in if (length(res) == 0 || res == -1) {: missing value where TRUE/FALSE needed\n```\n:::\n\n```{.r .cell-code}\nmini_idx <- c(1:20, 5897:5899)\ncat(all[mini_idx], sep = \"\\n\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in all[mini_idx]: object of type 'builtin' is not subsettable\n```\n:::\n:::\n\nThe part we want is in the `<trkseg>` tags. We'd like to turn this into a tidy dataframe where each row represents one GPS reading and the columns contain information like speed, distance, traveled, elevation gained, etc.\n\n## GPX reader version 0: using plotKML::readGPX\n\nUsing `plotKML::readGPX` we can read the representative file into R:\n\n::: {.cell}\n\n```{.r .cell-code}\ntrajectory <- gpx::read_gpx(fname)$tracks |> \n  as_tibble() |> \n  unnest() |> \n  janitor::clean_names() |> \n  select(-extensions, segment_id)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in gpx::read_gpx(fname): Specified file does not exist\n```\n:::\n\n```{.r .cell-code}\ntrajectory\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'trajectory' not found\n```\n:::\n:::\n\nWe want to compare location at $t$ and $t - 1$, so we create a lagged column of longitudes and latitudes. We put longitude and latitude together into a vector to play well with `raster::pointDistance`, which we'll use to compute the great circle distance between two points.\n\n::: {.cell}\n\n```{.r .cell-code}\nlagged <- trajectory |> \n    mutate(x = map2(longitude, latitude, c),  # create lagged position, this means the \n           x_old = lag(x),         # first row isn't complete\n           t_old = lag(time)) |> \n    slice(-1)                      # remove incomplete first row\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(trajectory, x = map2(longitude, latitude, c), x_old = lag(x), : object 'trajectory' not found\n```\n:::\n\n```{.r .cell-code}\nlagged\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'lagged' not found\n```\n:::\n:::\n\nIt turns out this data is not contiguous. Strava has a feature called autopause which detects pauses in runs (for example, at a stoplight), and GPS readings during paused periods are not include in the GPX files[^1]. GPS readings typically happen once every second. I plotted the time gaps between readings and realized that time gaps greater than three seconds between two GPS recordings indicated a pause. This lets me break the run down into a series of contigous segments:\n\n[^1]: It took me a two months to realize this, mostly because I didn't plot enough of the data. If you're curous how Strava detects paused movement, you can read more [here](https://medium.com/strava-engineering/improving-auto-pause-for-everyone-13f253c66f9e). It seems to involve more if-statements than fun models.\n\n::: {.cell}\n\n```{.r .cell-code}\nsegmented <- lagged |> \n  mutate(rest = as.numeric(time - t_old),     # seconds\n         new_segment = as.numeric(rest > 3),  \n         segment = cumsum(new_segment)) |>\n  \n  # don't want t_old to be from previous segment\n  group_by(segment) |> \n  slice(-1)  \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(lagged, rest = as.numeric(time - t_old), new_segment = as.numeric(rest > : object 'lagged' not found\n```\n:::\n\n```{.r .cell-code}\nsegmented\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'segmented' not found\n```\n:::\n:::\n\nNow I calculate some information about each time point and segment that I'll use in downstream analyses:\n\n::: {.cell}\n\n```{.r .cell-code}\nlonlat_dist <- partial(raster::pointDistance, lonlat = TRUE)\n\nuseful <- segmented |> \n  mutate(\n    seg_length = max(time) - min(t_old),    # seconds\n    dx = map2_dbl(x, x_old, lonlat_dist),   # meters\n    dx = 0.000621371 * dx,                  # miles\n    dt = rest / 60^2,                       # hours\n    speed = dx / dt,                        # mph\n    pace = 60 * dt / dx,                    # min / mile\n    elevation = elevation                   # feet\n  ) |> \n  dplyr::select(-elevation, -x, -x_old, -t_old, -new_segment) |> \n  ungroup()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(segmented, seg_length = max(time) - min(t_old), dx = map2_dbl(x, : object 'segmented' not found\n```\n:::\n\n```{.r .cell-code}\nuseful\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'useful' not found\n```\n:::\n:::\n\nWe can quickly visualize instantaneous speed throughout the run:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(useful, aes(time, speed, group = segment)) +\n  geom_point() +\n  geom_line(alpha = 0.5) +\n  labs(title = \"Speed throughout example run\",\n       y = \"Speed (mph)\") +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(useful, aes(time, speed, group = segment)): object 'useful' not found\n```\n:::\n:::\n\nWe can see two short pauses present in the run at around 18:08 and 18:17.\n\nWe're going to use the code above a whole bunch, so we wrap it up into a helper function. I'm not sure that `raster::pointDistance` is the best option for calculating the distance between two points, so we use a `dist_func` argument to make it easy to switch out.\n\n::: {.cell}\n\n```{.r .cell-code}\nget_metrics <- function(gps_df, dist_func = lonlat_dist)  {\n  gps_df |> \n    mutate(x = map2(longitude, latitude, c),\n           x_old = lag(x),\n           t_old = lag(time)) |> \n    slice(-1) |> \n    mutate(rest = as.numeric(time - t_old),\n           new_segment = as.numeric(rest > 3),\n           segment = cumsum(new_segment) + 1) |>\n    group_by(segment) |> \n    slice(-1) |>\n    mutate(seg_length = max(time) - min(t_old),\n           dx = map2_dbl(x, x_old, dist_func),\n           dx = 0.000621371 * dx, \n           dt = rest / 60^2,    \n           speed = dx / dt,       \n           pace = 60 * dt / dx) |>\n    dplyr::select(-x, -x_old, -t_old, -new_segment, -rest) |> \n    ungroup()\n}\n```\n:::\n\nThis means our initial `read_gpx` function is just two lines:\n\n::: {.cell}\n\n```{.r .cell-code}\nread_gpx0 <- function(fname) {\n  gps_df <- gpx::read_gpx(fname)$tracks |> \n    as_tibble() |> \n    unnest() |> \n    janitor::clean_names() |> \n    select(-extensions, -segment_id)\n  \n  get_metrics(gps_df)\n}\n```\n:::\n\nWe can use `profvis::profvis` to create an interactive visualization of how long it takes to read the example file.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(profvis)\n\nprofvis(read_gpx0(fname))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nprofvis: code exited with error:\nSpecified file does not exist\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in parse_rprof_lines(lines, expr_source): No parsing data available. Maybe your function was too fast?\n```\n:::\n:::\n\nIn the default view, the horizontal axis represents time and the box represents the call stack. All the boxes above `plotKML::readGPX` are functions called by `plotKML::readGPX`. Here it seems like `plotKML::readGPX` takes about 400 milliseconds to run. So about half the time is spent reading in the file, and half calculating metrics. Most of the time calculating metrics is in `raster::pointDistance`, which is fairly up the call stack - you may have to click and drag the plot to see it.\n\n## GPX reader version 1: no more plotKML::GPX\n\nThen I broke my R library and couldn't use `plotKML::readGPX` for a little while. Since GPX files are XML files, I used the `xml2` package as a replacement. `xml2` has a function `as_list` that let me treat the XML as an R list. We extract the relevant portion of the list and `purrr::map_dfr` each GPS recording into a row of a `tibble`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xml2)\n\nrun_xml <- read_xml(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n```{.r .cell-code}\nrun_list <- as_list(run_xml)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as_list(run_xml): object 'run_xml' not found\n```\n:::\n\n```{.r .cell-code}\ngps_pts <- run_list$gpx$trk$trkseg\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'run_list' not found\n```\n:::\n\n```{.r .cell-code}\nextract_gps_point <- function(point) {\n  tibble(\n    longitude = attr(point, \"lon\"),\n    latitude = attr(point, \"lat\"),\n    ele = point$ele[[1]],\n    time = point$time[[1]]\n  )\n}\n\nmap_dfr(gps_pts, extract_gps_point)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in map(.x, .f, ...): object 'gps_pts' not found\n```\n:::\n:::\n\nThen we wrap this in a function.\n\n::: {.cell}\n\n```{.r .cell-code}\nread_gpx1 <- function(fname) {\n  run_xml <- read_xml(fname)\n  run_list <- as_list(run_xml)\n  \n  extract_gps_point <- function(point) {\n    tibble(\n      longitude = attr(point, \"lon\"),\n      latitude = attr(point, \"lat\"),\n      ele = point$ele[[1]],\n      time = point$time[[1]]\n    )\n  }\n  \n  gps_df <- map_dfr(run_list$gpx$trk$trkseg, extract_gps_point)\n  get_metrics(gps_df)\n}\n```\n:::\n\nThe next part is critical when trying to speed up code: **test that the new code does the same thing as the old code**.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(testthat)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'testthat'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    matches\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    is_null\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:readr':\n\n    edition_get, local_edition\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tidyr':\n\n    matches\n```\n:::\n\n```{.r .cell-code}\nexpected <- read_gpx0(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in gpx::read_gpx(fname): Specified file does not exist\n```\n:::\n\n```{.r .cell-code}\nresult_1 <- read_gpx1(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n```{.r .cell-code}\n# silence means everything went well\nexpect_equal(expected, result_1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval_bare(expr, quo_get_env(quo)): object 'expected' not found\n```\n:::\n:::\n\nThis turned out to be too slow, so we profile and see which lines are taking the most amount of time.\n\n::: {.cell}\n\n```{.r .cell-code}\nprofvis(read_gpx1(fname))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nprofvis: code exited with error:\n'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in parse_rprof_lines(lines, expr_source): No parsing data available. Maybe your function was too fast?\n```\n:::\n:::\n\nHere we see that we spend most of our time on the functions `as_list` and `tibble`.\n\n# GPX reader version 2: no more tibble\n\n`tibble`s are somewhat heavy objects, and we can bind lists together instead of `tibble`s, so let's try that next. We only change one line from `read_gpx1`.\n\n::: {.cell}\n\n```{.r .cell-code}\nread_gpx2 <- function(fname) {\n  run_xml <- read_xml(fname)\n  run_list <- as_list(run_xml)\n  \n  extract_gps_point <- function(point) {\n    list(longitude = attr(point, \"lon\"),\n         latitude = attr(point, \"lat\"),\n         ele = point$ele[[1]],\n         time = point$time[[1]])\n  }\n  \n  gps_df <- map_dfr(run_list$gpx$trk$trkseg, extract_gps_point)\n  get_metrics(gps_df)\n}\n\nresult_2 <- read_gpx2(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n```{.r .cell-code}\nexpect_equal(expected, result_2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval_bare(expr, quo_get_env(quo)): object 'expected' not found\n```\n:::\n:::\n\nOur results are still as expected, which is good.  We profile again to see if we've done any better, which we have. Now we're at about 1.5 seconds instead of 2.5 seconds.\n\n::: {.cell}\n\n```{.r .cell-code}\nprofvis(read_gpx2(fname))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nprofvis: code exited with error:\n'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in parse_rprof_lines(lines, expr_source): No parsing data available. Maybe your function was too fast?\n```\n:::\n:::\n\nI needed to this for about fifty files though, so this was still slow enough to be somewhat frustrating. Now `xml2::as_list` is really killing us. \n\n## GPX reader version 3: now with more xml2\n\nLuckily, we can use `xml2` to manipulate the XML via a fast C package instead. For this next part I tried functions exported by `xml2` until they worked and occasionally read the documentation.\n\n::: {.cell}\n\n```{.r .cell-code}\nread_gpx_xml <- function(fname) {\n  # get the interested nodes\n  run_xml <- read_xml(fname)\n  trk <- xml_child(run_xml, 2)\n  trkseg <- xml_child(trk, 2)\n  trkpts <- xml_children(trkseg)  # nodeset where each node is a GPS reading\n  \n  # get the longitude and latitude for each node\n  latlon_list <- xml_attrs(trkpts)  \n  latlon <- do.call(rbind, latlon_list)\n  \n  # get the time and elevation for each node\n  ele_time_vec <- xml_text(xml_children(trkpts))\n  ele_time <- matrix(ele_time_vec, ncol = 2, byrow = TRUE)\n  colnames(ele_time) <- c(\"ele\", \"time\")\n  \n  as_tibble(cbind(latlon, ele_time))\n}\n\nread_gpx3 <- function(fname) {\n  gps_df <- read_gpx_xml(fname)\n  get_metrics(gps_df)\n}\n\nresult_3 <- read_gpx3(fname)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n```{.r .cell-code}\nexpect_equal(expected, result_3)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval_bare(expr, quo_get_env(quo)): object 'expected' not found\n```\n:::\n:::\n\nAgain we see if there's anywhere else we can speed things up:\n\n::: {.cell}\n\n```{.r .cell-code}\nprofvis(read_gpx3(fname))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nprofvis: code exited with error:\n'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in parse_rprof_lines(lines, expr_source): No parsing data available. Maybe your function was too fast?\n```\n:::\n:::\n\nWe're way faster, taking less than half a second! Now the most time is spent on `raster::pointDistance`, which we call a ton of times. What does `pointDistance` do? It takes two pairs `(lat1, lon1)` and `(lat2, lon2)` the distance between them[^2].\n\n[^2]: We can't calculate the distance using the L2 norm because longitude and latitude are spherical coordinates, not Euclidean coordinates.\n\n## GPX reader version 4: drop into Rcpp\n\nNext I Googled how to perform this calculation myself and found [this](http://www.movable-type.co.uk/scripts/latlong.html#ellipsoid) and [this](https://www.r-bloggers.com/great-circle-distance-calculations-in-r/). The `Rcpp` implementation looks like:\n\n::: {.cell}\n\n```{.rcpp .cell-code}\n#include <Rcpp.h>\n\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\ndouble haversine_dist(const NumericVector p1, const NumericVector p2) {\n  \n  double lat1 = p1[0] * M_PI / 180;\n  double lon1 = p1[1] * M_PI / 180;\n  double lat2 = p2[0] * M_PI / 180;\n  double lon2 = p2[1] * M_PI / 180;\n  \n  double d_lat = lat2 - lat1;\n  double d_lon = lon2 - lon1;\n  \n  double a = pow(sin(d_lat / 2.0), 2) + \n    cos(lat1) * cos(lat2) * pow(sin(d_lon / 2.0), 2);\n  double c = 2 * asin(std::min(1.0, sqrt(a)));\n  \n  return 6378137 * c; // 6378137 is the radius of the earth in meters\n}\n```\n:::\n\nThe haversine distance is fast to calculate at the cost of some small error, which we can see below:\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- c(0, 0)\np2 <- c(1, 1)\n\ndist_expected <- raster::pointDistance(p1, p2, lonlat = TRUE)\ndist_result <- haversine_dist(p1, p2)\n\ndist_result - dist_expected\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 525.9688\n```\n:::\n:::\n\nIt turns out that \"small error\" on the geological scale is big error on the neighborhood run scale. Put all together, the C++ version looks like:\n\n::: {.cell}\n\n```{.r .cell-code}\nread_gpx4 <- function(fname) {\n  gps_df <- read_gpx_xml(fname)\n  get_metrics(gps_df, dist_func = haversine_dist)\n}\n```\n:::\n\nWe profile one more time:\n\n::: {.cell}\n\n```{.r .cell-code}\nprofvis(read_gpx4(fname))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nprofvis: code exited with error:\n'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in parse_rprof_lines(lines, expr_source): No parsing data available. Maybe your function was too fast?\n```\n:::\n:::\n\nNow it takes only about 0.1 seconds, but the result isn't accurate enough anymore. I wasn't in the mood to implement a more precise great circle distance calculation, but hopefully this illustrates the general principle of dropping into `Rcpp` and also why it's important to test when profiling.\n\n## Comparing the various GPX readers\n\nNow we can compare how long each version takes using the `bench` package.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bench)\n\nmark(\n  read_gpx0(fname),\n  read_gpx1(fname),\n  read_gpx2(fname),\n  read_gpx3(fname),\n  read_gpx4(fname),\n  iterations = 5,   # how many times to run everything. 5 is very low.\n  relative = TRUE,\n  check = FALSE     # since readgpx4 isn't right, will error without this\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in gpx::read_gpx(fname): Specified file does not exist\n```\n:::\n:::\n\nHere timings are relative. We see that `read_gpx4` is about ten times faster than `read_gpx1` and two times faster than `read_gpx0`.\n\n## Embarrassing parallelization with `furrr`\n\nIn the end, I needed to do this for about fifty files. Since we can process each file independently of the other files, this operation is *embarrassingly parallel*. I actually wanted to use this data, so I didn't use the C++ haversine distance function. We can write with a single `map` call to process all the files at once:\n\n::: {.cell}\n\n```{.r .cell-code}\nrun_files_subset <- run_files[1:10]\n\nmap_dfr(run_files_subset, read_gpx3, .id = \"run\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n:::\n\nWhich means we can also write this as a parallelized `map` call with `furrr` like so:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: future\n```\n:::\n\n```{.r .cell-code}\nplan(multiprocess)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Strategy 'multiprocess' is deprecated in future (>= 1.20.0). Instead,\nexplicitly specify either 'multisession' or 'multicore'. In the current R\nsession, 'multiprocess' equals 'multicore'.\n```\n:::\n\n```{.r .cell-code}\nfuture_map_dfr(run_files_subset, read_gpx3, .id = \"run\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n:::\n\nNote that other than loading `furrr` and calling `plan(multiprocess)` all we've had to do to get parallelism is to call `furrr::future_map_dfr`, which has exactly the same API as `purrr::map_dfr`. My computer has two cores, meaning there's a maximum possible speedup of two, and we achieve nearly that:\n\n::: {.cell}\n\n```{.r .cell-code}\nmark( \n  map_dfr(run_files_subset, read_gpx3, .id = \"run\"),\n  future_map_dfr(run_files_subset, read_gpx3, .id = \"run\"),\n  iterations = 5,\n  relative = TRUE\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'NA' does not exist in current working directory ('/dabox/hayes/quarto-blog/blog/2018-06-15_speeding-up-gpx-ingest-profiling-rcpp-and-furrr').\n```\n:::\n:::\n\n## Wrap Up\n\nThis was a low stakes exercise in speeding up R code. By the time I'd written all of these it would have been several hundred times faster to use `read_gpx0` and just save the results to a `.rds` file. Still, it was fun to work through the profiling workflow and I look forward to enterprising strangers on the internet pointing out places where things can get faster still.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}