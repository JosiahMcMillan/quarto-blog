@article{balch_satellite_2019,
  title = {Satellite Conjunction Analysis and the False Confidence Theorem},
  author = {Balch, Michael Scott and Martin, Ryan and Ferson, Scott},
  year = {2019},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {475},
  number = {2227},
  pages = {20180565},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2018.0565},
  langid = {english},
  file = {/home/alex/Zotero/storage/WDP444XI/Balch et al. - 2019 - Satellite conjunction analysis and the false confi.pdf}
}

@article{cella_valid_nodate,
  title = {Valid Distribution-Free Inferential Models for Prediction},
  author = {Cella, Leonardo and Martin, Ryan},
  pages = {22},
  abstract = {A fundamental problem in statistics and machine learning is that of using observed data to predict future observations. This is particularly challenging for model-based approaches because often the goal is to carry out this prediction with no or minimal model assumptions. For example, the inferential model (IM) approach is attractive because it has certain validity guarantees, but requires specification of a parametric model. Here we show that a new perspective on a recently developed generalized IM approach can be applied to construct an IM for prediction that satisfies the desirable validity guarantees without specification of a model. One important special case of this approach corresponds to the powerful conformal prediction framework and, consequently, the desirable properties of conformal prediction follow immediately from the general IM validity theory.},
  langid = {english},
  file = {/home/alex/Zotero/storage/K58IHGAA/Cella and Martin - Valid distribution-free inferential models for pre.pdf}
}

@article{hong_model_2020,
  title = {Model Misspecification, {{Bayesian}} versus Credibility Estimation, and {{Gibbs}} Posteriors},
  author = {Hong, Liang and Martin, Ryan},
  year = {2020},
  month = jan,
  journal = {Scandinavian Actuarial Journal},
  pages = {1--16},
  issn = {0346-1238, 1651-2030},
  doi = {10.1080/03461238.2019.1711154},
  abstract = {In the context of predicting future claims, a fully Bayesian analysis \textendash{} one that specifies a statistical model, prior distribution, and updates using Bayes's formula \textendash{} is often viewed as the gold-standard, while B\"uhlmann's credibility estimator serves as a simple approximation. But those desirable properties that give the Bayesian solution its elevated status depend critically on the posited model being correctly specified. Here we investigate the asymptotic behavior of Bayesian posterior distributions under a misspecified model, and our conclusion is that misspecification bias generally has damaging effects that can lead to inaccurate inference and prediction. The credibility estimator, on the other hand, is not sensitive at all to model misspecification, giving it an advantage over the Bayesian solution in those practically relevant cases where the model is uncertain. This begs the question: does robustness to model misspecification require that we abandon uncertainty quantification based on a posterior distribution? Our answer to this question is No, and we offer an alternative Gibbs posterior construction. Furthermore, we argue that this Gibbs perspective provides a new characterization of B\"uhlmann's credibility estimator.},
  langid = {english},
  file = {/home/alex/Zotero/storage/3IVG58LB/Hong and Martin - 2020 - Model misspecification, Bayesian versus credibilit.pdf}
}

@article{liu_inferential_nodate,
  title = {Inferential Models and Possibility Measures},
  author = {Liu, Chuanhai and Martin, Ryan},
  pages = {21},
  abstract = {The inferential model (IM) framework produces data-dependent, non-additive degrees of belief about the unknown parameter that are provably valid. The validity property guarantees, among other things, that inference procedures derived from the IM control frequentist error rates at the nominal level. A technical complication is that IMs are built on a relatively unfamiliar theory of random sets. Here we develop an alternative\textemdash and practically equivalent\textemdash formulation, based on a theory of possibility measures, which is simpler in many respects. This new perspective also sheds light on the relationship between IMs and Fisher's fiducial inference, as well as on the construction of optimal IMs.},
  langid = {english},
  file = {/home/alex/Zotero/storage/YWNHQLQQ/Liu and Martin - Inferential models and possibility measures.pdf}
}

@article{martin_false_2019,
  title = {False Confidence, Non-Additive Beliefs, and Valid Statistical Inference},
  author = {Martin, Ryan},
  year = {2019},
  month = oct,
  journal = {International Journal of Approximate Reasoning},
  volume = {113},
  eprint = {1607.05051},
  eprinttype = {arxiv},
  pages = {39--73},
  issn = {0888613X},
  doi = {10.1016/j.ijar.2019.06.005},
  abstract = {Statistics has made tremendous advances since the times of Fisher, Neyman, Jeffreys, and others, but the fundamental and practically relevant questions about probability and inference that puzzled our founding fathers remain unanswered. To bridge this gap, I propose to look beyond the two dominating schools of thought and ask the following three questions: what do scientists need out of statistics, do the existing frameworks meet these needs, and, if not, how to fill the void? To the first question, I contend that scientists seek to convert their data, posited statistical model, etc., into calibrated degrees of belief about quantities of interest. To the second question, I argue that any framework that returns additive beliefs, i.e., probabilities, necessarily suffers from false confidence\textemdash certain false hypotheses tend to be assigned high probability\textemdash and, therefore, risks systematic bias. This reveals the fundamental importance of non-additive beliefs in the context of statistical inference. But non-additivity alone is not enough so, to the third question, I offer a sufficient condition, called validity, for avoiding false confidence, and present a framework, based on random sets and belief functions, that provably meets this condition. Finally, I discuss characterizations of p-values and confidence intervals in terms of valid non-additive beliefs, which imply that users of these classical procedures are already following the proposed framework without knowing it.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/alex/Zotero/storage/KAC3XV26/Martin - 2019 - False confidence, non-additive beliefs, and valid .pdf}
}

@article{martin_imprecise-probabilistic_2021,
  title = {An Imprecise-Probabilistic Characterization of Frequentist Statistical Inference},
  author = {Martin, Ryan},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.10904 [math, stat]},
  eprint = {2112.10904},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Between the two dominant schools of thought in statistics, namely, Bayesian and classical/frequentist, a main difference is that the former is grounded in the mathematically rigorous theory of probability while the latter is not. In this paper, I show that the latter is grounded in a different but equally mathematically rigorous theory of imprecise probability. Specifically, I show that for every suitable testing or confidence procedure with error rate control guarantees, there exists a consonant plausibility function whose derived testing or confidence procedure is no less efficient. Beyond its foundational implications, this characterization has at least two important practical consequences: first, it simplifies the interpretation of p-values and confidence regions, thus creating opportunities for improved education and scientific communication; second, the constructive proof of the main results leads to a strategy for new and improved methods in challenging inference problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/alex/Zotero/storage/GKDGT3JH/Martin - 2021 - An imprecise-probabilistic characterization of fre.pdf}
}

@article{martin_inferential_2013,
  title = {Inferential Models: {{A}} Framework for Prior-Free Posterior Probabilistic Inference},
  shorttitle = {Inferential Models},
  author = {Martin, Ryan and Liu, Chuanhai},
  year = {2013},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {108},
  number = {501},
  eprint = {1206.4091},
  eprinttype = {arxiv},
  pages = {301--313},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2012.747960},
  abstract = {Posterior probabilistic statistical inference without priors is an important but so far elusive goal. Fisher's fiducial inference, Dempster\textendash Shafer theory of belief functions, and Bayesian inference with default priors are attempts to achieve this goal but, to date, none has given a completely satisfactory picture. This paper presents a new framework for probabilistic inference, based on inferential models (IMs), which not only provides data-dependent probabilistic measures of uncertainty about the unknown parameter, but does so with an automatic long-run frequency calibration property. The key to this new approach is the identification of an unobservable auxiliary variable associated with observable data and unknown parameter, and the prediction of this auxiliary variable with a random set before conditioning on data. Here we present a three-step IM construction, and prove a frequency-calibration property of the IM's belief function under mild conditions. A corresponding optimality theory is developed, which helps to resolve the non-uniqueness issue. Several examples are presented to illustrate this new approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/alex/Zotero/storage/6CACXVN5/Martin and Liu - 2013 - Inferential models A framework for prior-free pos.pdf}
}

@book{martin_inferential_2020,
  title = {Inferential {{Models}}: {{Reasoning}} with {{Uncertainty}}.},
  shorttitle = {Inferential {{Models}}},
  author = {Martin, Ryan and Chuanhai, Liu},
  year = {2020},
  publisher = {{CRC PRESS}},
  address = {{S.l.}},
  isbn = {978-0-367-73780-1},
  langid = {english},
  annotation = {OCLC: 1201660248},
  file = {/home/alex/Zotero/storage/PI8DPMHH/Martin and Liu - Inferential Models Reasoning with Uncertainty.pdf}
}

@article{martin_mathematical_2017,
  title = {A Mathematical Characterization of Confidence as Valid Belief},
  author = {Martin, Ryan},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.00486 [math, stat]},
  eprint = {1707.00486},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Confidence is a fundamental concept in statistics, but there is a tendency to misinterpret it as probability. In this paper, I argue that an intuitively and mathematically more appropriate interpretation of confidence is through belief/plausibility functions, in particular, those that satisfy a certain validity property. Given their close connection with confidence, it is natural to ask how a valid belief/plausibility function can be constructed directly. The inferential model (IM) framework provides such a construction, and here I prove a complete-class theorem stating that, for every nominal confidence region, there exists a valid IM whose plausibility regions are contained by the given confidence region. This characterization has implications for statistics understanding and communication, and highlights the importance of belief functions and the IM framework.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/alex/Zotero/storage/BLUSAVUI/Martin - 2017 - A mathematical characterization of confidence as v.pdf}
}

@article{martin_response_2021,
  title = {Response to the Comment {{Confidence}} in Confidence Distributions!},
  author = {Martin, Ryan and Balch, Michael S. and Ferson, Scott},
  year = {2021},
  month = jun,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {477},
  number = {2250},
  pages = {20200579},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2020.0579},
  langid = {english},
  file = {/home/alex/Zotero/storage/APZN8ADQ/Martin et al. - 2021 - Response to the comment Confidence in confidence d.pdf}
}

@article{martin_valid_nodate,
  title = {Valid and Efficient Imprecise-Probabilistic Inference across a Spectrum of Partial Prior Information},
  author = {Martin, Ryan},
  pages = {54},
  abstract = {Bayesian inference quantifies uncertainty directly and formally using classical probability theory, while frequentist inference does so indirectly and informally through the use of procedures with error rate control. Both have merits in the appropriate context, but the context isn't binary. If no prior information is available, then no prior distribution can be ruled out, so this context is best characterized as every prior. This implies there's an entire spectrum of contexts depending on what, if any, partial prior information is available, with ``Bayesian'' (one prior) and ``frequentist'' (every prior) on opposite extremes. Common examples between the two extremes include those high-dimensional problems where, e.g., sparsity assumptions are relevant but fall short of determining a complete prior distribution. This paper ties the two frameworks together by treating those cases where only partial prior information is available using the theory of imprecise probability. The end result is a unified framework of (imprecise-probabilistic) statistical inference with a new validity condition that implies both frequentist-style error rate control for derived procedures and Bayesian-style no-sure-loss properties, relative to the given partial prior information. This theory contains both the classical ``Bayesian'' and ``frequentist'' frameworks as special cases, since they're both valid in this new sense relative to their respective partial priors. Different constructions of these valid inferential models are considered, and compared based on their efficiency.},
  langid = {english},
  file = {/home/alex/Zotero/storage/VALE6ZCP/Martin - Valid and eﬃcient imprecise-probabilistic inferenc.pdf}
}

@book{schweder_confidence_2016,
  title = {Confidence, {{Likelihood}}, {{Probability}}: {{Statistical Inference}} with {{Confidence Distributions}}},
  shorttitle = {Confidence, {{Likelihood}}, {{Probability}}},
  author = {Schweder, Tore and Hjort, Nils Lid},
  year = {2016},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139046671},
  isbn = {978-1-139-04667-1},
  langid = {english},
  file = {/home/alex/Zotero/storage/9BM6H898/Schweder and Hjort - 2016 - Confidence, Likelihood, Probability Statistical I.pdf}
}

@article{seaman_hidden_2012,
  title = {Hidden {{Dangers}} of {{Specifying Noninformative Priors}}},
  author = {Seaman, John W. and Seaman, John W. and Stamey, James D.},
  year = {2012},
  month = may,
  journal = {The American Statistician},
  volume = {66},
  number = {2},
  pages = {77--84},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2012.695938},
  langid = {english},
  file = {/home/alex/Zotero/storage/Q7VIJTYS/Seaman et al. - 2012 - Hidden Dangers of Specifying Noninformative Priors.pdf}
}

@article{stark_constraints_2015,
  title = {Constraints versus {{Priors}}},
  author = {Stark, Philip B.},
  year = {2015},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {3},
  number = {1},
  pages = {586--598},
  issn = {2166-2525},
  doi = {10.1137/130920721},
  abstract = {There are deep and important philosophical differences between Bayesian and frequentist approaches to quantifying uncertainty. However, some practitioners choose between these approaches primarily on the basis of convenience. For instance, the ability to incorporate parameter constraints is sometimes cited as a reason to use Bayesian methods. This reflects two misunderstandings: First, frequentist methods can indeed incorporate constraints on parameter values. Second, it ignores the crucial question of what the result of the analysis will mean. Bayesian and frequentist measures of uncertainty have similar sounding names but quite different meanings. For instance, Bayesian uncertainties typically involve expectations with respect to the posterior distribution of the parameter, holding the data fixed; frequentist uncertainties typically involve expectations with respect to the distribution of the data, holding the parameter fixed. Bayesian methods, including methods incorporating parameter constraints, require supplementing the constraints with a prior probability distribution for parameter values. This can cause frequentist and Bayesian estimates and their nominal uncertainties to differ substantially, even when the prior is ``uninformative.'' This paper gives simple examples where ``uninformative'' priors are, in fact, extremely informative, and sketches how to measure how much information the prior adds to the constraint. Bayesian methods can have good frequentist behavior, and a frequentist can use Bayesian methods and quantify the uncertainty by frequentist means\textemdash but absent a meaningful prior, Bayesian uncertainty measures lack meaning. The paper ends with brief reflections on practice.},
  langid = {english},
  file = {/home/alex/Zotero/storage/99MMTVM6/Stark - 2015 - Constraints versus Priors.pdf;/home/alex/Zotero/storage/FMS9GJND/stark2015.pdf#view=FitH.pdf}
}

@article{zhang_inferential_2011,
  title = {Inferential {{Models}} for {{Linear Regression}}},
  author = {Zhang, Zuoyi and Xu, Huiping and Martin, Ryan and Liu, Chuanhai},
  year = {2011},
  month = sep,
  journal = {Pakistan Journal of Statistics and Operation Research},
  volume = {7},
  number = {2-Sp},
  issn = {2220-5810, 1816-2711},
  doi = {10.18187/pjsor.v7i2-Sp.301},
  abstract = {Linear regression is arguably one of the most widely used statistical methods. However, important problems, especially variable selection, remain a challenge for classical modes of inference. This paper develops a recently proposed framework of inferential models (IMs) in the linear regression context. In general, the IM framework is able to produce meaningful probabilistic summaries of the statistical evidence for and against assertions about the unknown parameter of interest, and these summaries are shown to be properly calibrated in a frequentist sense. Here we demonstrate by example that the IM framework is promising for linear regression analysis--including model checking, variable selection, and prediction---and for uncertain inference in general.},
  langid = {english},
  file = {/home/alex/Zotero/storage/GENU52KM/Zhang et al. - 2011 - Inferential Models for Linear Regression.pdf}
}
