---
title: "the value of methods software is purpose specific"
subtitle: |
  understanding the gap between code useful to method producers and method consumers
date: "2022-05-14"
categories: [statistical software]
bibliography: references.bib
draft: true
---

Academics, and methodologists in particular, produce shitty code. This shitty code is frustrating to practicing data analysts, and an unsurprising consequence of this frustration is that we are entering a period of [standards proliferation](https://xkcd.com/927/) for research software. The idea is roughly that if we write down what good software looks like, methodologists will start writing good software, and practitioners will ultimately have usable tools.

I am a little skeptical of this idea: the standards that people are writing are very high standards and there are a lot of circumstances when it's perfectly reasonable for methods producers to write bad code. My hope is that by highlighting different needs of methods producers and consumers, we can make software contributions more legible and perhaps have more realistic expectations. My basic thesis here is that it takes too much work for methods producers to write usable code. Instead of asking them to write useful code, especially in absence of meaningful incentives, I propose that a third group of "methods translators" take on the primary responsibility for turning research code in useful code. This dramatically lowers the labor expectations for methods producers: instead of producing polished and final software products, they only need to produce code that can form the basis of a future software product.

### A normative typology of methodological software contributions

1. **Pseudo-code**. Pseudo-code describes an algorithm in formal and unambiguous language, but not in a computer code. Pseudo-code is primarily useful to methods producers who want to study theoretical properties of an algorithm. The pseudo-code itself is not software, and may omit key practical details needed to implement an algorithm. Pseudo-code represents an intellectual rather than a practical contribution.

2. **Reference implementation**. A reference implementation is a basic translation of pseudo-code into runnable computer code. Reference implementations should run on toy problems and produce results that can be used to test more fully featured and computationally efficient implementations. The primary goal of a reference implementation, however, is to be exceptionally readable. The goal of a reference implementation is not to be useful software in and of itself; the goal is to enable someone else to base useful software off of the reference implementation. Bare bones reference implementations devoid of object orientation (to the extent possible) are best. The notation in a reference implementation should also closely match the notation in the paper or document that proposes the algorithm. Reference implementations are minor but important software implementations. Including a reference implementation should be a minimum standard for methods work. 

3. **Proof-of-concept**. Proof-of-concept implements are canonical research code. These implementations are designed to run on research problems and include features that researchers need to get their research done. These additional features typically obscure the details of the implementation and thus make proof-of-concept contributions less useful than reference implementations as the basis for further development. It is not important that proof-of-concept implementations be fully featured or well-designed or computationally efficient. They should typically be released publicly once and should be un-maintained. This code should primarily be useful for *other researchers* who also want to use the code *for research*. Proof-of-concept implementations can take substantial amounts of work, and we should recognize this contribution. We should also recognize that this labor is unlikely to benefit anyone beyond the original authors of the scripts. Sharing proof-of-concept implementations is important for reproducibility purposes, but a terse reference implementation is likely to be a more meaningful contribution to the community at large.

4. **Medium data**. The goal of a middling or medium data contribution is to help practitioners solve practical but technically unchallenging problems; to do things like fit new models on less than a gigabyte of data, for example. This type of software is designed for use by methods consumers rather than methods producers. The primary difference between a proof-of-concept implementation and a middling implementation is adherence to software engineering best practices and some attention to the user experience. The software should be usable, well-documented, tested and released in an easily installable form. The authors should provide at least some basic and ongoing maintenance for the code. They should also make a genuine effort to disseminate the software by advertising it and providing basic tutorials on its use. The internal structure of the code does not have to be perfect, but it needs to be understandable enough to have moderate faith in its correctness. It's important to note that the move from a proof-of-concept implementation to a middling implementation provides no personal benefit to the researcher, the benefit here is to the community of methods consumers.

5. **Production ready**. The final level of methods software is software that is ready for use in production, by which I mean that is reliable, feature complete and scales to technically challenging settings. These implementations should follow software engineering best practices and have responsive maintenance teams, as well as a semi-frequent release cycle and extensive documentation. The primary differences between a middling implementation and a production implementation are the internal design of the software, increased UX polish, and thorough testing. Production implementations should handle errors and edge cases gracefully and produce results that are reliably correct.

I think a lot of current software frustration stems from a desire (from methods consumers) for medium data or production implementations, while there is rarely any need for methods producers to go beyond a proof-of-concept implementation. Especially in methods land, but also more generally, the value of software is purpose specific. Methods producers and consumers have different needs and skill sets, and I think it makes a lot of sense to highlight an intermediary developer role to bridge the gap. To this effectively
