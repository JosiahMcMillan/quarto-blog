---
title: "a worked hypothesis testing problem"
subtitle: |
  some pointers on things that can go right and wrong
date: "TODO"
categories: [hypothesis testing, frequentism, p-hacking]
draft: true
fig-width: 10
fig-height: 6
---

In an data science course that I am currently TA-ing, we just gave out the following problem.

Suppose you have the following six sequences of coin flips. Exactly one sequence is generated by repeatedly flipping a fair coin. Which sequence is it?

```{r}
#| echo: true

sequences <- c(
  A = "HTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHTHHTHTHTHTHTHTHTTHTHTHTHTHTHTHHTHTHTHTHTHTHTHTHTHTHTHTHTHTHHTTHTHTHTHTHTHTHTHTHTHTHTHTHHTHTHTHTHTHTHTHTHTHTHTHTTHTHTHTHTHTHTHTHTHTHTHTHTHHTHTHTHTHTHTHTHTHTHTHTHHTHTHTHTH",
  B = "HHHTHTTTHHTHHTHHHTTTTHTHTHHTTHTHHHTHHTHTTTHTHHHTHTTTHTHTHHTHTHTTHTHHTHTHTTTHTHHHTHTHTTHTHTHHTHTHTHHHTHTTTHTHHTHTHTHHTTTHTHHTHHTTTTHTHTHHHTHTTHTHHTHTHTTHTHHTHTHHHTHHHTHTTTHTTHTTTHTHHHTHTHTTHTHHTHHTHTTT",
  C = "HHTHTHTTTHTHHHTHHTTTHTHHTHTTTHTHTHHTHTHTTHTHHHHHHTTTHTHTHHTHTTTHTHHTHTHTTTHTHHHTTHTTTHTHTHHHHTHTTHHTTTTTHTHHHTHTHTTTTTHHHTHHTHHTHHHTTTTHTHTHHHTHHTTTTTHTHHHTHTHTHTTTHTHHHTHTHTHTTHTHHTHTHTHTTTTHTHHHTHTH",
  D = "HTHHHHHHHTHTTHHTTHHHTHTHTTTHHTHHHTHHTTHTTTTTTTTTHTHHTTTTTHTHTHTHHTTHTTHTTTTTHHHTHTTTHTHTHHHTHTTTTHTHTHHTTHTHTTHHTHTHHHHTHTTHHTTHTTHTTHTHHHHHHTTTTTTHHHTTHTHHHHTTTHTTHHHTTHTHHTTTHHTHHTTTHTHHTHHHTHHTTHHH",
  E = "HHHHHHHHHHHTTTTTTTTTTTHHHHHHHHHHHHTTTTTTTTTTTHHHHHHHHHHHHHTTTTTTTTTTHHHHHHHHHHTTTTTTTTHHHHHHHHTTTTTTTHHHHHHHHHTTTTTTTTTHHHHHHHHTTTHHHHHHHHHHHTTTTTTTTTTTHHHHHHHHHHHHTTTTTTTTTTTHHHHHHHHHHHHHTTTTTTTTTTHH",
  F = "TTHTTTHTTTTTTTHTHTHTHTTHTTHTHHTHHTTTHHTHTTTHTHHTHHHTHTTHHTHHTTHTHTTTTHTHTTTHHTTTTTTTTHTHHTTHTTTTTTHTHTHTHTTTHTTHHTTHTTTHHTTTHTTHTTTTHTTTTHHTTTHTHTHHHTTTTTTHTHHTTTTTTTTTTTTHHHTTTHHHTTTHTTTHTHTTHTTTTTHT"
)
```

To make the problem a little more concrete, here is a visualization of the coin flip sequences. Which one looks genuinely random to you? Try to solve this problem on your own before reading on!

```{r}
#| label: fig-observed
#| fig-cap: Sequences A, B, C, D, E and F. Only one sequence was generated randomly using a fair coin.
#| message: false
#| warning: false
#| column: body-outset
#| code-fold: true
library(tidyverse)
library(glue)

theme_set(theme_classic(16))

observed_nested <- tibble(
  name = glue("Seq {LETTERS[1:6]}"),
  sequence = sequences
) |> 
  mutate(
    result = str_split(sequence, "")
  )

observed_long <- observed_nested  |> 
  select(-sequence) |> 
  unnest(c(result)) |> 
  group_by(name) |> 
  mutate(
    index = row_number()
  )

observed_long |> 
  ggplot(aes(x = index, y = result, group = name)) +
  geom_line(color = "grey") +
  geom_point(aes(color = result)) +
  scale_color_brewer(type = "qual") +
  facet_grid(rows = vars(name)) +
  labs(
    title = "Sequences of coin flips",
    subtitle = "Only one sequence was generated from fair and dependent coin flips",
    caption = "Each sequence consists of 200 results, either 'heads' or 'tails'"
  ) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  )
```

## Eye-balling it

Staring at the sequences, I notice a couple things at first:

-   `A` always alternates between heads and tails, on every single flip
-   `E` stays heads for a long time, then switches to tails for a long time, and then the cycle repeats
-   All of the other cycles switch back and forth between heads and tails fairly often, with some long streaks of all heads or all tails as well

Immediately I think I can rule out `A` and `E`, because each flip appears to depend on the previous flips, and this shouldn't happen when I repeatedly flip a fair coin. Hopefully, I can develop a test later on that will capture this intuition more quantitatively, but for now, I just rule out `A` and `E`.

There are some other patterns that look suspicious to me, but I've previously learned that my intuition about which sequences are random and which aren't isn't very good. Part of the challenge here is that I am mainly looking at non-random sequences in @fig-observed, and this doesn't tell me very much about which patterns are surprising under a random model. So next I generate six sequences by repeatedly flipping a fair coin (from this point onward, when I refer to a "random sequence", I mean sequence of heads and tails generated by repeatedly flipping a fair coin just like this).

```{r}
#| label: fig-fair
#| column: body-outset
#| code-fold: true
#| fig-cap: Six sequences sampled by repeatedly flipping a fair coin, for comparison.
set.seed(27)

sample_sequence <- function() {
  outcomes <- c("H", "T")
  sample(outcomes, size = 200, replace = TRUE)
}

num_samples <- 6

long_fair <- tibble(
  result = map(1:num_samples, \(x) sample_sequence())
) |> 
  mutate(
    name = glue("Fair {LETTERS[1:num_samples]}")
  ) |> 
  unnest(c(result)) |> 
  group_by(name) |> 
  mutate(
    index = row_number()
  )

long_fair |> 
  ggplot(aes(x = index, y = result, group = name)) +
  geom_line(color = "grey") +
  geom_point(aes(color = result)) +
  scale_color_brewer(type = "qual") +
  facet_grid(rows = vars(name)) +
  labs(
    title = "Sequences of coin flips",
    subtitle = "Every sequence was generated generated from fair and dependent coin flips",
    caption = "Each sequence consists of 200 results, either 'heads' or 'tails'"
  ) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  )
```

In light of these fair sequences, I can't really eye-ball anything else that looks suspicious.

## Counting heads

The next thing that comes to mind is that, if a coin is fair, then roughly half the flips should be heads and half should be tails. Since I know how to simulate sequences of repeatedly flipped fair coins, I'll generate a bunch of sequences from this null model, and then compare the number of heads in the observed sequences to the number of heads in the simulated sequences.

```{r}
#| column: body-outset
#| code-fold: true
count_heads <- function(results) {
  sum(results == "H")
}

sample_statistic_under_null <- function(test_statistic, num_samples = 1000) {
  map_dbl(1:num_samples, \(x) test_statistic(sample_sequence()))
}

null_distribution <- sample_statistic_under_null(count_heads)

observed_statistics <- observed_nested |> 
  mutate(
    statistic = map_dbl(result, count_heads)
  ) |> 
  select(name, statistic)

observed_statistics |> 
  mutate(
    null_sample = list(null_distribution)
  ) |> 
  unnest(c(null_sample)) |> 
  ggplot(aes(x = null_sample)) +
  geom_histogram(
    binwidth = 1,
    center = 0
  ) +
  geom_vline(aes(xintercept = statistic, color = name)) +
  facet_wrap(
    vars(name),
    scales = "free",
    nrow = 1
  ) +
  scale_color_brewer(palette = "Dark2") +
  scale_x_continuous(breaks = c(80, 100, 120)) +
  labs(
    title = "Number of heads in each sequence",
    subtitle = "compare to number of heads in random sequences (grey)"
  ) +
  theme(
    legend.position = "none",
    axis.title = element_blank()
  )

ci <- quantile(null_distribution, c(0.05, 0.95))
```

By looking at the grey histogram (which is repeated in each panel), I can observed that, in ninety percent of the sequences from a repeatedly flipped fair coin, the number of heads in the sequence is between the fifth percentile $q_{0.05} =$ `r quantile(null_distribution, 0.05)` and the ninety-fifth percentile $q_{0.95} =$ `r quantile(null_distribution, 0.95)`. This means that it is *unlikely* that I will see less than `r quantile(null_distribution, 0.05)` heads or more than `r quantile(null_distribution, 0.95)` heads in a random sequence.

```{r}
#| code-fold: true
library(gt)

head_count_stats <- observed_statistics |> 
  mutate(
    expected = mean(null_distribution),
    observed_quantile = map_dbl(statistic, \(stat) ecdf(null_distribution)(stat)),
    p_value = pmin(observed_quantile, 1 - observed_quantile),
    likelihood = 0.5^200,
    log_likelihood = 200 * log(0.5)
  )

head_count_stats |> 
  select(name, statistic) |> 
  gt() |> 
  cols_label(
    name = "Sequence",
    statistic = "Heads in sequence"
  )
```

Since sequence F has only 70 heads, it seems unlikely that sequence F comes from the null model of independent, fair coin flips.

The approach that I have demonstrated above is to compute a *rejection region* for a test statistic that contains my test statistic $90\%$ of the time under the null model. This is not the only way to approach the problem. I could also ask, for each sequence, how often do I see a larger number of heads in random sequences. This is known as computing a *p-value*.

```{r}
#| code-fold: true
head_count_stats |> 
  select(name, p_value) |> 
  gt() |> 
  cols_label(
    name = "Sequence",
    p_value = "P-value"
  )
```

When the p-value is small, it means that the number of observed heads in a sequence is either much higher or much lower than typically seen in random sequences. In particular, a p-value of $p$ (which should always be between zero and one) means that random sequences had as many heads as the mystery sequence only $p \cdot 100\%$ of the time.

More intuitively: small p-values indicate that data is not compatible with a given model. In this case, the very small p-value for sequence $F$ indicates that sequence $F$ is not compatible the random sequence model for coin flips. Sequences $A, B, C, D$ and to a less extent $E$ are all compatible with the random sequence model.


:::callout-warning
### Misleading intuition

If we stare at the p-values a little longer, we might be tempted to conclude that sequence $C$ is the most likely to be sampled from the random model, since it has the highest p-value. The intuition goes roughly like this: a low p-value tells us that it is improbable that the data $X_1, ..., X_n$ came from the null model. Therefore a high p-value tells us that is probable the data $X_1, ..., X_n$ came from the null model.

This is a very common misconception, and if we are a little bit more careful in interpreting the p-value, we can see where it comes from, and also why this reasoning doesn't quite work. More precisely, low p-value tells us that it is improbable that the observed test statistic $T(X_1, ..., X_n)$ would be observed by chance under the null model. The probability of seeing the test statistic is *not* the probability of seeing the data.

Different test statistics can be used to examine the compatibility of the data with different aspects of the null model! One test statistic might have a small p-value, and a different test statistic might have a large p-value, both for the same null model.

Because hypothesis tests function through a test statistic, we cannot use them to compare the relative probability of hypotheses. To make this slightly more concrete, let's compute the probability of observing sequences $A, B, C, D, E$ and $F$ under the null model. This turns out to be $(1/2)^{200}$ for every single sequence. Under the null model, every single sequence is equally likely!

Takeaways:

1. P-values tell us about the probability of the test statistic, not the data.
2. This means we need to pick good test statistics, which is generally a challenging problem.
3. The data can be compatible with multiple models at the same time!

rain example?

reject vs accept vs fail to reject. sometime framed as: cannot "accept" the null hypothesis, can only reject it

Similarly, if we compute the expected number of heads under the null model, we cannot conclude that sequence $C$ is the most likely. Having a sample mean close to a population mean does not tell us about probability of the data under the null.

```{r}
#| code-fold: true
head_count_stats |> 
  select(-p_value, -observed_quantile, -likelihood) |> 
  gt() |> 
  cols_label(
    name = "Sequence",
    statistic = "Heads in sequence",
    expected = "E[num heads|null model]",
    log_likelihood = "log P(data|null model)"
  )
```

:::

## Longest streak of heads in a row

tried one test, time to develop another one. let's try streaks. how do i come up with this as a test statistic? by looking at the data? actually, no, more know that streak lengths are a defining feature of binary random sequences a priori. using a priori knowledge to develop tests is typically a good thing.

```{r}
#| column: body-outset
#| code-fold: true
plot_test <- function(test_statistic) {
  
  null_distribution <- sample_statistic_under_null(test_statistic)
  
  observed_statistics <- observed_nested |> 
    mutate(
      statistic = map_dbl(result, test_statistic)
    ) |> 
    select(name, statistic)
  
  observed_statistics |> 
    mutate(
      null_sample = list(null_distribution)
    ) |> 
    unnest(c(null_sample)) |> 
    ggplot(aes(x = null_sample)) +
    geom_histogram(
      binwidth = 1,
      center = 0
    ) +
    geom_vline(aes(xintercept = statistic, color = name)) +
    facet_wrap(
      vars(name),
      scales = "free",
      nrow = 1
    ) +
    scale_color_brewer(palette = "Dark2") +
    theme(
      legend.position = "none",
      axis.title = element_blank()
    )
}  

longest_heads_subsequence <- function(results) {
  rle_encoded <- rle(results)
  head_sequence_indicators <- rle_encoded$values == "H"
  
  if (!any(head_sequence_indicators)) {
    warning("No head sequences!")
    return(NA)
  }
  
  heads_seq_lengths <- rle_encoded$lengths[head_sequence_indicators]
  max(heads_seq_lengths)
}

plot_test(longest_heads_subsequence) +
  labs(
    title = "Longest streak of heads in each sequence",
    subtitle = "compare to longest streaks of heads in random sequences (grey)"
  )
```

Now we're down to sequences $C$ and $D$ as viable. $C$ has exactly 100 heads, $D$ does not. Is this suspicious? I leave it for you to find out.

## Some suspicious patterns

> I found out that the probability of "HHHHHHTTTTTT" occurring 1 or more times in 200 flips is smaller than 0.05, so I decided to eliminate flips4. So we now have flips2 and flips3 left.

```{r}
#| column: body-outset
#| code-fold: true

# HTHHHHHHTTTTTTHHHTTHTHHHHTTTH
# HHHHHHTTTTTT
num_subseq <- function(results, subseq) {
  collapsed <- paste0(results, collapse = "")
  str_count(collapsed, subseq)
}

pattern1_count <- function(results) {
  num_subseq(results, subseq = "HTHHHHHHTTTTTTHHHTTHTHHHHTTTH")
}

plot_test(pattern1_count) +
  labs(
    title = "Frequency of sequence HTHHHHHHTTTTTTHHHTTHTHHHHTTTH",
    subtitle = "compare to frequency under null model (grey)"
  ) +
  scale_x_continuous(breaks = 0:1, expand = 0:1)
```

So we reject sequence $D$.

```{r}

num_islands <- function(results) {
  rle_encoded <- rle(results)
  sum(table(rle_encoded$lengths))
}

matches <- function(results) {
  sum(results == dplyr::lag(results), na.rm = TRUE)
}

plot_test(num_islands) +
  labs(
    title = "Number of islands",
    subtitle = "compare to frequency under null model (grey)"
  )

plot_test(matches) +
  labs(
    title = "Number of matches",
    subtitle = "compare to frequency under null model (grey)"
  )
```

but we also reject sequence $C$! welcome to testing hell. what is going on here. all data has surprising variation.

looking at it, seeing something surprising, and then testing if that variation is surprising can lead you astray. this is called p-hacking.

To avoid this problem, do you think it is sufficient to choose a test statistic before looking at your data?

Do you think it is a good idea to plan an entire data analysis before looking at your actual data?

what makes a good test statistic? i... uh... have no idea. i.e. what gives a certain procedure guarantees and other procedures no guarantees

We have come up with test statistics that reject every sequence. This cannot possibly be correct! Which sequence do you think is actually random? What tests do you feel most confident in? Why?

Follow up questions:

One of the six sequences was generated by repeatedly flipping a weighted coin, with probability $p \in (0, 1)$ of heads, where $p$ is unknown.

1. Do you have any suspicions about which sequence it was? Why?

2. Can you still use a similar approach to figure out find the sequence from a weighted coin? Why or why not?

3. Overall, we did $X$ total hypothesis tests. Each test had a certain probability of type I error, we did not choose to conduct tests independently. Collectively, across all of our tests, what is the chance of at least one false positive result? Does this seem like a problem to you? Can you think of any solutions?
