---
title: "what statistical software is supposed to do"
subtitle: |
  i argue that expectations for research software should vary with the purpose of that software
date: "2022-05-14"
categories: [statistical software]
bibliography: references.bib
draft: true
---

It has become something of a truism that academics, and methodologists in particular, produce shitty code. This shitty code is frustrating to practicing data analysts in industry, and in the academy, and we are starting to reach a period of [standards proliferation](https://xkcd.com/927/). One key idea here is that is it important to clearly define expectations for research software, such that users can evaluate the quality of software, and academics can receive credit for their coding work. So far I've been involved in at least two such efforts: (1) the ROpenSci [statistical software reviewing guidelines](https://ropensci.org/stat-software-review/), and (2) the [tidymodels implementation principles](https://github.com/tidymodels/model-implementation-principles). At the same time I am increasingly disillusioned with these sorts of efforts, and as I spend more and more time in academia, I find them less and less relevant. Very broadly, I think we are increasingly pushing methods producers to be software producers, and increasingly expecting an imaginary class of methodologist-programmer-researcher unicorns to make methods programmatically accessible and usable for the masses. I don't think this is in any sense a sustainable or reliable approach to distributing methods; the labor demands are simply unrealistic and inconsistent with the structure of academic incentives. I outline an alternative model for methods dissemination based on focused expectations of research code and incentivizing translational roles.

what do we want to happen: methods dissemination. want to learn about a method and actually be able to use it on data. software availability and functionality is a hellscape if you venture beyond the few methods popular enough to attract sophisticated developers as designers and maintainers. software available influences the models we fit


i mean this taxonomy as a corrective to several ongoing pushes within research software engineering, which are pushing for SWE best practices for the sake of SWE best practices. these expectations are too high. expecting each new method to come accompanied by a production-grade package is absurd in terms of labor expectations, and doubly absurd when you realizing that academic methodologists are not rewarded or incentivized to write software by and large. software is often an extension of research and intended to further research, not to further data analysis or concrete data analytic problems.

people who want to use software are often frustrated by the code from people who write software and yell at the people writing software that they need higher standards. i think partially this is because academics don't know how or understand what good code is, but i think this is also in large part because they are writing code that serves a different purpose

propose a path for methodology to reach people: theory paper -> tutorial -> software

## Introduction

distinct set of expectations for reproducibility of analysis code. if method is accompanied by empirical results on that method, a separate set of "empirical" standards applies

the value of the software is purpose-specific

audience: people who produce methods but not software

academic credit and practitioner credit as two separate measures

>    Support or maintenance of the software clarified
>    Document setup, use, and expectations of code

1. Pseudo-code
    - Purpose: describe the algorithm for theoretical analysis
1. Proof-of-concept implementation.
    - Purpose: run on toy problems, small simulations
    - Credit: should be a basic expectation of methods research, useful for simulations in particular
    - possibly just a reference implementation wrapped up in a package
    - Release: likely once
    - Effort: hacked together in a couple afternoons
1. Reference implementation.
    - Purpose:
    - Expectations to meet: be exceptionally readable
    - breaks on anything beyond a toy problem?
1. Middling
    - Purpose: allow the method to be used on small and medium datasets, say up to a 1 Gb in size (glmnet-y)
    - Not production grade, not feature complete
    - Tentatively trustworthy
    - Might not follow SWE best practices, internally potentially a mess
    - Written by academic power users who can hack things out and get them done, although often inelegantly
    - Big time investment
1. Production-ready (spacy)
    - Purpose: reliability, scale, feature-complete problem solving
    - Follows SWE best practices
    - A professional was likely involved in writing this code
    - Maintained, responsive bug-fixes, lots of service work. not a single release, stable release cycle
    - Tested, build system, documented


following software engineering best practices?

software designed for problem solving: spacy. uses fancy algorithms but doesn't tell you what they are, and only implements a single methodology. hyperparameter tuning and method select happen by default. you can tune these things if you want via a config system.

software for research: most R packages, NLTK for example. low-level, not very useful, primary good if you want to compare methods. which is something researchers do but not really something that engineers tend to do.


@lee_barely_2021
@taschuk_ten_2017
@wilson_good_2017
@peer_active_2020

{{< tweet id=1501572647972261892 >}}
