@article{gignac2020,
  title = {The {{Dunning-Kruger}} Effect Is (Mostly) a Statistical Artefact: {{Valid}} Approaches to Testing the Hypothesis with Individual Differences Data},
  shorttitle = {The {{Dunning-Kruger}} Effect Is (Mostly) a Statistical Artefact},
  author = {Gignac, Gilles E. and Zajenkowski, Marcin},
  year = {2020},
  month = may,
  journal = {Intelligence},
  volume = {80},
  pages = {101449},
  issn = {01602896},
  doi = {10.1016/j.intell.2020.101449},
  urldate = {2021-01-05},
  abstract = {The Dunning-Kruger hypothesis states that the degree to which people can estimate their ability accurately depends, in part, upon possessing the ability in question. Consequently, people with lower levels of the ability tend to self-assess their ability less well than people who have relatively higher levels of the ability. The most common method used to test the Dunning-Kruger hypothesis involves plotting the self-assessed and objectively assessed means across four categories (quartiles) of objective ability. However, this method has been argued to be confounded by the better-than-average effect and regression toward the mean. In this investigation, it is argued that the Dunning-Kruger hypothesis can be tested validly with two inferential statistical techniques: the Glejser test of heteroscedasticity and nonlinear (quadratic) regression. On the basis of a sample of 929 general community participants who completed a self-assessment of intelligence and the Advanced Raven's Progressive Matrices, we failed to identify statistically significant heteroscedasticity, contrary to the Dunning-Kruger hypothesis. Additionally, the association between objectively measured intelligence and self-assessed intelligence was found to be essentially entirely linear, again, contrary to the Dunning-Kruger hypothesis. It is concluded that, although the phenomenon described by the Dunning-Kruger hypothesis may be to some degree plausible for some skills, the magnitude of the effect may be much smaller than reported previously.},
  langid = {english},
  file = {/home/alex/Zotero/storage/8TBEINJ7/Gignac and Zajenkowski - 2020 - The Dunning-Kruger effect is (mostly) a statistica.pdf}
}

@article{nuhfer2017,
  title = {How {{Random Noise}} and a {{Graphical Convention Subverted Behavioral Scientists}}' {{Explanations}} of {{Self-Assessment Data}}: {{Numeracy Underlies Better Alternatives}}},
  shorttitle = {How {{Random Noise}} and a {{Graphical Convention Subverted Behavioral Scientists}}' {{Explanations}} of {{Self-Assessment Data}}},
  author = {Nuhfer, Edward and {California State University (retired)} and Fleischer, Steven and {California State University - Channel Islands} and Cogan, Christopher and {Ventura College} and Wirth, Karl and {Macalester College} and Gaze, Eric and {Bowdoin College}},
  year = {2017},
  month = jan,
  journal = {Numeracy},
  volume = {10},
  number = {1},
  issn = {19364660},
  doi = {10.5038/1936-4660.10.1.4},
  urldate = {2023-11-28},
  abstract = {Despite nearly two decades of research, researchers have not resolved whether people generally perceive their skills accurately or inaccurately. In this paper, we trace this lack of resolution to numeracy, specifically to the frequently overlooked complications that arise from the noisy data produced by the paired measures that researchers employ to determine self-assessment accuracy. To illustrate the complications and ways to resolve them, we employ a large dataset (N = 1154) obtained from paired measures of documented reliability to study self-assessed proficiency in science literacy. We collected demographic information that allowed both criterion-referenced and normative-based analyses of selfassessment data. We used these analyses to propose a quantitatively based classification scale and show how its use informs the nature of self-assessment. Much of the current consensus about peoples' inability to self-assess accurately comes from interpreting normative data presented in the KrugerDunning type graphical format or closely related (y - x) vs. (x) graphical conventions. Our data show that peoples' self-assessments of competence, in general, reflect a genuine competence that they can demonstrate. That finding contradicts the current consensus about the nature of self-assessment. Our results further confirm that experts are more proficient in self-assessing their abilities than novices and that women, in general, self-assess more accurately than men. The validity of interpretations of data depends strongly upon how carefully the researchers consider the numeracy that underlies graphical presentations and conclusions. Our results indicate that carefully measured self-assessments provide valid, measurable and valuable information about proficiency.},
  langid = {english},
  file = {/home/alex/Zotero/storage/CZAVG4ZJ/Nuhfer et al. - 2017 - How Random Noise and a Graphical Convention Subver.pdf}
}
