
@article{burkner_monotonic_2018,
	title = {Monotonic {Effects}: {A} {Principled} {Approach} for {Including} {Ordinal} {Predictors} in {Regression} {Models}},
	url = {https://psyarxiv.com/9qkhj},
	language = {en},
	author = {Bürkner, Paul-Christian and Charpentier, Emmanuel},
	year = {2018},
	pages = {20},
	file = {Bürkner and Charpentier - Monotonic Effects A Principled Approach for Inclu.pdf:C\:\\Users\\alex\\Zotero\\storage\\9VXRC8T6\\Bürkner and Charpentier - Monotonic Effects A Principled Approach for Inclu.pdf:application/pdf}
}

@article{cho_intelligent_2018,
	title = {Intelligent {Initialization} and {Adaptive} {Thresholding} for {Iterative} {Matrix} {Completion}; {Some} {Statistical} and {Algorithmic} {Theory} for {Adaptive}-{Impute}},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2018.1518238},
	doi = {10.1080/10618600.2018.1518238},
	abstract = {Over the past decade, various matrix completion algorithms have been developed. Thresholded singular value decomposition (SVD) is a popular technique in implementing many of them. A sizable number of studies have shown its theoretical and empirical excellence, but choosing the right threshold level still remains as a key empirical diﬃculty. This paper proposes a novel matrix completion algorithm which iterates thresholded SVD with theoretically-justiﬁed and data-dependent values of thresholding parameters. The estimate of the proposed algorithm enjoys the ∗This research is supported by NSF grant DMS-1309998 and ARO grant W911NF-15-1-0423.},
	language = {en},
	urldate = {2018-12-10},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cho, Juhee and Kim, Donggyu and Rohe, Karl},
	month = sep,
	year = {2018},
	pages = {1--26},
	file = {Cho et al. - 2018 - Intelligent Initialization and Adaptive Thresholdi.pdf:C\:\\Users\\alex\\Zotero\\storage\\3936LKKS\\Cho et al. - 2018 - Intelligent Initialization and Adaptive Thresholdi.pdf:application/pdf}
}

@article{simpson_modelling_2018,
	title = {Modelling palaeoecological time series using generalized additive models},
	url = {https://www.biorxiv.org/content/10.1101/322248v2},
	language = {en},
	author = {Simpson, Gavin L},
	year = {2018},
	pages = {36},
	file = {Simpson - 2018 - Modelling palaeoecological time series using gener.pdf:C\:\\Users\\alex\\Zotero\\storage\\YTWHDS3Q\\Simpson - 2018 - Modelling palaeoecological time series using gener.pdf:application/pdf}
}

@article{hicks_elements_2019,
	title = {Elements and {Principles} of {Data} {Analysis}},
	url = {http://arxiv.org/abs/1903.07639},
	abstract = {The data revolution has led to an increased interest in the practice of data analysis. As a result, there has been a proliferation of “data science” training programs. Because data science has been previously deﬁned as an intersection of already-established ﬁelds or union of emerging technologies, the following problems arise: (1) There is little agreement about what is data science; (2) Data science becomes secondary to established ﬁelds in a university setting; and (3) It is diﬃcult to have discussions on what it means to learn about data science, to teach data science courses and to be a data scientist. To address these problems, we propose to deﬁne the ﬁeld from ﬁrst principles based on the activities of people who analyze data with a language and taxonomy for describing a data analysis in a manner spanning disciplines. Here, we describe the elements and principles of data analysis. This leads to two insights: it suggests a formal mechanism to evaluate data analyses based on objective characteristics, and it provides a framework to teach students how to build data analyses. We argue that the elements and principles of data analysis lay the foundational framework for a more general theory of data science.},
	language = {en},
	urldate = {2019-03-25},
	journal = {arXiv:1903.07639 [stat]},
	author = {Hicks, Stephanie C. and Peng, Roger D.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07639},
	keywords = {Statistics - Applications},
	annote = {Comment: 27 pages, 9 figures, 1 table},
	file = {Hicks and Peng - 2019 - Elements and Principles of Data Analysis.pdf:C\:\\Users\\alex\\Zotero\\storage\\MFR7H7PZ\\Hicks and Peng - 2019 - Elements and Principles of Data Analysis.pdf:application/pdf}
}

@article{allen_sparse_2013-1,
	title = {Sparse and {Functional} {Principal} {Components} {Analysis}},
	url = {http://arxiv.org/abs/1309.2895},
	abstract = {Regularized variants of Principal Components Analysis, especially Sparse PCA and Functional PCA, are among the most useful tools for the analysis of complex high-dimensional data. Many examples of massive data, have both sparse and functional (smooth) aspects and may benefit from a regularization scheme that can capture both forms of structure. For example, in neuro-imaging data, the brain's response to a stimulus may be restricted to a discrete region of activation (spatial sparsity), while exhibiting a smooth response within that region. We propose a unified approach to regularized PCA which can induce both sparsity and smoothness in both the row and column principal components. Our framework generalizes much of the previous literature, with sparse, functional, two-way sparse, and two-way functional PCA all being special cases of our approach. Our method permits flexible combinations of sparsity and smoothness that lead to improvements in feature selection and signal recovery, as well as more interpretable PCA factors. We demonstrate the efficacy of our method on simulated data and a neuroimaging example on EEG data.},
	language = {en},
	urldate = {2019-05-19},
	journal = {arXiv:1309.2895 [stat]},
	author = {Allen, Genevera I. and Weylandt, Michael},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.2895},
	keywords = {Statistics - Machine Learning},
	file = {1309.2895.pdf:C\:\\Users\\alex\\Zotero\\storage\\BJMKQ593\\1309.2895.pdf:application/pdf}
}

@article{zink_fair_2019,
	title = {Fair {Regression} for {Health} {Care} {Spending}},
	url = {http://arxiv.org/abs/1901.10566},
	abstract = {The distribution of health care payments to insurance plans has substantial consequences for social policy. Risk adjustment formulas predict spending in health insurance markets in order to provide fair beneﬁts and health care coverage for all enrollees, regardless of their health status. Unfortunately, current risk adjustment formulas are known to undercompensate payments to health insurers for speciﬁc groups of enrollees (by underpredicting their spending). Much of the existing algorithmic fairness literature for group fairness to date has focused on classiﬁers and binary outcomes. To improve risk adjustment formulas for undercompensated groups, we expand on concepts from the statistics, computer science, and health economics literature to develop new fair regression methods for continuous outcomes by building fairness considerations directly into the objective function. We additionally propose a novel measure of fairness while asserting that a suite of metrics is necessary in order to evaluate risk adjustment formulas more fully. Our data application using the IBM MarketScan Research Databases and simulation studies demonstrate that these new fair regression methods may lead to massive improvements in group fairness with only small reductions in overall ﬁt.},
	language = {en},
	urldate = {2019-05-19},
	journal = {arXiv:1901.10566 [cs, stat]},
	author = {Zink, Anna and Rose, Sherri},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.10566},
	keywords = {Computer Science - Computers and Society, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 31 pages, 3 figures},
	file = {Zink and Rose - 2019 - Fair Regression for Health Care Spending.pdf:C\:\\Users\\alex\\Zotero\\storage\\K3HW7J65\\Zink and Rose - 2019 - Fair Regression for Health Care Spending.pdf:application/pdf}
}

@book{kirk_thoughtful_2015,
	address = {Beijing},
	edition = {First edition, October 2014},
	title = {Thoughtful machine learning},
	isbn = {978-1-4493-7406-8},
	language = {en},
	publisher = {O'Reilly},
	author = {Kirk, Matthew},
	year = {2015},
	keywords = {Algorithms, Machine learning},
	file = {Kirk - 2015 - Thoughtful machine learning.pdf:C\:\\Users\\alex\\Zotero\\storage\\RYX2KHIH\\Kirk - 2015 - Thoughtful machine learning.pdf:application/pdf}
}

@inproceedings{breck_ml_2017,
	address = {Boston, MA},
	title = {The {ML} test score: {A} rubric for {ML} production readiness and technical debt reduction},
	isbn = {978-1-5386-2715-0},
	shorttitle = {The {ML} test score},
	url = {http://ieeexplore.ieee.org/document/8258038/},
	doi = {10.1109/BigData.2017.8258038},
	abstract = {Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large ofﬂine research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difﬁcult to formulate speciﬁc tests, given that the actual prediction behavior of any given model is difﬁcult to specify a priori. In this paper, we present 28 speciﬁc tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.},
	language = {en},
	urldate = {2019-05-28},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
	month = dec,
	year = {2017},
	pages = {1123--1132},
	file = {Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:C\:\\Users\\alex\\Zotero\\storage\\FI6X3D6R\\Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:application/pdf}
}

@book{chambers_statistical_1999,
	address = {Boca Raton, London, New York},
	title = {Statistical models in {S}},
	isbn = {978-0-412-05301-6 978-0-412-05291-0},
	language = {en},
	publisher = {Chapman and Hall/CRC},
	author = {Chambers, John M and Hastie, Trevor J},
	year = {1999},
	note = {The "White Book"},
	file = {Chambers and Hastie - 1999 - Statistical models in S.pdf:C\:\\Users\\alex\\Zotero\\storage\\GQHGMBB3\\Chambers and Hastie - 1999 - Statistical models in S.pdf:application/pdf}
}